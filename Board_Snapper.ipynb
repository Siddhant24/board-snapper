{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Board Snapper.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siddhant24/board-snapper/blob/master/Board_Snapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "c7hLWYB3z1qd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b27d0b5-d07c-400a-94d7-71b148ed3050"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import math\n",
        "import os\n",
        "import keras\n",
        "import pandas as pd\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Conv2D, Conv3D, TimeDistributed, Dropout, Flatten, MaxPooling2D, MaxPool3D, Reshape, LSTM, Bidirectional\n",
        "from keras import regularizers\n",
        "\n",
        "#!rm board-snapper -r\n",
        "# !git clone https://github.com/Siddhant24/board-snapper\n",
        "# !pip install keras-metrics\n",
        "import keras_metrics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YolWF-YNz1qk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(x_folder,y_folder,start=1,end = 50):\n",
        "    x_data=[];\n",
        "    x_paths=[];\n",
        "    y_data=None;\n",
        "    for i in range(start,end):\n",
        "        src='lec'+str(i);\n",
        "        image_folder=x_folder+src;\n",
        "        #for img in sorted(os.listdir(image_folder)):\n",
        "        for j in range(1, len(os.listdir(image_folder)) + 1):\n",
        "            img = 'slide'+str(j)+'.jpg'\n",
        "            image=cv2.resize(cv2.imread(os.path.join(image_folder,img)),(224,224));\n",
        "            x_data.append(image);\n",
        "            x_paths.append(str(os.path.join(image_folder,img)));\n",
        "        print(y_folder+src+'.csv')\n",
        "        label=np.array(np.loadtxt(y_folder+src+'.csv',dtype = int));\n",
        "        y_data = np.concatenate([y_data, label]) if y_data is not None else label;\n",
        "        print(src+\"loaded\");\n",
        "\n",
        "    return np.array(x_data),np.array(y_data),x_paths;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MnAFpTeUz1qn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def format_data(in_x,in_y,index,timestep,batch_size):\n",
        "    total_samples=in_x.shape[0];\n",
        "    dim=(batch_size,timestep,in_x.shape[1],in_x.shape[2],in_x.shape[3])\n",
        "    \n",
        "    x_train=np.zeros(batch_size*timestep*in_x.shape[1]*in_x.shape[2]*in_x.shape[3]).reshape(*dim);\n",
        "    y_train=np.zeros(batch_size);\n",
        "    \n",
        "    offset=int(timestep/2);\n",
        "    for i in range(dim[0]):\n",
        "        x_train[i]=in_x[i+(index*batch_size):i+(index*batch_size)+timestep];\n",
        "        y_train[i]=in_y[i+(index*batch_size)+offset];\n",
        "    return x_train,y_train;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KjmlBLN0z1qp",
        "colab_type": "code",
        "outputId": "3fac6163-6e71-4448-f77f-51adaea24e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "cell_type": "code",
      "source": [
        "def getmodel(timesteps):\n",
        "    model=Sequential();\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'),input_shape=(timesteps,224, 224, 3)));\n",
        "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))));\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3),activation='relu',kernel_regularizer = regularizers.l2('0.5'))));\n",
        "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3),activation='relu',kernel_regularizer = regularizers.l2('0.5'))));\n",
        "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3),activation='relu',kernel_regularizer = regularizers.l2('0.5'))));\n",
        "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3),activation='relu',kernel_regularizer = regularizers.l2('0.5'))));\n",
        "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))));\n",
        "    model.add(TimeDistributed(Flatten()));\n",
        "    model.add(Bidirectional(LSTM(32,activation='tanh',return_sequences=False, kernel_regularizer=regularizers.l2('0.5')),merge_mode='concat'));\n",
        "    model.add(Dense(1,activation='sigmoid'));\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', keras_metrics.precision(), keras_metrics.recall()]);\n",
        "    model.summary();\n",
        "    return model;\n",
        "\n",
        "timesteps=20;\n",
        "lrcn=getmodel(timesteps);\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_1 (TimeDist (None, 20, 222, 222, 32)  896       \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 20, 111, 111, 32)  0         \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 20, 109, 109, 32)  9248      \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 20, 54, 54, 32)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 20, 52, 52, 32)    9248      \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 20, 26, 26, 32)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 20, 24, 24, 32)    9248      \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 20, 12, 12, 32)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, 20, 10, 10, 32)    9248      \n",
            "_________________________________________________________________\n",
            "time_distributed_10 (TimeDis (None, 20, 5, 5, 32)      0         \n",
            "_________________________________________________________________\n",
            "time_distributed_11 (TimeDis (None, 20, 800)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 64)                213248    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 251,201\n",
            "Trainable params: 251,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GK7qnlg-z1qt",
        "colab_type": "code",
        "outputId": "2d127a24-f752-43b6-b412-a10f957d71b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        }
      },
      "cell_type": "code",
      "source": [
        "x_folder='board-snapper/slides/';\n",
        "y_folder='board-snapper/labels/';\n",
        "print(\"loading data\");\n",
        "x_data,y_data,x_paths=load_data(x_folder,y_folder, 21, 50);\n",
        "x_data = x_data[...,::-1]; ## BGR to RGB\n",
        "print(\"data loaded\");"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data\n",
            "board-snapper/labels/lec21.csv\n",
            "lec21loaded\n",
            "board-snapper/labels/lec22.csv\n",
            "lec22loaded\n",
            "board-snapper/labels/lec23.csv\n",
            "lec23loaded\n",
            "board-snapper/labels/lec24.csv\n",
            "lec24loaded\n",
            "board-snapper/labels/lec25.csv\n",
            "lec25loaded\n",
            "board-snapper/labels/lec26.csv\n",
            "lec26loaded\n",
            "board-snapper/labels/lec27.csv\n",
            "lec27loaded\n",
            "board-snapper/labels/lec28.csv\n",
            "lec28loaded\n",
            "board-snapper/labels/lec29.csv\n",
            "lec29loaded\n",
            "board-snapper/labels/lec30.csv\n",
            "lec30loaded\n",
            "board-snapper/labels/lec31.csv\n",
            "lec31loaded\n",
            "board-snapper/labels/lec32.csv\n",
            "lec32loaded\n",
            "board-snapper/labels/lec33.csv\n",
            "lec33loaded\n",
            "board-snapper/labels/lec34.csv\n",
            "lec34loaded\n",
            "board-snapper/labels/lec35.csv\n",
            "lec35loaded\n",
            "board-snapper/labels/lec36.csv\n",
            "lec36loaded\n",
            "board-snapper/labels/lec37.csv\n",
            "lec37loaded\n",
            "board-snapper/labels/lec38.csv\n",
            "lec38loaded\n",
            "board-snapper/labels/lec39.csv\n",
            "lec39loaded\n",
            "board-snapper/labels/lec40.csv\n",
            "lec40loaded\n",
            "board-snapper/labels/lec41.csv\n",
            "lec41loaded\n",
            "board-snapper/labels/lec42.csv\n",
            "lec42loaded\n",
            "board-snapper/labels/lec43.csv\n",
            "lec43loaded\n",
            "board-snapper/labels/lec44.csv\n",
            "lec44loaded\n",
            "board-snapper/labels/lec45.csv\n",
            "lec45loaded\n",
            "board-snapper/labels/lec46.csv\n",
            "lec46loaded\n",
            "board-snapper/labels/lec47.csv\n",
            "lec47loaded\n",
            "board-snapper/labels/lec48.csv\n",
            "lec48loaded\n",
            "board-snapper/labels/lec49.csv\n",
            "lec49loaded\n",
            "data loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5CpB5E4Wz1qx",
        "colab_type": "code",
        "outputId": "9ba7f18b-bb5a-4b68-c7a9-4147e4ceef94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54060
        }
      },
      "cell_type": "code",
      "source": [
        "timesteps=20;\n",
        "lrcn=getmodel(timesteps);\n",
        "total_samples=x_data.shape[0];\n",
        "iter=total_samples-timesteps;\n",
        "batch_size=16;\n",
        "\n",
        "print(\"No. of samples: \", total_samples)\n",
        "print(\"No. of batches: \", int(iter/batch_size))\n",
        "\n",
        "\n",
        "epochs=1;\n",
        "for e in range(epochs):\n",
        "    for i in range(int(iter/batch_size)):\n",
        "        x_train, y_train= format_data(x_data,y_data,i,timesteps,batch_size);\n",
        "        print('training on batch '+str(i)+' for epoch number '+str(e))\n",
        "        loss = lrcn.train_on_batch(x_train,y_train);\n",
        "        print(loss)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_12 (TimeDis (None, 20, 222, 222, 32)  896       \n",
            "_________________________________________________________________\n",
            "time_distributed_13 (TimeDis (None, 20, 111, 111, 32)  0         \n",
            "_________________________________________________________________\n",
            "time_distributed_14 (TimeDis (None, 20, 109, 109, 32)  9248      \n",
            "_________________________________________________________________\n",
            "time_distributed_15 (TimeDis (None, 20, 54, 54, 32)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_16 (TimeDis (None, 20, 52, 52, 32)    9248      \n",
            "_________________________________________________________________\n",
            "time_distributed_17 (TimeDis (None, 20, 26, 26, 32)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_18 (TimeDis (None, 20, 24, 24, 32)    9248      \n",
            "_________________________________________________________________\n",
            "time_distributed_19 (TimeDis (None, 20, 12, 12, 32)    0         \n",
            "_________________________________________________________________\n",
            "time_distributed_20 (TimeDis (None, 20, 10, 10, 32)    9248      \n",
            "_________________________________________________________________\n",
            "time_distributed_21 (TimeDis (None, 20, 5, 5, 32)      0         \n",
            "_________________________________________________________________\n",
            "time_distributed_22 (TimeDis (None, 20, 800)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 64)                213248    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 251,201\n",
            "Trainable params: 251,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "No. of samples:  25184\n",
            "No. of batches:  1572\n",
            "training on batch 0 for epoch number 0\n",
            "[285.04254, 0.125, 0.0, 0.0]\n",
            "training on batch 1 for epoch number 0\n",
            "[277.71313, 0.125, 0.06666666644444444, 0.9999999500000026]\n",
            "training on batch 2 for epoch number 0\n",
            "[269.4408, 1.0, 0.06666666644444444, 0.9999999500000026]\n",
            "training on batch 3 for epoch number 0\n",
            "[261.03427, 0.9375, 0.06666666644444444, 0.6666666444444451]\n",
            "training on batch 4 for epoch number 0\n",
            "[252.58063, 1.0, 0.06666666644444444, 0.6666666444444451]\n",
            "training on batch 5 for epoch number 0\n",
            "[244.15953, 1.0, 0.06666666644444444, 0.6666666444444451]\n",
            "training on batch 6 for epoch number 0\n",
            "[235.8227, 1.0, 0.06666666644444444, 0.6666666444444451]\n",
            "training on batch 7 for epoch number 0\n",
            "[227.65497, 1.0, 0.06666666644444444, 0.6666666444444451]\n",
            "training on batch 8 for epoch number 0\n",
            "[219.62251, 0.875, 0.06666666644444444, 0.3999999920000002]\n",
            "training on batch 9 for epoch number 0\n",
            "[211.66452, 1.0, 0.06666666644444444, 0.3999999920000002]\n",
            "training on batch 10 for epoch number 0\n",
            "[203.95917, 1.0, 0.06666666644444444, 0.3999999920000002]\n",
            "training on batch 11 for epoch number 0\n",
            "[196.59781, 0.8125, 0.06666666644444444, 0.249999996875]\n",
            "training on batch 12 for epoch number 0\n",
            "[189.2293, 0.875, 0.06666666644444444, 0.19999999800000004]\n",
            "training on batch 13 for epoch number 0\n",
            "[182.00415, 1.0, 0.06666666644444444, 0.19999999800000004]\n",
            "training on batch 14 for epoch number 0\n",
            "[175.11008, 1.0, 0.06666666644444444, 0.19999999800000004]\n",
            "training on batch 15 for epoch number 0\n",
            "[168.40228, 1.0, 0.06666666644444444, 0.19999999800000004]\n",
            "training on batch 16 for epoch number 0\n",
            "[162.03018, 0.875, 0.06666666644444444, 0.1666666652777778]\n",
            "training on batch 17 for epoch number 0\n",
            "[155.66486, 0.9375, 0.06666666644444444, 0.15384615266272192]\n",
            "training on batch 18 for epoch number 0\n",
            "[149.56992, 0.9375, 0.06666666644444444, 0.1428571418367347]\n",
            "training on batch 19 for epoch number 0\n",
            "[143.62686, 0.9375, 0.06666666644444444, 0.13333333244444445]\n",
            "training on batch 20 for epoch number 0\n",
            "[137.82205, 1.0, 0.06666666644444444, 0.13333333244444445]\n",
            "training on batch 21 for epoch number 0\n",
            "[132.29086, 1.0, 0.06666666644444444, 0.13333333244444445]\n",
            "training on batch 22 for epoch number 0\n",
            "[127.15018, 0.875, 0.06666666644444444, 0.11764705813148789]\n",
            "training on batch 23 for epoch number 0\n",
            "[121.976425, 0.875, 0.06666666644444444, 0.10526315734072021]\n",
            "training on batch 24 for epoch number 0\n",
            "[116.77788, 1.0, 0.06666666644444444, 0.10526315734072021]\n",
            "training on batch 25 for epoch number 0\n",
            "[111.97653, 1.0, 0.06666666644444444, 0.10526315734072021]\n",
            "training on batch 26 for epoch number 0\n",
            "[107.3431, 1.0, 0.06666666644444444, 0.10526315734072021]\n",
            "training on batch 27 for epoch number 0\n",
            "[103.35385, 0.75, 0.06666666644444444, 0.0869565213610586]\n",
            "training on batch 28 for epoch number 0\n",
            "[98.57205, 1.0, 0.06666666644444444, 0.0869565213610586]\n",
            "training on batch 29 for epoch number 0\n",
            "[94.545395, 0.9375, 0.06666666644444444, 0.0833333329861111]\n",
            "training on batch 30 for epoch number 0\n",
            "[90.43421, 1.0, 0.06666666644444444, 0.0833333329861111]\n",
            "training on batch 31 for epoch number 0\n",
            "[86.71598, 0.9375, 0.06666666644444444, 0.07999999968]\n",
            "training on batch 32 for epoch number 0\n",
            "[83.02226, 0.9375, 0.06666666644444444, 0.07692307662721894]\n",
            "training on batch 33 for epoch number 0\n",
            "[79.46109, 0.9375, 0.06666666644444444, 0.07407407379972565]\n",
            "training on batch 34 for epoch number 0\n",
            "[76.040016, 0.9375, 0.06666666644444444, 0.07142857117346939]\n",
            "training on batch 35 for epoch number 0\n",
            "[73.042435, 0.8125, 0.06666666644444444, 0.06451612882414151]\n",
            "training on batch 36 for epoch number 0\n",
            "[69.45268, 1.0, 0.06666666644444444, 0.06451612882414151]\n",
            "training on batch 37 for epoch number 0\n",
            "[66.4199, 1.0, 0.06666666644444444, 0.06451612882414151]\n",
            "training on batch 38 for epoch number 0\n",
            "[63.95769, 0.8125, 0.06666666644444444, 0.05882352923875432]\n",
            "training on batch 39 for epoch number 0\n",
            "[60.875114, 0.9375, 0.06666666644444444, 0.05714285697959184]\n",
            "training on batch 40 for epoch number 0\n",
            "[58.03507, 1.0, 0.06666666644444444, 0.05714285697959184]\n",
            "training on batch 41 for epoch number 0\n",
            "[55.466114, 1.0, 0.06666666644444444, 0.05714285697959184]\n",
            "training on batch 42 for epoch number 0\n",
            "[53.002556, 1.0, 0.06666666644444444, 0.05714285697959184]\n",
            "training on batch 43 for epoch number 0\n",
            "[50.638924, 1.0, 0.06666666644444444, 0.05714285697959184]\n",
            "training on batch 44 for epoch number 0\n",
            "[48.542263, 0.9375, 0.06666666644444444, 0.055555555401234566]\n",
            "training on batch 45 for epoch number 0\n",
            "[46.559067, 0.875, 0.06666666644444444, 0.052631578808864266]\n",
            "training on batch 46 for epoch number 0\n",
            "[44.13627, 1.0, 0.06666666644444444, 0.052631578808864266]\n",
            "training on batch 47 for epoch number 0\n",
            "[42.141327, 1.0, 0.06666666644444444, 0.052631578808864266]\n",
            "training on batch 48 for epoch number 0\n",
            "[40.75521, 0.8125, 0.06666666644444444, 0.04878048768590125]\n",
            "training on batch 49 for epoch number 0\n",
            "[38.438126, 1.0, 0.06666666644444444, 0.04878048768590125]\n",
            "training on batch 50 for epoch number 0\n",
            "[36.84046, 0.9375, 0.06666666644444444, 0.047619047505668935]\n",
            "training on batch 51 for epoch number 0\n",
            "[34.99829, 1.0, 0.06666666644444444, 0.047619047505668935]\n",
            "training on batch 52 for epoch number 0\n",
            "[33.58069, 0.9375, 0.06666666644444444, 0.046511627798810164]\n",
            "training on batch 53 for epoch number 0\n",
            "[31.859486, 1.0, 0.06666666644444444, 0.046511627798810164]\n",
            "training on batch 54 for epoch number 0\n",
            "[30.391315, 1.0, 0.06666666644444444, 0.046511627798810164]\n",
            "training on batch 55 for epoch number 0\n",
            "[29.650866, 0.8125, 0.06666666644444444, 0.04347826077504726]\n",
            "training on batch 56 for epoch number 0\n",
            "[27.667374, 1.0, 0.06666666644444444, 0.04347826077504726]\n",
            "training on batch 57 for epoch number 0\n",
            "[26.400288, 1.0, 0.06666666644444444, 0.04347826077504726]\n",
            "training on batch 58 for epoch number 0\n",
            "[25.18083, 1.0, 0.06666666644444444, 0.04347826077504726]\n",
            "training on batch 59 for epoch number 0\n",
            "[24.00851, 1.0, 0.06666666644444444, 0.04347826077504726]\n",
            "training on batch 60 for epoch number 0\n",
            "[23.496273, 0.8125, 0.06666666644444444, 0.04081632644731362]\n",
            "training on batch 61 for epoch number 0\n",
            "[21.832428, 1.0, 0.06666666644444444, 0.04081632644731362]\n",
            "training on batch 62 for epoch number 0\n",
            "[20.820644, 1.0, 0.06666666644444444, 0.04081632644731362]\n",
            "training on batch 63 for epoch number 0\n",
            "[19.847256, 1.0, 0.06666666644444444, 0.04081632644731362]\n",
            "training on batch 64 for epoch number 0\n",
            "[18.915794, 1.0, 0.06666666644444444, 0.04081632644731362]\n",
            "training on batch 65 for epoch number 0\n",
            "[18.448225, 0.875, 0.06666666644444444, 0.0392156861976163]\n",
            "training on batch 66 for epoch number 0\n",
            "[17.185936, 1.0, 0.06666666644444444, 0.0392156861976163]\n",
            "training on batch 67 for epoch number 0\n",
            "[16.380587, 1.0, 0.06666666644444444, 0.0392156861976163]\n",
            "training on batch 68 for epoch number 0\n",
            "[15.824717, 0.9375, 0.06666666644444444, 0.03846153838757396]\n",
            "training on batch 69 for epoch number 0\n",
            "[14.878263, 1.0, 0.06666666644444444, 0.03846153838757396]\n",
            "training on batch 70 for epoch number 0\n",
            "[14.177316, 1.0, 0.06666666644444444, 0.03846153838757396]\n",
            "training on batch 71 for epoch number 0\n",
            "[14.403039, 0.75, 0.06666666644444444, 0.03571428565051021]\n",
            "training on batch 72 for epoch number 0\n",
            "[12.898673, 1.0, 0.06666666644444444, 0.03571428565051021]\n",
            "training on batch 73 for epoch number 0\n",
            "[12.310228, 1.0, 0.06666666644444444, 0.03571428565051021]\n",
            "training on batch 74 for epoch number 0\n",
            "[11.729064, 1.0, 0.06666666644444444, 0.03571428565051021]\n",
            "training on batch 75 for epoch number 0\n",
            "[11.169357, 1.0, 0.06666666644444444, 0.03571428565051021]\n",
            "training on batch 76 for epoch number 0\n",
            "[10.637324, 1.0, 0.06666666644444444, 0.03571428565051021]\n",
            "training on batch 77 for epoch number 0\n",
            "[10.337599, 0.9375, 0.06666666644444444, 0.035087719236688214]\n",
            "training on batch 78 for epoch number 0\n",
            "[9.656086, 1.0, 0.06666666644444444, 0.035087719236688214]\n",
            "training on batch 79 for epoch number 0\n",
            "[9.201045, 1.0, 0.06666666644444444, 0.035087719236688214]\n",
            "training on batch 80 for epoch number 0\n",
            "[8.7671175, 1.0, 0.06666666644444444, 0.035087719236688214]\n",
            "training on batch 81 for epoch number 0\n",
            "[8.354132, 1.0, 0.06666666644444444, 0.035087719236688214]\n",
            "training on batch 82 for epoch number 0\n",
            "[7.961137, 1.0, 0.06666666644444444, 0.035087719236688214]\n",
            "training on batch 83 for epoch number 0\n",
            "[7.8566055, 0.9375, 0.06666666644444444, 0.03448275856123662]\n",
            "training on batch 84 for epoch number 0\n",
            "[7.7715855, 0.875, 0.06666666644444444, 0.03333333327777778]\n",
            "training on batch 85 for epoch number 0\n",
            "[6.8952656, 1.0, 0.06666666644444444, 0.03333333327777778]\n",
            "training on batch 86 for epoch number 0\n",
            "[6.825022, 0.9375, 0.06666666644444444, 0.03278688519215265]\n",
            "training on batch 87 for epoch number 0\n",
            "[6.505362, 0.9375, 0.06666666644444444, 0.03225806446409989]\n",
            "training on batch 88 for epoch number 0\n",
            "[6.606075, 0.8125, 0.06666666644444444, 0.030769230721893494]\n",
            "training on batch 89 for epoch number 0\n",
            "[6.0603495, 0.875, 0.06666666644444444, 0.029850746224103365]\n",
            "training on batch 90 for epoch number 0\n",
            "[5.621422, 1.0, 0.06666666644444444, 0.029850746224103365]\n",
            "training on batch 91 for epoch number 0\n",
            "[5.3449955, 1.0, 0.06666666644444444, 0.029850746224103365]\n",
            "training on batch 92 for epoch number 0\n",
            "[5.0635915, 1.0, 0.06666666644444444, 0.029850746224103365]\n",
            "training on batch 93 for epoch number 0\n",
            "[4.9440002, 0.9375, 0.06666666644444444, 0.02941176466262976]\n",
            "training on batch 94 for epoch number 0\n",
            "[4.5731077, 1.0, 0.06666666644444444, 0.02941176466262976]\n",
            "training on batch 95 for epoch number 0\n",
            "[4.355519, 1.0, 0.06666666644444444, 0.02941176466262976]\n",
            "training on batch 96 for epoch number 0\n",
            "[4.153073, 1.0, 0.06666666644444444, 0.02941176466262976]\n",
            "training on batch 97 for epoch number 0\n",
            "[4.3573275, 0.875, 0.06666666644444444, 0.028571428530612248]\n",
            "training on batch 98 for epoch number 0\n",
            "[3.786327, 1.0, 0.06666666644444444, 0.028571428530612248]\n",
            "training on batch 99 for epoch number 0\n",
            "[3.616537, 1.0, 0.06666666644444444, 0.028571428530612248]\n",
            "training on batch 100 for epoch number 0\n",
            "[3.4546394, 1.0, 0.06666666644444444, 0.028571428530612248]\n",
            "training on batch 101 for epoch number 0\n",
            "[3.3002877, 1.0, 0.06666666644444444, 0.028571428530612248]\n",
            "training on batch 102 for epoch number 0\n",
            "[3.3850937, 0.9375, 0.06666666644444444, 0.028169014044832377]\n",
            "training on batch 103 for epoch number 0\n",
            "[3.4850636, 0.875, 0.06666666644444444, 0.027397260236442112]\n",
            "training on batch 104 for epoch number 0\n",
            "[3.3430326, 0.875, 0.06666666644444444, 0.026666666631111113]\n",
            "training on batch 105 for epoch number 0\n",
            "[2.7549014, 1.0, 0.06666666644444444, 0.026666666631111113]\n",
            "training on batch 106 for epoch number 0\n",
            "[2.84602, 0.9375, 0.06666666644444444, 0.026315789439058174]\n",
            "training on batch 107 for epoch number 0\n",
            "[2.5280442, 1.0, 0.06666666644444444, 0.026315789439058174]\n",
            "training on batch 108 for epoch number 0\n",
            "[2.4217024, 1.0, 0.06666666644444444, 0.026315789439058174]\n",
            "training on batch 109 for epoch number 0\n",
            "[2.5043035, 0.9375, 0.06666666644444444, 0.025974025940293476]\n",
            "training on batch 110 for epoch number 0\n",
            "[2.2238474, 1.0, 0.06666666644444444, 0.025974025940293476]\n",
            "training on batch 111 for epoch number 0\n",
            "[2.1281416, 1.0, 0.06666666644444444, 0.025974025940293476]\n",
            "training on batch 112 for epoch number 0\n",
            "[2.3970046, 0.875, 0.06666666644444444, 0.025316455664156388]\n",
            "training on batch 113 for epoch number 0\n",
            "[1.9545292, 1.0, 0.06666666644444444, 0.025316455664156388]\n",
            "training on batch 114 for epoch number 0\n",
            "[1.8715587, 1.0, 0.06666666644444444, 0.025316455664156388]\n",
            "training on batch 115 for epoch number 0\n",
            "[2.1523237, 0.875, 0.06666666644444444, 0.024691357994208202]\n",
            "training on batch 116 for epoch number 0\n",
            "[1.7147045, 1.0, 0.06666666644444444, 0.024691357994208202]\n",
            "training on batch 117 for epoch number 0\n",
            "[1.6428033, 1.0, 0.06666666644444444, 0.024691357994208202]\n",
            "training on batch 118 for epoch number 0\n",
            "[1.5727613, 1.0, 0.06666666644444444, 0.024691357994208202]\n",
            "training on batch 119 for epoch number 0\n",
            "[1.5043676, 1.0, 0.06666666644444444, 0.024691357994208202]\n",
            "training on batch 120 for epoch number 0\n",
            "[1.6406157, 0.9375, 0.06666666644444444, 0.024390243872694825]\n",
            "training on batch 121 for epoch number 0\n",
            "[1.3775209, 1.0, 0.06666666644444444, 0.024390243872694825]\n",
            "training on batch 122 for epoch number 0\n",
            "[1.752264, 0.875, 0.06666666644444444, 0.02380952378117914]\n",
            "training on batch 123 for epoch number 0\n",
            "[1.2677795, 1.0, 0.06666666644444444, 0.02380952378117914]\n",
            "training on batch 124 for epoch number 0\n",
            "[1.2168943, 1.0, 0.06666666644444444, 0.02380952378117914]\n",
            "training on batch 125 for epoch number 0\n",
            "[1.1684127, 1.0, 0.06666666644444444, 0.02380952378117914]\n",
            "training on batch 126 for epoch number 0\n",
            "[1.5475476, 0.875, 0.06666666644444444, 0.02325581392644673]\n",
            "training on batch 127 for epoch number 0\n",
            "[1.0846627, 1.0, 0.06666666644444444, 0.02325581392644673]\n",
            "training on batch 128 for epoch number 0\n",
            "[1.2475497, 0.9375, 0.06666666644444444, 0.02298850572070287]\n",
            "training on batch 129 for epoch number 0\n",
            "[1.0013944, 1.0, 0.06666666644444444, 0.02298850572070287]\n",
            "training on batch 130 for epoch number 0\n",
            "[0.9642403, 1.0, 0.06666666644444444, 0.02298850572070287]\n",
            "training on batch 131 for epoch number 0\n",
            "[1.337701, 0.875, 0.06666666644444444, 0.022471910087110216]\n",
            "training on batch 132 for epoch number 0\n",
            "[0.9006201, 1.0, 0.06666666644444444, 0.022471910087110216]\n",
            "training on batch 133 for epoch number 0\n",
            "[0.8717141, 1.0, 0.06666666644444444, 0.022471910087110216]\n",
            "training on batch 134 for epoch number 0\n",
            "[0.83888245, 1.0, 0.06666666644444444, 0.022471910087110216]\n",
            "training on batch 135 for epoch number 0\n",
            "[1.1956228, 0.875, 0.06666666644444444, 0.021978021953870306]\n",
            "training on batch 136 for epoch number 0\n",
            "[0.77754, 1.0, 0.06666666644444444, 0.021978021953870306]\n",
            "training on batch 137 for epoch number 0\n",
            "[0.750056, 1.0, 0.06666666644444444, 0.021978021953870306]\n",
            "training on batch 138 for epoch number 0\n",
            "[0.72078645, 1.0, 0.06666666644444444, 0.021978021953870306]\n",
            "training on batch 139 for epoch number 0\n",
            "[0.69174623, 1.0, 0.06666666644444444, 0.021978021953870306]\n",
            "training on batch 140 for epoch number 0\n",
            "[0.66267616, 1.0, 0.06666666644444444, 0.021978021953870306]\n",
            "training on batch 141 for epoch number 0\n",
            "[1.0852833, 0.875, 0.06666666644444444, 0.02150537632096196]\n",
            "training on batch 142 for epoch number 0\n",
            "[0.61512446, 1.0, 0.06666666644444444, 0.02150537632096196]\n",
            "training on batch 143 for epoch number 0\n",
            "[0.5936606, 1.0, 0.06666666644444444, 0.02150537632096196]\n",
            "training on batch 144 for epoch number 0\n",
            "[0.5730641, 1.0, 0.06666666644444444, 0.02150537632096196]\n",
            "training on batch 145 for epoch number 0\n",
            "[0.5528146, 1.0, 0.06666666644444444, 0.02150537632096196]\n",
            "training on batch 146 for epoch number 0\n",
            "[0.5332626, 1.0, 0.06666666644444444, 0.02150537632096196]\n",
            "training on batch 147 for epoch number 0\n",
            "[0.51444197, 1.0, 0.06666666644444444, 0.02150537632096196]\n",
            "training on batch 148 for epoch number 0\n",
            "[1.2353733, 0.8125, 0.06666666644444444, 0.020833333311631945]\n",
            "training on batch 149 for epoch number 0\n",
            "[0.7203428, 0.9375, 0.06666666644444444, 0.020618556679774684]\n",
            "training on batch 150 for epoch number 0\n",
            "[1.1517718, 0.8125, 0.06666666644444444, 0.019999999980000002]\n",
            "training on batch 151 for epoch number 0\n",
            "[0.46284544, 1.0, 0.06666666644444444, 0.019999999980000002]\n",
            "training on batch 152 for epoch number 0\n",
            "[0.4599813, 1.0, 0.06666666644444444, 0.019999999980000002]\n",
            "training on batch 153 for epoch number 0\n",
            "[0.4562242, 1.0, 0.06666666644444444, 0.019999999980000002]\n",
            "training on batch 154 for epoch number 0\n",
            "[0.44686422, 1.0, 0.06666666644444444, 0.019999999980000002]\n",
            "training on batch 155 for epoch number 0\n",
            "[0.77104753, 0.875, 0.06666666644444444, 0.01960784311803153]\n",
            "training on batch 156 for epoch number 0\n",
            "[0.42391562, 1.0, 0.06666666644444444, 0.01960784311803153]\n",
            "training on batch 157 for epoch number 0\n",
            "[0.41107702, 1.0, 0.06666666644444444, 0.01960784311803153]\n",
            "training on batch 158 for epoch number 0\n",
            "[0.56999123, 0.9375, 0.06666666644444444, 0.01941747570930342]\n",
            "training on batch 159 for epoch number 0\n",
            "[0.5564918, 0.9375, 0.06666666644444444, 0.019230769212278107]\n",
            "training on batch 160 for epoch number 0\n",
            "[0.36908036, 1.0, 0.06666666644444444, 0.019230769212278107]\n",
            "training on batch 161 for epoch number 0\n",
            "[0.35592133, 1.0, 0.06666666644444444, 0.019230769212278107]\n",
            "training on batch 162 for epoch number 0\n",
            "[0.34216535, 1.0, 0.06666666644444444, 0.019230769212278107]\n",
            "training on batch 163 for epoch number 0\n",
            "[0.32912737, 1.0, 0.06666666644444444, 0.019230769212278107]\n",
            "training on batch 164 for epoch number 0\n",
            "[0.5161264, 0.9375, 0.06666666644444444, 0.01904761902947846]\n",
            "training on batch 165 for epoch number 0\n",
            "[0.30575818, 1.0, 0.06666666644444444, 0.01904761902947846]\n",
            "training on batch 166 for epoch number 0\n",
            "[0.2954566, 1.0, 0.06666666644444444, 0.01904761902947846]\n",
            "training on batch 167 for epoch number 0\n",
            "[0.28566384, 1.0, 0.06666666644444444, 0.01904761902947846]\n",
            "training on batch 168 for epoch number 0\n",
            "[1.1428536, 0.75, 0.06666666644444444, 0.01834862383637741]\n",
            "training on batch 169 for epoch number 0\n",
            "[0.48352814, 0.9375, 0.06666666644444444, 0.018181818165289257]\n",
            "training on batch 170 for epoch number 0\n",
            "[0.4736156, 0.9375, 0.06666666644444444, 0.018018018001785572]\n",
            "training on batch 171 for epoch number 0\n",
            "[0.6676083, 0.875, 0.06666666644444444, 0.017699115028584855]\n",
            "training on batch 172 for epoch number 0\n",
            "[0.2562741, 1.0, 0.06666666644444444, 0.017699115028584855]\n",
            "training on batch 173 for epoch number 0\n",
            "[0.44322628, 0.9375, 0.06666666644444444, 0.017543859633733457]\n",
            "training on batch 174 for epoch number 0\n",
            "[0.4361249, 0.9375, 0.06666666644444444, 0.017391304332703215]\n",
            "training on batch 175 for epoch number 0\n",
            "[0.42817724, 0.9375, 0.06666666644444444, 0.01724137929548157]\n",
            "training on batch 176 for epoch number 0\n",
            "[0.4205377, 0.9375, 0.06666666644444444, 0.017094017079406824]\n",
            "training on batch 177 for epoch number 0\n",
            "[0.7520269, 0.8125, 0.06666666644444444, 0.01666666665277778]\n",
            "training on batch 178 for epoch number 0\n",
            "[0.25002038, 1.0, 0.06666666644444444, 0.01666666665277778]\n",
            "training on batch 179 for epoch number 0\n",
            "[0.4041153, 0.9375, 0.06666666644444444, 0.016528925606174444]\n",
            "training on batch 180 for epoch number 0\n",
            "[0.40071398, 0.9375, 0.06666666644444444, 0.016393442609513573]\n",
            "training on batch 181 for epoch number 0\n",
            "[0.25029606, 1.0, 0.06666666644444444, 0.016393442609513573]\n",
            "training on batch 182 for epoch number 0\n",
            "[0.39095238, 0.9375, 0.06666666644444444, 0.016260162588406374]\n",
            "training on batch 183 for epoch number 0\n",
            "[0.38554603, 0.9375, 0.06666666644444444, 0.016129032245057233]\n",
            "training on batch 184 for epoch number 0\n",
            "[0.38033134, 0.9375, 0.06666666644444444, 0.0159999999872]\n",
            "training on batch 185 for epoch number 0\n",
            "[0.52790856, 0.875, 0.06666666644444444, 0.015748031483662966]\n",
            "training on batch 186 for epoch number 0\n",
            "[0.6765995, 0.8125, 0.06666666644444444, 0.015384615372781065]\n",
            "training on batch 187 for epoch number 0\n",
            "[0.5185679, 0.875, 0.06666666644444444, 0.015151515140036731]\n",
            "training on batch 188 for epoch number 0\n",
            "[0.21570006, 1.0, 0.06666666644444444, 0.015151515140036731]\n",
            "training on batch 189 for epoch number 0\n",
            "[0.36050478, 0.9375, 0.06666666644444444, 0.015037593973655944]\n",
            "training on batch 190 for epoch number 0\n",
            "[0.35678267, 0.9375, 0.06666666644444444, 0.014925373123190021]\n",
            "training on batch 191 for epoch number 0\n",
            "[0.353045, 0.9375, 0.06666666644444444, 0.014814814803840878]\n",
            "training on batch 192 for epoch number 0\n",
            "[0.34930927, 0.9375, 0.06666666644444444, 0.014705882342128029]\n",
            "training on batch 193 for epoch number 0\n",
            "[0.34561405, 0.9375, 0.06666666644444444, 0.014598540135329534]\n",
            "training on batch 194 for epoch number 0\n",
            "[0.1903546, 1.0, 0.06666666644444444, 0.014598540135329534]\n",
            "training on batch 195 for epoch number 0\n",
            "[0.18400183, 1.0, 0.06666666644444444, 0.014598540135329534]\n",
            "training on batch 196 for epoch number 0\n",
            "[0.33455384, 0.9375, 0.06666666644444444, 0.01449275361268641]\n",
            "training on batch 197 for epoch number 0\n",
            "[0.16980419, 1.0, 0.06666666644444444, 0.01449275361268641]\n",
            "training on batch 198 for epoch number 0\n",
            "[0.32820392, 0.9375, 0.06666666644444444, 0.014388489198281663]\n",
            "training on batch 199 for epoch number 0\n",
            "[0.15583858, 1.0, 0.06666666644444444, 0.014388489198281663]\n",
            "training on batch 200 for epoch number 0\n",
            "[0.67153907, 0.8125, 0.06666666644444444, 0.014084507032334855]\n",
            "training on batch 201 for epoch number 0\n",
            "[0.49656883, 0.875, 0.06666666644444444, 0.013888888879243827]\n",
            "training on batch 202 for epoch number 0\n",
            "[0.49424687, 0.875, 0.06666666644444444, 0.01369863012760368]\n",
            "training on batch 203 for epoch number 0\n",
            "[0.4906253, 0.875, 0.06666666644444444, 0.013513513504382763]\n",
            "training on batch 204 for epoch number 0\n",
            "[0.14051, 1.0, 0.06666666644444444, 0.013513513504382763]\n",
            "training on batch 205 for epoch number 0\n",
            "[0.48280364, 0.875, 0.06666666644444444, 0.013333333324444445]\n",
            "training on batch 206 for epoch number 0\n",
            "[0.308675, 0.9375, 0.06666666644444444, 0.013245033103811237]\n",
            "training on batch 207 for epoch number 0\n",
            "[0.30652326, 0.9375, 0.06666666644444444, 0.013157894728185597]\n",
            "training on batch 208 for epoch number 0\n",
            "[0.3044809, 0.9375, 0.06666666644444444, 0.013071895416292879]\n",
            "training on batch 209 for epoch number 0\n",
            "[0.1368306, 1.0, 0.06666666644444444, 0.013071895416292879]\n",
            "training on batch 210 for epoch number 0\n",
            "[0.30060887, 0.9375, 0.06666666644444444, 0.012987012978579862]\n",
            "training on batch 211 for epoch number 0\n",
            "[0.29874864, 0.9375, 0.06666666644444444, 0.01290322579812695]\n",
            "training on batch 212 for epoch number 0\n",
            "[0.4624753, 0.875, 0.06666666644444444, 0.012738853495070795]\n",
            "training on batch 213 for epoch number 0\n",
            "[0.7891871, 0.75, 0.06666666644444444, 0.012422360240731454]\n",
            "training on batch 214 for epoch number 0\n",
            "[0.45531297, 0.875, 0.06666666644444444, 0.01226993864277918]\n",
            "training on batch 215 for epoch number 0\n",
            "[0.2927609, 0.9375, 0.06666666644444444, 0.012195121943783463]\n",
            "training on batch 216 for epoch number 0\n",
            "[0.13661915, 1.0, 0.06666666644444444, 0.012195121943783463]\n",
            "training on batch 217 for epoch number 0\n",
            "[0.2906575, 0.9375, 0.06666666644444444, 0.012121212113865933]\n",
            "training on batch 218 for epoch number 0\n",
            "[0.28950983, 0.9375, 0.06666666644444444, 0.01204819276382639]\n",
            "training on batch 219 for epoch number 0\n",
            "[0.43964496, 0.875, 0.06666666644444444, 0.011904761897675737]\n",
            "training on batch 220 for epoch number 0\n",
            "[0.1372893, 1.0, 0.06666666644444444, 0.011904761897675737]\n",
            "training on batch 221 for epoch number 0\n",
            "[0.1361887, 1.0, 0.06666666644444444, 0.011904761897675737]\n",
            "training on batch 222 for epoch number 0\n",
            "[0.13389033, 1.0, 0.06666666644444444, 0.011904761897675737]\n",
            "training on batch 223 for epoch number 0\n",
            "[0.43477368, 0.875, 0.06666666644444444, 0.011764705875432527]\n",
            "training on batch 224 for epoch number 0\n",
            "[0.12817836, 1.0, 0.06666666644444444, 0.011764705875432527]\n",
            "training on batch 225 for epoch number 0\n",
            "[0.43423274, 0.875, 0.06666666644444444, 0.011627906969983776]\n",
            "training on batch 226 for epoch number 0\n",
            "[0.12247369, 1.0, 0.06666666644444444, 0.011627906969983776]\n",
            "training on batch 227 for epoch number 0\n",
            "[0.11924443, 1.0, 0.06666666644444444, 0.011627906969983776]\n",
            "training on batch 228 for epoch number 0\n",
            "[0.115333624, 1.0, 0.06666666644444444, 0.011627906969983776]\n",
            "training on batch 229 for epoch number 0\n",
            "[0.27372938, 0.9375, 0.06666666644444444, 0.011560693634936016]\n",
            "training on batch 230 for epoch number 0\n",
            "[0.43813357, 0.875, 0.06666666644444444, 0.011428571422040817]\n",
            "training on batch 231 for epoch number 0\n",
            "[0.27141848, 0.9375, 0.06666666644444444, 0.011363636357179753]\n",
            "training on batch 232 for epoch number 0\n",
            "[0.101328775, 1.0, 0.06666666644444444, 0.011363636357179753]\n",
            "training on batch 233 for epoch number 0\n",
            "[0.2695076, 0.9375, 0.06666666644444444, 0.011299435021864727]\n",
            "training on batch 234 for epoch number 0\n",
            "[0.4420542, 0.875, 0.06666666644444444, 0.011173184351299898]\n",
            "training on batch 235 for epoch number 0\n",
            "[0.09351577, 1.0, 0.06666666644444444, 0.011173184351299898]\n",
            "training on batch 236 for epoch number 0\n",
            "[0.44300932, 0.875, 0.06666666644444444, 0.011049723750801257]\n",
            "training on batch 237 for epoch number 0\n",
            "[0.08983733, 1.0, 0.06666666644444444, 0.011049723750801257]\n",
            "training on batch 238 for epoch number 0\n",
            "[0.26557893, 0.9375, 0.06666666644444444, 0.010989010982973071]\n",
            "training on batch 239 for epoch number 0\n",
            "[0.086260945, 1.0, 0.06666666644444444, 0.010989010982973071]\n",
            "training on batch 240 for epoch number 0\n",
            "[0.08409676, 1.0, 0.06666666644444444, 0.010989010982973071]\n",
            "training on batch 241 for epoch number 0\n",
            "[0.08153759, 1.0, 0.06666666644444444, 0.010989010982973071]\n",
            "training on batch 242 for epoch number 0\n",
            "[0.44874915, 0.875, 0.06666666644444444, 0.010869565211483932]\n",
            "training on batch 243 for epoch number 0\n",
            "[0.2632802, 0.9375, 0.06666666644444444, 0.01081081080496713]\n",
            "training on batch 244 for epoch number 0\n",
            "[0.07545082, 1.0, 0.06666666644444444, 0.01081081080496713]\n",
            "training on batch 245 for epoch number 0\n",
            "[0.073600456, 1.0, 0.06666666644444444, 0.01081081080496713]\n",
            "training on batch 246 for epoch number 0\n",
            "[0.2623891, 0.9375, 0.06666666644444444, 0.010752688166261995]\n",
            "training on batch 247 for epoch number 0\n",
            "[0.06966204, 1.0, 0.06666666644444444, 0.010752688166261995]\n",
            "training on batch 248 for epoch number 0\n",
            "[0.06760894, 1.0, 0.06666666644444444, 0.010752688166261995]\n",
            "training on batch 249 for epoch number 0\n",
            "[0.2624206, 0.9375, 0.06666666644444444, 0.01069518716005605]\n",
            "training on batch 250 for epoch number 0\n",
            "[0.06352431, 1.0, 0.06666666644444444, 0.01069518716005605]\n",
            "training on batch 251 for epoch number 0\n",
            "[0.0615087, 1.0, 0.06666666644444444, 0.01069518716005605]\n",
            "training on batch 252 for epoch number 0\n",
            "[0.0593618, 1.0, 0.06666666644444444, 0.01069518716005605]\n",
            "training on batch 253 for epoch number 0\n",
            "[0.05713459, 1.0, 0.06666666644444444, 0.01069518716005605]\n",
            "training on batch 254 for epoch number 0\n",
            "[0.26486543, 0.9375, 0.06666666644444444, 0.010638297866681757]\n",
            "training on batch 255 for epoch number 0\n",
            "[0.05308701, 1.0, 0.06666666644444444, 0.010638297866681757]\n",
            "training on batch 256 for epoch number 0\n",
            "[0.051241726, 1.0, 0.06666666644444444, 0.010638297866681757]\n",
            "training on batch 257 for epoch number 0\n",
            "[0.04936814, 1.0, 0.06666666644444444, 0.010638297866681757]\n",
            "training on batch 258 for epoch number 0\n",
            "[0.047493458, 1.0, 0.06666666644444444, 0.010638297866681757]\n",
            "training on batch 259 for epoch number 0\n",
            "[0.2696852, 0.9375, 0.06666666644444444, 0.010582010576411635]\n",
            "training on batch 260 for epoch number 0\n",
            "[0.044219803, 1.0, 0.06666666644444444, 0.010582010576411635]\n",
            "training on batch 261 for epoch number 0\n",
            "[0.042777486, 1.0, 0.06666666644444444, 0.010582010576411635]\n",
            "training on batch 262 for epoch number 0\n",
            "[0.27266186, 0.9375, 0.06666666644444444, 0.010526315783933518]\n",
            "training on batch 263 for epoch number 0\n",
            "[0.2732961, 0.9375, 0.06666666644444444, 0.01047120418299937]\n",
            "training on batch 264 for epoch number 0\n",
            "[0.27345586, 0.9375, 0.06666666644444444, 0.01041666666124132]\n",
            "training on batch 265 for epoch number 0\n",
            "[0.03900936, 1.0, 0.06666666644444444, 0.01041666666124132]\n",
            "training on batch 266 for epoch number 0\n",
            "[0.038415767, 1.0, 0.06666666644444444, 0.01041666666124132]\n",
            "training on batch 267 for epoch number 0\n",
            "[0.03772693, 1.0, 0.06666666644444444, 0.01041666666124132]\n",
            "training on batch 268 for epoch number 0\n",
            "[0.03695804, 1.0, 0.06666666644444444, 0.01041666666124132]\n",
            "training on batch 269 for epoch number 0\n",
            "[0.27413338, 0.9375, 0.06666666644444444, 0.010362694295148864]\n",
            "training on batch 270 for epoch number 0\n",
            "[0.03557497, 1.0, 0.06666666644444444, 0.010362694295148864]\n",
            "training on batch 271 for epoch number 0\n",
            "[0.034941867, 1.0, 0.06666666644444444, 0.010362694295148864]\n",
            "training on batch 272 for epoch number 0\n",
            "[0.03423887, 1.0, 0.06666666644444444, 0.010362694295148864]\n",
            "training on batch 273 for epoch number 0\n",
            "[0.2753169, 0.9375, 0.06666666644444444, 0.010309278345201404]\n",
            "training on batch 274 for epoch number 0\n",
            "[0.03299398, 1.0, 0.06666666644444444, 0.010309278345201404]\n",
            "training on batch 275 for epoch number 0\n",
            "[0.032432154, 1.0, 0.06666666644444444, 0.010309278345201404]\n",
            "training on batch 276 for epoch number 0\n",
            "[0.2759727, 0.9375, 0.06666666644444444, 0.01025641025115056]\n",
            "training on batch 277 for epoch number 0\n",
            "[0.0314375, 1.0, 0.06666666644444444, 0.01025641025115056]\n",
            "training on batch 278 for epoch number 0\n",
            "[0.030987222, 1.0, 0.06666666644444444, 0.01025641025115056]\n",
            "training on batch 279 for epoch number 0\n",
            "[0.030466124, 1.0, 0.06666666644444444, 0.01025641025115056]\n",
            "training on batch 280 for epoch number 0\n",
            "[0.2766248, 0.9375, 0.06666666644444444, 0.010204081627446897]\n",
            "training on batch 281 for epoch number 0\n",
            "[0.029556993, 1.0, 0.06666666644444444, 0.010204081627446897]\n",
            "training on batch 282 for epoch number 0\n",
            "[0.029151263, 1.0, 0.06666666644444444, 0.010204081627446897]\n",
            "training on batch 283 for epoch number 0\n",
            "[0.02867811, 1.0, 0.06666666644444444, 0.010204081627446897]\n",
            "training on batch 284 for epoch number 0\n",
            "[0.27732337, 0.9375, 0.06666666644444444, 0.010152284258805948]\n",
            "training on batch 285 for epoch number 0\n",
            "[0.027864559, 1.0, 0.06666666644444444, 0.010152284258805948]\n",
            "training on batch 286 for epoch number 0\n",
            "[0.027506473, 1.0, 0.06666666644444444, 0.010152284258805948]\n",
            "training on batch 287 for epoch number 0\n",
            "[0.27754173, 0.9375, 0.06666666644444444, 0.01010101009590858]\n",
            "training on batch 288 for epoch number 0\n",
            "[0.026895063, 1.0, 0.06666666644444444, 0.01010101009590858]\n",
            "training on batch 289 for epoch number 0\n",
            "[0.026625913, 1.0, 0.06666666644444444, 0.01010101009590858]\n",
            "training on batch 290 for epoch number 0\n",
            "[0.52829444, 0.875, 0.06666666644444444, 0.009999999995]\n",
            "training on batch 291 for epoch number 0\n",
            "[0.27614874, 0.9375, 0.06666666644444444, 0.009950248751268534]\n",
            "training on batch 292 for epoch number 0\n",
            "[0.7702391, 0.8125, 0.06666666644444444, 0.009803921563821608]\n",
            "training on batch 293 for epoch number 0\n",
            "[0.027913673, 1.0, 0.06666666644444444, 0.009803921563821608]\n",
            "training on batch 294 for epoch number 0\n",
            "[0.5092987, 0.875, 0.06666666644444444, 0.00970873785936469]\n",
            "training on batch 295 for epoch number 0\n",
            "[0.26596224, 0.9375, 0.06666666644444444, 0.009661835744124717]\n",
            "training on batch 296 for epoch number 0\n",
            "[0.031812795, 1.0, 0.06666666644444444, 0.009661835744124717]\n",
            "training on batch 297 for epoch number 0\n",
            "[0.2605087, 0.9375, 0.06666666644444444, 0.009615384610761835]\n",
            "training on batch 298 for epoch number 0\n",
            "[0.25813067, 0.9375, 0.06666666644444444, 0.009569377985851972]\n",
            "training on batch 299 for epoch number 0\n",
            "[0.036190785, 1.0, 0.06666666644444444, 0.009569377985851972]\n",
            "training on batch 300 for epoch number 0\n",
            "[0.0374835, 1.0, 0.06666666644444444, 0.009569377985851972]\n",
            "training on batch 301 for epoch number 0\n",
            "[0.25270516, 0.9375, 0.06666666644444444, 0.009523809519274376]\n",
            "training on batch 302 for epoch number 0\n",
            "[0.039583143, 1.0, 0.06666666644444444, 0.009523809519274376]\n",
            "training on batch 303 for epoch number 0\n",
            "[0.040370084, 1.0, 0.06666666644444444, 0.009523809519274376]\n",
            "training on batch 304 for epoch number 0\n",
            "[0.040839586, 1.0, 0.06666666644444444, 0.009523809519274376]\n",
            "training on batch 305 for epoch number 0\n",
            "[0.24924847, 0.9375, 0.06666666644444444, 0.009478672981289729]\n",
            "training on batch 306 for epoch number 0\n",
            "[0.24871, 0.9375, 0.06666666644444444, 0.009433962259700961]\n",
            "training on batch 307 for epoch number 0\n",
            "[0.04186037, 1.0, 0.06666666644444444, 0.009433962259700961]\n",
            "training on batch 308 for epoch number 0\n",
            "[0.24768592, 0.9375, 0.06666666644444444, 0.009389671357094051]\n",
            "training on batch 309 for epoch number 0\n",
            "[0.45197964, 0.875, 0.06666666644444444, 0.009302325577068686]\n",
            "training on batch 310 for epoch number 0\n",
            "[0.04337904, 1.0, 0.06666666644444444, 0.009302325577068686]\n",
            "training on batch 311 for epoch number 0\n",
            "[0.043982208, 1.0, 0.06666666644444444, 0.009302325577068686]\n",
            "training on batch 312 for epoch number 0\n",
            "[0.24534309, 0.9375, 0.06666666644444444, 0.009259259254972566]\n",
            "training on batch 313 for epoch number 0\n",
            "[0.044630054, 1.0, 0.06666666644444444, 0.009259259254972566]\n",
            "training on batch 314 for epoch number 0\n",
            "[0.044681307, 1.0, 0.06666666644444444, 0.009259259254972566]\n",
            "training on batch 315 for epoch number 0\n",
            "[0.044417657, 1.0, 0.06666666644444444, 0.009259259254972566]\n",
            "training on batch 316 for epoch number 0\n",
            "[0.043879323, 1.0, 0.06666666644444444, 0.009259259254972566]\n",
            "training on batch 317 for epoch number 0\n",
            "[0.24513355, 0.9375, 0.06666666644444444, 0.009216589857503876]\n",
            "training on batch 318 for epoch number 0\n",
            "[0.04259841, 1.0, 0.06666666644444444, 0.009216589857503876]\n",
            "training on batch 319 for epoch number 0\n",
            "[0.04186768, 1.0, 0.06666666644444444, 0.009216589857503876]\n",
            "training on batch 320 for epoch number 0\n",
            "[0.24612345, 0.9375, 0.06666666644444444, 0.009174311922397104]\n",
            "training on batch 321 for epoch number 0\n",
            "[0.040320903, 1.0, 0.06666666644444444, 0.009174311922397104]\n",
            "training on batch 322 for epoch number 0\n",
            "[0.039509952, 1.0, 0.06666666644444444, 0.009174311922397104]\n",
            "training on batch 323 for epoch number 0\n",
            "[0.2475008, 0.9375, 0.06666666644444444, 0.009132420087154145]\n",
            "training on batch 324 for epoch number 0\n",
            "[0.03789085, 1.0, 0.06666666644444444, 0.009132420087154145]\n",
            "training on batch 325 for epoch number 0\n",
            "[0.03708113, 1.0, 0.06666666644444444, 0.009132420087154145]\n",
            "training on batch 326 for epoch number 0\n",
            "[0.24914461, 0.9375, 0.06666666644444444, 0.00909090908677686]\n",
            "training on batch 327 for epoch number 0\n",
            "[0.03551784, 1.0, 0.06666666644444444, 0.00909090908677686]\n",
            "training on batch 328 for epoch number 0\n",
            "[0.034756623, 1.0, 0.06666666644444444, 0.00909090908677686]\n",
            "training on batch 329 for epoch number 0\n",
            "[0.033894263, 1.0, 0.06666666644444444, 0.00909090908677686]\n",
            "training on batch 330 for epoch number 0\n",
            "[0.2518166, 0.9375, 0.06666666644444444, 0.009049773751561189]\n",
            "training on batch 331 for epoch number 0\n",
            "[0.25239256, 0.9375, 0.06666666644444444, 0.009009009004950897]\n",
            "training on batch 332 for epoch number 0\n",
            "[0.031925898, 1.0, 0.06666666644444444, 0.009009009004950897]\n",
            "training on batch 333 for epoch number 0\n",
            "[0.031418636, 1.0, 0.06666666644444444, 0.009009009004950897]\n",
            "training on batch 334 for epoch number 0\n",
            "[0.699579, 0.8125, 0.06666666644444444, 0.008888888884938271]\n",
            "training on batch 335 for epoch number 0\n",
            "[0.2531517, 0.9375, 0.06666666644444444, 0.008849557518208161]\n",
            "training on batch 336 for epoch number 0\n",
            "[0.9147215, 0.75, 0.06666666644444444, 0.008695652170132325]\n",
            "training on batch 337 for epoch number 0\n",
            "[0.4674679, 0.875, 0.06666666644444444, 0.0086206896514566]\n",
            "training on batch 338 for epoch number 0\n",
            "[0.035460334, 1.0, 0.06666666644444444, 0.0086206896514566]\n",
            "training on batch 339 for epoch number 0\n",
            "[0.45559412, 0.875, 0.06666666644444444, 0.00854700854335598]\n",
            "training on batch 340 for epoch number 0\n",
            "[0.44955075, 0.875, 0.06666666644444444, 0.008474576267595518]\n",
            "training on batch 341 for epoch number 0\n",
            "[0.64320236, 0.8125, 0.06666666644444444, 0.008368200833318745]\n",
            "training on batch 342 for epoch number 0\n",
            "[0.04622237, 1.0, 0.06666666644444444, 0.008368200833318745]\n",
            "training on batch 343 for epoch number 0\n",
            "[0.049486283, 1.0, 0.06666666644444444, 0.008368200833318745]\n",
            "training on batch 344 for epoch number 0\n",
            "[0.052157268, 1.0, 0.06666666644444444, 0.008368200833318745]\n",
            "training on batch 345 for epoch number 0\n",
            "[0.23772472, 0.9375, 0.06666666644444444, 0.00833333332986111]\n",
            "training on batch 346 for epoch number 0\n",
            "[0.056200784, 1.0, 0.06666666644444444, 0.00833333332986111]\n",
            "training on batch 347 for epoch number 0\n",
            "[0.057553373, 1.0, 0.06666666644444444, 0.00833333332986111]\n",
            "training on batch 348 for epoch number 0\n",
            "[0.058301266, 1.0, 0.06666666644444444, 0.00833333332986111]\n",
            "training on batch 349 for epoch number 0\n",
            "[0.23657553, 0.9375, 0.06666666644444444, 0.008298755183278526]\n",
            "training on batch 350 for epoch number 0\n",
            "[0.05873522, 1.0, 0.06666666644444444, 0.008298755183278526]\n",
            "training on batch 351 for epoch number 0\n",
            "[0.058473743, 1.0, 0.06666666644444444, 0.008298755183278526]\n",
            "training on batch 352 for epoch number 0\n",
            "[0.23647818, 0.9375, 0.06666666644444444, 0.008264462806502289]\n",
            "training on batch 353 for epoch number 0\n",
            "[0.595039, 0.8125, 0.06666666644444444, 0.008163265302790504]\n",
            "training on batch 354 for epoch number 0\n",
            "[0.23632447, 0.9375, 0.06666666644444444, 0.008130081297508097]\n",
            "training on batch 355 for epoch number 0\n",
            "[0.058498077, 1.0, 0.06666666644444444, 0.008130081297508097]\n",
            "training on batch 356 for epoch number 0\n",
            "[0.23610169, 0.9375, 0.06666666644444444, 0.00809716598862463]\n",
            "training on batch 357 for epoch number 0\n",
            "[0.23603036, 0.9375, 0.06666666644444444, 0.008064516125780437]\n",
            "training on batch 358 for epoch number 0\n",
            "[0.23595649, 0.9375, 0.06666666644444444, 0.008032128510830471]\n",
            "training on batch 359 for epoch number 0\n",
            "[0.059237577, 1.0, 0.06666666644444444, 0.008032128510830471]\n",
            "training on batch 360 for epoch number 0\n",
            "[0.058974255, 1.0, 0.06666666644444444, 0.008032128510830471]\n",
            "training on batch 361 for epoch number 0\n",
            "[0.05827166, 1.0, 0.06666666644444444, 0.008032128510830471]\n",
            "training on batch 362 for epoch number 0\n",
            "[0.05720036, 1.0, 0.06666666644444444, 0.008032128510830471]\n",
            "training on batch 363 for epoch number 0\n",
            "[0.23621614, 0.9375, 0.06666666644444444, 0.007999999996800001]\n",
            "training on batch 364 for epoch number 0\n",
            "[0.054700866, 1.0, 0.06666666644444444, 0.007999999996800001]\n",
            "training on batch 365 for epoch number 0\n",
            "[0.053313714, 1.0, 0.06666666644444444, 0.007999999996800001]\n",
            "training on batch 366 for epoch number 0\n",
            "[0.05172323, 1.0, 0.06666666644444444, 0.007999999996800001]\n",
            "training on batch 367 for epoch number 0\n",
            "[0.23756242, 0.9375, 0.06666666644444444, 0.007968127486865288]\n",
            "training on batch 368 for epoch number 0\n",
            "[0.04856232, 1.0, 0.06666666644444444, 0.007968127486865288]\n",
            "training on batch 369 for epoch number 0\n",
            "[0.047008164, 1.0, 0.06666666644444444, 0.007968127486865288]\n",
            "training on batch 370 for epoch number 0\n",
            "[0.8207728, 0.75, 0.06666666644444444, 0.007843137251826221]\n",
            "training on batch 371 for epoch number 0\n",
            "[0.045254935, 1.0, 0.06666666644444444, 0.007843137251826221]\n",
            "training on batch 372 for epoch number 0\n",
            "[0.044895094, 1.0, 0.06666666644444444, 0.007843137251826221]\n",
            "training on batch 373 for epoch number 0\n",
            "[0.23952621, 0.9375, 0.06666666644444444, 0.007812499996948243]\n",
            "training on batch 374 for epoch number 0\n",
            "[0.0439026, 1.0, 0.06666666644444444, 0.007812499996948243]\n",
            "training on batch 375 for epoch number 0\n",
            "[0.043280814, 1.0, 0.06666666644444444, 0.007812499996948243]\n",
            "training on batch 376 for epoch number 0\n",
            "[0.042471632, 1.0, 0.06666666644444444, 0.007812499996948243]\n",
            "training on batch 377 for epoch number 0\n",
            "[0.041510463, 1.0, 0.06666666644444444, 0.007812499996948243]\n",
            "training on batch 378 for epoch number 0\n",
            "[0.04043159, 1.0, 0.06666666644444444, 0.007812499996948243]\n",
            "training on batch 379 for epoch number 0\n",
            "[0.24212441, 0.9375, 0.06666666644444444, 0.007782101164287121]\n",
            "training on batch 380 for epoch number 0\n",
            "[0.44700485, 0.875, 0.06666666644444444, 0.007722007719026251]\n",
            "training on batch 381 for epoch number 0\n",
            "[0.038087703, 1.0, 0.06666666644444444, 0.007722007719026251]\n",
            "training on batch 382 for epoch number 0\n",
            "[0.44865957, 0.875, 0.06666666644444444, 0.0076628352461061925]\n",
            "training on batch 383 for epoch number 0\n",
            "[0.037697244, 1.0, 0.06666666644444444, 0.0076628352461061925]\n",
            "training on batch 384 for epoch number 0\n",
            "[0.24308674, 0.9375, 0.06666666644444444, 0.007633587783345958]\n",
            "training on batch 385 for epoch number 0\n",
            "[0.037592605, 1.0, 0.06666666644444444, 0.007633587783345958]\n",
            "training on batch 386 for epoch number 0\n",
            "[0.03741789, 1.0, 0.06666666644444444, 0.007633587783345958]\n",
            "training on batch 387 for epoch number 0\n",
            "[0.037062634, 1.0, 0.06666666644444444, 0.007633587783345958]\n",
            "training on batch 388 for epoch number 0\n",
            "[0.036550652, 1.0, 0.06666666644444444, 0.007633587783345958]\n",
            "training on batch 389 for epoch number 0\n",
            "[0.035906337, 1.0, 0.06666666644444444, 0.007633587783345958]\n",
            "training on batch 390 for epoch number 0\n",
            "[0.45437735, 0.875, 0.06666666644444444, 0.00757575757288797]\n",
            "training on batch 391 for epoch number 0\n",
            "[0.03496012, 1.0, 0.06666666644444444, 0.00757575757288797]\n",
            "training on batch 392 for epoch number 0\n",
            "[0.24515966, 0.9375, 0.06666666644444444, 0.007547169808472766]\n",
            "training on batch 393 for epoch number 0\n",
            "[0.24526642, 0.9375, 0.06666666644444444, 0.007518796989654587]\n",
            "training on batch 394 for epoch number 0\n",
            "[0.03445515, 1.0, 0.06666666644444444, 0.007518796989654587]\n",
            "training on batch 395 for epoch number 0\n",
            "[0.6674512, 0.8125, 0.06666666644444444, 0.007434944235154295]\n",
            "training on batch 396 for epoch number 0\n",
            "[0.034931116, 1.0, 0.06666666644444444, 0.007434944235154295]\n",
            "training on batch 397 for epoch number 0\n",
            "[0.035342477, 1.0, 0.06666666644444444, 0.007434944235154295]\n",
            "training on batch 398 for epoch number 0\n",
            "[0.035533838, 1.0, 0.06666666644444444, 0.007434944235154295]\n",
            "training on batch 399 for epoch number 0\n",
            "[0.035523042, 1.0, 0.06666666644444444, 0.007434944235154295]\n",
            "training on batch 400 for epoch number 0\n",
            "[0.035331283, 1.0, 0.06666666644444444, 0.007434944235154295]\n",
            "training on batch 401 for epoch number 0\n",
            "[0.24456048, 0.9375, 0.06666666644444444, 0.0074074074046639226]\n",
            "training on batch 402 for epoch number 0\n",
            "[0.034815658, 1.0, 0.06666666644444444, 0.0074074074046639226]\n",
            "training on batch 403 for epoch number 0\n",
            "[0.24492732, 0.9375, 0.06666666644444444, 0.0073800737980147324]\n",
            "training on batch 404 for epoch number 0\n",
            "[0.034354538, 1.0, 0.06666666644444444, 0.0073800737980147324]\n",
            "training on batch 405 for epoch number 0\n",
            "[0.034059204, 1.0, 0.06666666644444444, 0.0073800737980147324]\n",
            "training on batch 406 for epoch number 0\n",
            "[0.033628378, 1.0, 0.06666666644444444, 0.0073800737980147324]\n",
            "training on batch 407 for epoch number 0\n",
            "[0.24611889, 0.9375, 0.06666666644444444, 0.0073529411737673]\n",
            "training on batch 408 for epoch number 0\n",
            "[0.2464172, 0.9375, 0.06666666644444444, 0.007326007323323806]\n",
            "training on batch 409 for epoch number 0\n",
            "[0.03259765, 1.0, 0.06666666644444444, 0.007326007323323806]\n",
            "training on batch 410 for epoch number 0\n",
            "[0.24679717, 0.9375, 0.06666666644444444, 0.007299270070328733]\n",
            "training on batch 411 for epoch number 0\n",
            "[0.03220287, 1.0, 0.06666666644444444, 0.007299270070328733]\n",
            "training on batch 412 for epoch number 0\n",
            "[0.03195436, 1.0, 0.06666666644444444, 0.007299270070328733]\n",
            "training on batch 413 for epoch number 0\n",
            "[0.031581115, 1.0, 0.06666666644444444, 0.007299270070328733]\n",
            "training on batch 414 for epoch number 0\n",
            "[0.0311011, 1.0, 0.06666666644444444, 0.007299270070328733]\n",
            "training on batch 415 for epoch number 0\n",
            "[0.030531395, 1.0, 0.06666666644444444, 0.007299270070328733]\n",
            "training on batch 416 for epoch number 0\n",
            "[0.02988831, 1.0, 0.06666666644444444, 0.007299270070328733]\n",
            "training on batch 417 for epoch number 0\n",
            "[0.2500755, 0.9375, 0.06666666644444444, 0.007272727270082644]\n",
            "training on batch 418 for epoch number 0\n",
            "[0.028721035, 1.0, 0.06666666644444444, 0.007272727270082644]\n",
            "training on batch 419 for epoch number 0\n",
            "[0.028182391, 1.0, 0.06666666644444444, 0.007272727270082644]\n",
            "training on batch 420 for epoch number 0\n",
            "[0.027584378, 1.0, 0.06666666644444444, 0.007272727270082644]\n",
            "training on batch 421 for epoch number 0\n",
            "[0.026939556, 1.0, 0.06666666644444444, 0.007272727270082644]\n",
            "training on batch 422 for epoch number 0\n",
            "[0.2538491, 0.9375, 0.06666666644444444, 0.007246376808968703]\n",
            "training on batch 423 for epoch number 0\n",
            "[0.025811933, 1.0, 0.06666666644444444, 0.007246376808968703]\n",
            "training on batch 424 for epoch number 0\n",
            "[0.025311166, 1.0, 0.06666666644444444, 0.007246376808968703]\n",
            "training on batch 425 for epoch number 0\n",
            "[0.2560458, 0.9375, 0.06666666644444444, 0.007220216603891618]\n",
            "training on batch 426 for epoch number 0\n",
            "[0.4886562, 0.875, 0.06666666644444444, 0.007168458778792667]\n",
            "training on batch 427 for epoch number 0\n",
            "[0.71988744, 0.8125, 0.06666666644444444, 0.007092198579045319]\n",
            "training on batch 428 for epoch number 0\n",
            "[0.025328169, 1.0, 0.06666666644444444, 0.007092198579045319]\n",
            "training on batch 429 for epoch number 0\n",
            "[0.025938297, 1.0, 0.06666666644444444, 0.007092198579045319]\n",
            "training on batch 430 for epoch number 0\n",
            "[0.026387414, 1.0, 0.06666666644444444, 0.007092198579045319]\n",
            "training on batch 431 for epoch number 0\n",
            "[0.026683254, 1.0, 0.06666666644444444, 0.007092198579045319]\n",
            "training on batch 432 for epoch number 0\n",
            "[0.026837299, 1.0, 0.06666666644444444, 0.007092198579045319]\n",
            "training on batch 433 for epoch number 0\n",
            "[0.25278547, 0.9375, 0.06666666644444444, 0.007067137806690057]\n",
            "training on batch 434 for epoch number 0\n",
            "[0.25252068, 0.9375, 0.06666666644444444, 0.007042253518647093]\n",
            "training on batch 435 for epoch number 0\n",
            "[0.25207478, 0.9375, 0.06666666644444444, 0.007017543857186826]\n",
            "training on batch 436 for epoch number 0\n",
            "[0.027806368, 1.0, 0.06666666644444444, 0.007017543857186826]\n",
            "training on batch 437 for epoch number 0\n",
            "[0.25109062, 0.9375, 0.06666666644444444, 0.0069930069905618855]\n",
            "training on batch 438 for epoch number 0\n",
            "[0.028504644, 1.0, 0.06666666644444444, 0.0069930069905618855]\n",
            "training on batch 439 for epoch number 0\n",
            "[0.028754333, 1.0, 0.06666666644444444, 0.0069930069905618855]\n",
            "training on batch 440 for epoch number 0\n",
            "[0.028852053, 1.0, 0.06666666644444444, 0.0069930069905618855]\n",
            "training on batch 441 for epoch number 0\n",
            "[0.028811596, 1.0, 0.06666666644444444, 0.0069930069905618855]\n",
            "training on batch 442 for epoch number 0\n",
            "[0.25031388, 0.9375, 0.06666666644444444, 0.00696864111255448]\n",
            "training on batch 443 for epoch number 0\n",
            "[0.25028956, 0.9375, 0.06666666644444444, 0.006944444442033178]\n",
            "training on batch 444 for epoch number 0\n",
            "[0.028818924, 1.0, 0.06666666644444444, 0.006944444442033178]\n",
            "training on batch 445 for epoch number 0\n",
            "[0.028837832, 1.0, 0.06666666644444444, 0.006944444442033178]\n",
            "training on batch 446 for epoch number 0\n",
            "[0.028726636, 1.0, 0.06666666644444444, 0.006944444442033178]\n",
            "training on batch 447 for epoch number 0\n",
            "[0.25043142, 0.9375, 0.06666666644444444, 0.006920415222518887]\n",
            "training on batch 448 for epoch number 0\n",
            "[0.25047895, 0.9375, 0.06666666644444444, 0.006896551721759809]\n",
            "training on batch 449 for epoch number 0\n",
            "[0.02856549, 1.0, 0.06666666644444444, 0.006896551721759809]\n",
            "training on batch 450 for epoch number 0\n",
            "[0.25034675, 0.9375, 0.06666666644444444, 0.006872852231315171]\n",
            "training on batch 451 for epoch number 0\n",
            "[0.028675115, 1.0, 0.06666666644444444, 0.006872852231315171]\n",
            "training on batch 452 for epoch number 0\n",
            "[0.028668094, 1.0, 0.06666666644444444, 0.006872852231315171]\n",
            "training on batch 453 for epoch number 0\n",
            "[0.25032398, 0.9375, 0.06666666644444444, 0.0068493150661474945]\n",
            "training on batch 454 for epoch number 0\n",
            "[0.02857024, 1.0, 0.06666666644444444, 0.0068493150661474945]\n",
            "training on batch 455 for epoch number 0\n",
            "[0.028475579, 1.0, 0.06666666644444444, 0.0068493150661474945]\n",
            "training on batch 456 for epoch number 0\n",
            "[0.25063136, 0.9375, 0.06666666644444444, 0.006825938564223229]\n",
            "training on batch 457 for epoch number 0\n",
            "[0.028232245, 1.0, 0.06666666644444444, 0.006825938564223229]\n",
            "training on batch 458 for epoch number 0\n",
            "[0.028078597, 1.0, 0.06666666644444444, 0.006825938564223229]\n",
            "training on batch 459 for epoch number 0\n",
            "[0.25117427, 0.9375, 0.06666666644444444, 0.0068027210861215225]\n",
            "training on batch 460 for epoch number 0\n",
            "[0.027741373, 1.0, 0.06666666644444444, 0.0068027210861215225]\n",
            "training on batch 461 for epoch number 0\n",
            "[0.2515037, 0.9375, 0.06666666644444444, 0.006779661014650962]\n",
            "training on batch 462 for epoch number 0\n",
            "[0.027538842, 1.0, 0.06666666644444444, 0.006779661014650962]\n",
            "training on batch 463 for epoch number 0\n",
            "[0.027407454, 1.0, 0.06666666644444444, 0.006779661014650962]\n",
            "training on batch 464 for epoch number 0\n",
            "[0.2519847, 0.9375, 0.06666666644444444, 0.006756756754474068]\n",
            "training on batch 465 for epoch number 0\n",
            "[0.027116695, 1.0, 0.06666666644444444, 0.006756756754474068]\n",
            "training on batch 466 for epoch number 0\n",
            "[0.026950968, 1.0, 0.06666666644444444, 0.006756756754474068]\n",
            "training on batch 467 for epoch number 0\n",
            "[0.25262225, 0.9375, 0.06666666644444444, 0.006734006731739391]\n",
            "training on batch 468 for epoch number 0\n",
            "[0.026608622, 1.0, 0.06666666644444444, 0.006734006731739391]\n",
            "training on batch 469 for epoch number 0\n",
            "[0.70607185, 0.8125, 0.06666666644444444, 0.006666666664444444]\n",
            "training on batch 470 for epoch number 0\n",
            "[0.7027692, 0.8125, 0.06666666644444444, 0.0066006600638281645]\n",
            "training on batch 471 for epoch number 0\n",
            "[0.028160699, 1.0, 0.06666666644444444, 0.0066006600638281645]\n",
            "training on batch 472 for epoch number 0\n",
            "[0.24946856, 0.9375, 0.06666666644444444, 0.006578947366256925]\n",
            "training on batch 473 for epoch number 0\n",
            "[0.46626294, 0.875, 0.06666666644444444, 0.00653594771028237]\n",
            "training on batch 474 for epoch number 0\n",
            "[0.24672388, 0.9375, 0.06666666644444444, 0.006514657978333987]\n",
            "training on batch 475 for epoch number 0\n",
            "[0.45734057, 0.875, 0.06666666644444444, 0.006472491907290455]\n",
            "training on batch 476 for epoch number 0\n",
            "[0.24364898, 0.9375, 0.06666666644444444, 0.0064516129011446405]\n",
            "training on batch 477 for epoch number 0\n",
            "[0.03704406, 1.0, 0.06666666644444444, 0.0064516129011446405]\n",
            "training on batch 478 for epoch number 0\n",
            "[0.2410697, 0.9375, 0.06666666644444444, 0.006430868165134769]\n",
            "training on batch 479 for epoch number 0\n",
            "[0.040265508, 1.0, 0.06666666644444444, 0.006430868165134769]\n",
            "training on batch 480 for epoch number 0\n",
            "[0.63492286, 0.8125, 0.06666666644444444, 0.006369426749563876]\n",
            "training on batch 481 for epoch number 0\n",
            "[0.043587517, 1.0, 0.06666666644444444, 0.006369426749563876]\n",
            "training on batch 482 for epoch number 0\n",
            "[0.045230564, 1.0, 0.06666666644444444, 0.006369426749563876]\n",
            "training on batch 483 for epoch number 0\n",
            "[0.23705189, 0.9375, 0.06666666644444444, 0.006349206347190728]\n",
            "training on batch 484 for epoch number 0\n",
            "[0.047692657, 1.0, 0.06666666644444444, 0.006349206347190728]\n",
            "training on batch 485 for epoch number 0\n",
            "[0.23630086, 0.9375, 0.06666666644444444, 0.006329113922047748]\n",
            "training on batch 486 for epoch number 0\n",
            "[0.42268673, 0.875, 0.06666666644444444, 0.006289308174122859]\n",
            "training on batch 487 for epoch number 0\n",
            "[0.42063326, 0.875, 0.06666666644444444, 0.006249999998046874]\n",
            "training on batch 488 for epoch number 0\n",
            "[0.05237986, 1.0, 0.06666666644444444, 0.006249999998046874]\n",
            "training on batch 489 for epoch number 0\n",
            "[0.053598586, 1.0, 0.06666666644444444, 0.006249999998046874]\n",
            "training on batch 490 for epoch number 0\n",
            "[0.05432969, 1.0, 0.06666666644444444, 0.006249999998046874]\n",
            "training on batch 491 for epoch number 0\n",
            "[0.41486958, 0.875, 0.06666666644444444, 0.006211180122294664]\n",
            "training on batch 492 for epoch number 0\n",
            "[0.59303474, 0.8125, 0.06666666644444444, 0.006153846151952662]\n",
            "training on batch 493 for epoch number 0\n",
            "[0.58891165, 0.8125, 0.06666666644444444, 0.006097560973750743]\n",
            "training on batch 494 for epoch number 0\n",
            "[0.23410198, 0.9375, 0.06666666644444444, 0.006079027353775371]\n",
            "training on batch 495 for epoch number 0\n",
            "[0.23396994, 0.9375, 0.06666666644444444, 0.006060606058769513]\n",
            "training on batch 496 for epoch number 0\n",
            "[0.06435244, 1.0, 0.06666666644444444, 0.006060606058769513]\n",
            "training on batch 497 for epoch number 0\n",
            "[0.065904945, 1.0, 0.06666666644444444, 0.006060606058769513]\n",
            "training on batch 498 for epoch number 0\n",
            "[0.23393877, 0.9375, 0.06666666644444444, 0.006042296070682085]\n",
            "training on batch 499 for epoch number 0\n",
            "[0.06754587, 1.0, 0.06666666644444444, 0.006042296070682085]\n",
            "training on batch 500 for epoch number 0\n",
            "[0.06768704, 1.0, 0.06666666644444444, 0.006042296070682085]\n",
            "training on batch 501 for epoch number 0\n",
            "[0.06727268, 1.0, 0.06666666644444444, 0.006042296070682085]\n",
            "training on batch 502 for epoch number 0\n",
            "[0.23391907, 0.9375, 0.06666666644444444, 0.006024096383727681]\n",
            "training on batch 503 for epoch number 0\n",
            "[0.06557802, 1.0, 0.06666666644444444, 0.006024096383727681]\n",
            "training on batch 504 for epoch number 0\n",
            "[0.23391515, 0.9375, 0.06666666644444444, 0.0060060060042024]\n",
            "training on batch 505 for epoch number 0\n",
            "[0.06330701, 1.0, 0.06666666644444444, 0.0060060060042024]\n",
            "training on batch 506 for epoch number 0\n",
            "[0.23398328, 0.9375, 0.06666666644444444, 0.005988023950302987]\n",
            "training on batch 507 for epoch number 0\n",
            "[0.060698453, 1.0, 0.06666666644444444, 0.005988023950302987]\n",
            "training on batch 508 for epoch number 0\n",
            "[0.40912864, 0.875, 0.06666666644444444, 0.00595238095060941]\n",
            "training on batch 509 for epoch number 0\n",
            "[0.05838919, 1.0, 0.06666666644444444, 0.00595238095060941]\n",
            "training on batch 510 for epoch number 0\n",
            "[0.41147795, 0.875, 0.06666666644444444, 0.00591715976156297]\n",
            "training on batch 511 for epoch number 0\n",
            "[0.056760646, 1.0, 0.06666666644444444, 0.00591715976156297]\n",
            "training on batch 512 for epoch number 0\n",
            "[0.41315436, 0.875, 0.06666666644444444, 0.005882352939446367]\n",
            "training on batch 513 for epoch number 0\n",
            "[0.05567815, 1.0, 0.06666666644444444, 0.005882352939446367]\n",
            "training on batch 514 for epoch number 0\n",
            "[0.23465542, 0.9375, 0.06666666644444444, 0.005865102637576215]\n",
            "training on batch 515 for epoch number 0\n",
            "[0.054613233, 1.0, 0.06666666644444444, 0.005865102637576215]\n",
            "training on batch 516 for epoch number 0\n",
            "[0.053839177, 1.0, 0.06666666644444444, 0.005865102637576215]\n",
            "training on batch 517 for epoch number 0\n",
            "[0.052809466, 1.0, 0.06666666644444444, 0.005865102637576215]\n",
            "training on batch 518 for epoch number 0\n",
            "[0.051571604, 1.0, 0.06666666644444444, 0.005865102637576215]\n",
            "training on batch 519 for epoch number 0\n",
            "[0.050169755, 1.0, 0.06666666644444444, 0.005865102637576215]\n",
            "training on batch 520 for epoch number 0\n",
            "[0.048643764, 1.0, 0.06666666644444444, 0.005865102637576215]\n",
            "training on batch 521 for epoch number 0\n",
            "[0.4265046, 0.875, 0.06666666644444444, 0.005830903788387491]\n",
            "training on batch 522 for epoch number 0\n",
            "[0.23711541, 0.9375, 0.06666666644444444, 0.00581395348668199]\n",
            "training on batch 523 for epoch number 0\n",
            "[0.04533516, 1.0, 0.06666666644444444, 0.00581395348668199]\n",
            "training on batch 524 for epoch number 0\n",
            "[0.044430975, 1.0, 0.06666666644444444, 0.00581395348668199]\n",
            "training on batch 525 for epoch number 0\n",
            "[0.238246, 0.9375, 0.06666666644444444, 0.0057971014475950424]\n",
            "training on batch 526 for epoch number 0\n",
            "[0.04258829, 1.0, 0.06666666644444444, 0.0057971014475950424]\n",
            "training on batch 527 for epoch number 0\n",
            "[0.04165288, 1.0, 0.06666666644444444, 0.0057971014475950424]\n",
            "training on batch 528 for epoch number 0\n",
            "[0.23968804, 0.9375, 0.06666666644444444, 0.005780346819138628]\n",
            "training on batch 529 for epoch number 0\n",
            "[0.039812036, 1.0, 0.06666666644444444, 0.005780346819138628]\n",
            "training on batch 530 for epoch number 0\n",
            "[0.03890635, 1.0, 0.06666666644444444, 0.005780346819138628]\n",
            "training on batch 531 for epoch number 0\n",
            "[0.24136227, 0.9375, 0.06666666644444444, 0.0057636887591459105]\n",
            "training on batch 532 for epoch number 0\n",
            "[0.24187823, 0.9375, 0.06666666644444444, 0.005747126435130136]\n",
            "training on batch 533 for epoch number 0\n",
            "[0.03664034, 1.0, 0.06666666644444444, 0.005747126435130136]\n",
            "training on batch 534 for epoch number 0\n",
            "[0.035994586, 1.0, 0.06666666644444444, 0.005747126435130136]\n",
            "training on batch 535 for epoch number 0\n",
            "[0.035254788, 1.0, 0.06666666644444444, 0.005747126435130136]\n",
            "training on batch 536 for epoch number 0\n",
            "[0.034439817, 1.0, 0.06666666644444444, 0.005747126435130136]\n",
            "training on batch 537 for epoch number 0\n",
            "[0.033566434, 1.0, 0.06666666644444444, 0.005747126435130136]\n",
            "training on batch 538 for epoch number 0\n",
            "[0.45855147, 0.875, 0.06666666644444444, 0.005714285712653061]\n",
            "training on batch 539 for epoch number 0\n",
            "[0.03225575, 1.0, 0.06666666644444444, 0.005714285712653061]\n",
            "training on batch 540 for epoch number 0\n",
            "[0.031767104, 1.0, 0.06666666644444444, 0.005714285712653061]\n",
            "training on batch 541 for epoch number 0\n",
            "[0.24701937, 0.9375, 0.06666666644444444, 0.0056980056963823345]\n",
            "training on batch 542 for epoch number 0\n",
            "[0.030828906, 1.0, 0.06666666644444444, 0.0056980056963823345]\n",
            "training on batch 543 for epoch number 0\n",
            "[0.0303713, 1.0, 0.06666666644444444, 0.0056980056963823345]\n",
            "training on batch 544 for epoch number 0\n",
            "[0.029838348, 1.0, 0.06666666644444444, 0.0056980056963823345]\n",
            "training on batch 545 for epoch number 0\n",
            "[0.02924352, 1.0, 0.06666666644444444, 0.0056980056963823345]\n",
            "training on batch 546 for epoch number 0\n",
            "[0.24992684, 0.9375, 0.06666666644444444, 0.005681818180204029]\n",
            "training on batch 547 for epoch number 0\n",
            "[0.2504539, 0.9375, 0.06666666644444444, 0.005665722377998379]\n",
            "training on batch 548 for epoch number 0\n",
            "[0.027927652, 1.0, 0.06666666644444444, 0.005665722377998379]\n",
            "training on batch 549 for epoch number 0\n",
            "[0.027601505, 1.0, 0.06666666644444444, 0.005665722377998379]\n",
            "training on batch 550 for epoch number 0\n",
            "[0.25168917, 0.9375, 0.06666666644444444, 0.005649717512528328]\n",
            "training on batch 551 for epoch number 0\n",
            "[0.026988406, 1.0, 0.06666666644444444, 0.005649717512528328]\n",
            "training on batch 552 for epoch number 0\n",
            "[0.0266927, 1.0, 0.06666666644444444, 0.005649717512528328]\n",
            "training on batch 553 for epoch number 0\n",
            "[0.026326457, 1.0, 0.06666666644444444, 0.005649717512528328]\n",
            "training on batch 554 for epoch number 0\n",
            "[0.025900615, 1.0, 0.06666666644444444, 0.005649717512528328]\n",
            "training on batch 555 for epoch number 0\n",
            "[0.025425386, 1.0, 0.06666666644444444, 0.005649717512528328]\n",
            "training on batch 556 for epoch number 0\n",
            "[0.024910346, 1.0, 0.06666666644444444, 0.005649717512528328]\n",
            "training on batch 557 for epoch number 0\n",
            "[0.024363961, 1.0, 0.06666666644444444, 0.005649717512528328]\n",
            "training on batch 558 for epoch number 0\n",
            "[0.023793925, 1.0, 0.06666666644444444, 0.005649717512528328]\n",
            "training on batch 559 for epoch number 0\n",
            "[0.25775906, 0.9375, 0.06666666644444444, 0.005633802815314421]\n",
            "training on batch 560 for epoch number 0\n",
            "[0.022827987, 1.0, 0.06666666644444444, 0.005633802815314421]\n",
            "training on batch 561 for epoch number 0\n",
            "[0.25916135, 0.9375, 0.06666666644444444, 0.0056179775265118035]\n",
            "training on batch 562 for epoch number 0\n",
            "[0.7343495, 0.8125, 0.06666666644444444, 0.005571030639116704]\n",
            "training on batch 563 for epoch number 0\n",
            "[0.2588822, 0.9375, 0.06666666644444444, 0.005555555554012346]\n",
            "training on batch 564 for epoch number 0\n",
            "[0.25802362, 0.9375, 0.06666666644444444, 0.005540166203451477]\n",
            "training on batch 565 for epoch number 0\n",
            "[0.02365701, 1.0, 0.06666666644444444, 0.005540166203451477]\n",
            "training on batch 566 for epoch number 0\n",
            "[0.024111789, 1.0, 0.06666666644444444, 0.005540166203451477]\n",
            "training on batch 567 for epoch number 0\n",
            "[0.024432901, 1.0, 0.06666666644444444, 0.005540166203451477]\n",
            "training on batch 568 for epoch number 0\n",
            "[0.02463116, 1.0, 0.06666666644444444, 0.005540166203451477]\n",
            "training on batch 569 for epoch number 0\n",
            "[0.48585865, 0.875, 0.06666666644444444, 0.005509641871760429]\n",
            "training on batch 570 for epoch number 0\n",
            "[0.025176182, 1.0, 0.06666666644444444, 0.005509641871760429]\n",
            "training on batch 571 for epoch number 0\n",
            "[0.2540954, 0.9375, 0.06666666644444444, 0.005494505492996015]\n",
            "training on batch 572 for epoch number 0\n",
            "[0.025936706, 1.0, 0.06666666644444444, 0.005494505492996015]\n",
            "training on batch 573 for epoch number 0\n",
            "[0.25302914, 0.9375, 0.06666666644444444, 0.0054794520532933004]\n",
            "training on batch 574 for epoch number 0\n",
            "[0.252451, 0.9375, 0.06666666644444444, 0.005464480872823912]\n",
            "training on batch 575 for epoch number 0\n",
            "[0.2517467, 0.9375, 0.06666666644444444, 0.005449591279169048]\n",
            "training on batch 576 for epoch number 0\n",
            "[0.6972559, 0.8125, 0.06666666644444444, 0.005405405403944485]\n",
            "training on batch 577 for epoch number 0\n",
            "[0.029009335, 1.0, 0.06666666644444444, 0.005405405403944485]\n",
            "training on batch 578 for epoch number 0\n",
            "[0.248301, 0.9375, 0.06666666644444444, 0.005390835578061769]\n",
            "training on batch 579 for epoch number 0\n",
            "[0.24715269, 0.9375, 0.06666666644444444, 0.005376344084576251]\n",
            "training on batch 580 for epoch number 0\n",
            "[0.4598069, 0.875, 0.06666666644444444, 0.005347593581457862]\n",
            "training on batch 581 for epoch number 0\n",
            "[0.033708043, 1.0, 0.06666666644444444, 0.005347593581457862]\n",
            "training on batch 582 for epoch number 0\n",
            "[0.2436406, 0.9375, 0.06666666644444444, 0.005333333331911111]\n",
            "training on batch 583 for epoch number 0\n",
            "[0.03615297, 1.0, 0.06666666644444444, 0.005333333331911111]\n",
            "training on batch 584 for epoch number 0\n",
            "[0.03711771, 1.0, 0.06666666644444444, 0.005333333331911111]\n",
            "training on batch 585 for epoch number 0\n",
            "[0.24142131, 0.9375, 0.06666666644444444, 0.005319148934755545]\n",
            "training on batch 586 for epoch number 0\n",
            "[0.2408989, 0.9375, 0.06666666644444444, 0.005305039786391236]\n",
            "training on batch 587 for epoch number 0\n",
            "[0.039412327, 1.0, 0.06666666644444444, 0.005305039786391236]\n",
            "training on batch 588 for epoch number 0\n",
            "[0.039966647, 1.0, 0.06666666644444444, 0.005305039786391236]\n",
            "training on batch 589 for epoch number 0\n",
            "[0.040267687, 1.0, 0.06666666644444444, 0.005305039786391236]\n",
            "training on batch 590 for epoch number 0\n",
            "[0.040337905, 1.0, 0.06666666644444444, 0.005305039786391236]\n",
            "training on batch 591 for epoch number 0\n",
            "[0.43953434, 0.875, 0.06666666644444444, 0.005277044853488906]\n",
            "training on batch 592 for epoch number 0\n",
            "[0.040517006, 1.0, 0.06666666644444444, 0.005277044853488906]\n",
            "training on batch 593 for epoch number 0\n",
            "[0.040600844, 1.0, 0.06666666644444444, 0.005277044853488906]\n",
            "training on batch 594 for epoch number 0\n",
            "[0.04047449, 1.0, 0.06666666644444444, 0.005277044853488906]\n",
            "training on batch 595 for epoch number 0\n",
            "[0.040161267, 1.0, 0.06666666644444444, 0.005277044853488906]\n",
            "training on batch 596 for epoch number 0\n",
            "[0.039686017, 1.0, 0.06666666644444444, 0.005277044853488906]\n",
            "training on batch 597 for epoch number 0\n",
            "[0.039073344, 1.0, 0.06666666644444444, 0.005277044853488906]\n",
            "training on batch 598 for epoch number 0\n",
            "[0.4437261, 0.875, 0.06666666644444444, 0.005249343830643217]\n",
            "training on batch 599 for epoch number 0\n",
            "[0.038121626, 1.0, 0.06666666644444444, 0.005249343830643217]\n",
            "training on batch 600 for epoch number 0\n",
            "[0.2414324, 0.9375, 0.06666666644444444, 0.005235602092870261]\n",
            "training on batch 601 for epoch number 0\n",
            "[0.03753157, 1.0, 0.06666666644444444, 0.005235602092870261]\n",
            "training on batch 602 for epoch number 0\n",
            "[0.24183038, 0.9375, 0.06666666644444444, 0.005221932113519077]\n",
            "training on batch 603 for epoch number 0\n",
            "[0.03696126, 1.0, 0.06666666644444444, 0.005221932113519077]\n",
            "training on batch 604 for epoch number 0\n",
            "[0.036608826, 1.0, 0.06666666644444444, 0.005221932113519077]\n",
            "training on batch 605 for epoch number 0\n",
            "[0.036126953, 1.0, 0.06666666644444444, 0.005221932113519077]\n",
            "training on batch 606 for epoch number 0\n",
            "[0.03553543, 1.0, 0.06666666644444444, 0.005221932113519077]\n",
            "training on batch 607 for epoch number 0\n",
            "[0.034852862, 1.0, 0.06666666644444444, 0.005221932113519077]\n",
            "training on batch 608 for epoch number 0\n",
            "[0.03409702, 1.0, 0.06666666644444444, 0.005221932113519077]\n",
            "training on batch 609 for epoch number 0\n",
            "[0.24497911, 0.9375, 0.06666666644444444, 0.005208333331976996]\n",
            "training on batch 610 for epoch number 0\n",
            "[0.032694902, 1.0, 0.06666666644444444, 0.005208333331976996]\n",
            "training on batch 611 for epoch number 0\n",
            "[0.24615929, 0.9375, 0.06666666644444444, 0.005194805193455895]\n",
            "training on batch 612 for epoch number 0\n",
            "[0.03158381, 1.0, 0.06666666644444444, 0.005194805193455895]\n",
            "training on batch 613 for epoch number 0\n",
            "[0.031051962, 1.0, 0.06666666644444444, 0.005194805193455895]\n",
            "training on batch 614 for epoch number 0\n",
            "[0.46511033, 0.875, 0.06666666644444444, 0.0051679586549953594]\n",
            "training on batch 615 for epoch number 0\n",
            "[0.030317511, 1.0, 0.06666666644444444, 0.0051679586549953594]\n",
            "training on batch 616 for epoch number 0\n",
            "[0.030073076, 1.0, 0.06666666644444444, 0.0051679586549953594]\n",
            "training on batch 617 for epoch number 0\n",
            "[0.46739882, 0.875, 0.06666666644444444, 0.005141388173485504]\n",
            "training on batch 618 for epoch number 0\n",
            "[0.029822465, 1.0, 0.06666666644444444, 0.005141388173485504]\n",
            "training on batch 619 for epoch number 0\n",
            "[0.46722907, 0.875, 0.06666666644444444, 0.005115089512758289]\n",
            "training on batch 620 for epoch number 0\n",
            "[0.030141875, 1.0, 0.06666666644444444, 0.005115089512758289]\n",
            "training on batch 621 for epoch number 0\n",
            "[0.03034114, 1.0, 0.06666666644444444, 0.005115089512758289]\n",
            "training on batch 622 for epoch number 0\n",
            "[0.24782577, 0.9375, 0.06666666644444444, 0.005102040815024989]\n",
            "training on batch 623 for epoch number 0\n",
            "[0.46467537, 0.875, 0.06666666644444444, 0.005076142130691334]\n",
            "training on batch 624 for epoch number 0\n",
            "[0.031145476, 1.0, 0.06666666644444444, 0.005076142130691334]\n",
            "training on batch 625 for epoch number 0\n",
            "[0.031529658, 1.0, 0.06666666644444444, 0.005076142130691334]\n",
            "training on batch 626 for epoch number 0\n",
            "[0.46110088, 0.875, 0.06666666644444444, 0.00505050504922967]\n",
            "training on batch 627 for epoch number 0\n",
            "[0.032340646, 1.0, 0.06666666644444444, 0.00505050504922967]\n",
            "training on batch 628 for epoch number 0\n",
            "[0.032744206, 1.0, 0.06666666644444444, 0.00505050504922967]\n",
            "training on batch 629 for epoch number 0\n",
            "[0.03296488, 1.0, 0.06666666644444444, 0.00505050504922967]\n",
            "training on batch 630 for epoch number 0\n",
            "[0.033020504, 1.0, 0.06666666644444444, 0.00505050504922967]\n",
            "training on batch 631 for epoch number 0\n",
            "[0.032927945, 1.0, 0.06666666644444444, 0.00505050504922967]\n",
            "training on batch 632 for epoch number 0\n",
            "[0.03270464, 1.0, 0.06666666644444444, 0.00505050504922967]\n",
            "training on batch 633 for epoch number 0\n",
            "[0.03236714, 1.0, 0.06666666644444444, 0.00505050504922967]\n",
            "training on batch 634 for epoch number 0\n",
            "[0.6748313, 0.8125, 0.06666666644444444, 0.005012531327064528]\n",
            "training on batch 635 for epoch number 0\n",
            "[0.24594963, 0.9375, 0.06666666644444444, 0.00499999999875]\n",
            "training on batch 636 for epoch number 0\n",
            "[0.24557577, 0.9375, 0.06666666644444444, 0.004987531170826052]\n",
            "training on batch 637 for epoch number 0\n",
            "[0.4571539, 0.875, 0.06666666644444444, 0.004962779155096084]\n",
            "training on batch 638 for epoch number 0\n",
            "[0.033953365, 1.0, 0.06666666644444444, 0.004962779155096084]\n",
            "training on batch 639 for epoch number 0\n",
            "[0.03458494, 1.0, 0.06666666644444444, 0.004962779155096084]\n",
            "training on batch 640 for epoch number 0\n",
            "[0.0349987, 1.0, 0.06666666644444444, 0.004962779155096084]\n",
            "training on batch 641 for epoch number 0\n",
            "[0.45139804, 0.875, 0.06666666644444444, 0.004938271603718945]\n",
            "training on batch 642 for epoch number 0\n",
            "[0.24281684, 0.9375, 0.06666666644444444, 0.004926108373170909]\n",
            "training on batch 643 for epoch number 0\n",
            "[0.24229437, 0.9375, 0.06666666644444444, 0.004914004912797541]\n",
            "training on batch 644 for epoch number 0\n",
            "[0.037266526, 1.0, 0.06666666644444444, 0.004914004912797541]\n",
            "training on batch 645 for epoch number 0\n",
            "[0.44500917, 0.875, 0.06666666644444444, 0.004889975548926656]\n",
            "training on batch 646 for epoch number 0\n",
            "[0.24079916, 0.9375, 0.06666666644444444, 0.004878048779298037]\n",
            "training on batch 647 for epoch number 0\n",
            "[0.24021153, 0.9375, 0.06666666644444444, 0.0048661800474778145]\n",
            "training on batch 648 for epoch number 0\n",
            "[0.23963551, 0.9375, 0.06666666644444444, 0.0048543689308605895]\n",
            "training on batch 649 for epoch number 0\n",
            "[0.43654716, 0.875, 0.06666666644444444, 0.004830917873229246]\n",
            "training on batch 650 for epoch number 0\n",
            "[0.042990156, 1.0, 0.06666666644444444, 0.004830917873229246]\n",
            "training on batch 651 for epoch number 0\n",
            "[0.6256456, 0.8125, 0.06666666644444444, 0.004796163068394205]\n",
            "training on batch 652 for epoch number 0\n",
            "[0.42856795, 0.875, 0.06666666644444444, 0.0047732696885982645]\n",
            "training on batch 653 for epoch number 0\n",
            "[0.42499718, 0.875, 0.06666666644444444, 0.0047505938230996215]\n",
            "training on batch 654 for epoch number 0\n",
            "[0.42116955, 0.875, 0.06666666644444444, 0.0047281323865890935]\n",
            "training on batch 655 for epoch number 0\n",
            "[0.053036373, 1.0, 0.06666666644444444, 0.0047281323865890935]\n",
            "training on batch 656 for epoch number 0\n",
            "[0.5936562, 0.8125, 0.06666666644444444, 0.0046948356796491]\n",
            "training on batch 657 for epoch number 0\n",
            "[0.41049775, 0.875, 0.06666666644444444, 0.004672897195169883]\n",
            "training on batch 658 for epoch number 0\n",
            "[0.061364107, 1.0, 0.06666666644444444, 0.004672897195169883]\n",
            "training on batch 659 for epoch number 0\n",
            "[0.40410516, 0.875, 0.06666666644444444, 0.004651162789616008]\n",
            "training on batch 660 for epoch number 0\n",
            "[0.2340217, 0.9375, 0.06666666644444444, 0.004640371228621723]\n",
            "training on batch 661 for epoch number 0\n",
            "[0.2341332, 0.9375, 0.06666666644444444, 0.004629629628557956]\n",
            "training on batch 662 for epoch number 0\n",
            "[0.071625024, 1.0, 0.06666666644444444, 0.004629629628557956]\n",
            "training on batch 663 for epoch number 0\n",
            "[0.39570588, 0.875, 0.06666666644444444, 0.004608294929813757]\n",
            "training on batch 664 for epoch number 0\n",
            "[0.23466267, 0.9375, 0.06666666644444444, 0.0045977011483683446]\n",
            "training on batch 665 for epoch number 0\n",
            "[0.3930647, 0.875, 0.06666666644444444, 0.004576659037854311]\n",
            "training on batch 666 for epoch number 0\n",
            "[0.7048191, 0.75, 0.06666666644444444, 0.004535147391261871]\n",
            "training on batch 667 for epoch number 0\n",
            "[0.08201172, 1.0, 0.06666666644444444, 0.004535147391261871]\n",
            "training on batch 668 for epoch number 0\n",
            "[0.23635124, 0.9375, 0.06666666644444444, 0.004524886876804324]\n",
            "training on batch 669 for epoch number 0\n",
            "[0.6883083, 0.75, 0.06666666644444444, 0.004484304931729976]\n",
            "training on batch 670 for epoch number 0\n",
            "[0.08969615, 1.0, 0.06666666644444444, 0.004484304931729976]\n",
            "training on batch 671 for epoch number 0\n",
            "[0.23830266, 0.9375, 0.06666666644444444, 0.004474272929647813]\n",
            "training on batch 672 for epoch number 0\n",
            "[0.38393515, 0.875, 0.06666666644444444, 0.0044543429834177405]\n",
            "training on batch 673 for epoch number 0\n",
            "[0.3831627, 0.875, 0.06666666644444444, 0.0044345897994601795]\n",
            "training on batch 674 for epoch number 0\n",
            "[0.24007194, 0.9375, 0.06666666644444444, 0.004424778760083014]\n",
            "training on batch 675 for epoch number 0\n",
            "[0.52306765, 0.8125, 0.06666666644444444, 0.004395604394638328]\n",
            "training on batch 676 for epoch number 0\n",
            "[0.3810768, 0.875, 0.06666666644444444, 0.00437636761392202]\n",
            "training on batch 677 for epoch number 0\n",
            "[0.10442829, 1.0, 0.06666666644444444, 0.00437636761392202]\n",
            "training on batch 678 for epoch number 0\n",
            "[0.3800492, 0.875, 0.06666666644444444, 0.004357298473996231]\n",
            "training on batch 679 for epoch number 0\n",
            "[0.106941804, 1.0, 0.06666666644444444, 0.004357298473996231]\n",
            "training on batch 680 for epoch number 0\n",
            "[0.24335201, 0.9375, 0.06666666644444444, 0.004347826086011342]\n",
            "training on batch 681 for epoch number 0\n",
            "[0.2432335, 0.9375, 0.06666666644444444, 0.004338394792985163]\n",
            "training on batch 682 for epoch number 0\n",
            "[0.105954595, 1.0, 0.06666666644444444, 0.004338394792985163]\n",
            "training on batch 683 for epoch number 0\n",
            "[0.24240363, 0.9375, 0.06666666644444444, 0.004329004328067315]\n",
            "training on batch 684 for epoch number 0\n",
            "[0.10250646, 1.0, 0.06666666644444444, 0.004329004328067315]\n",
            "training on batch 685 for epoch number 0\n",
            "[0.099999666, 1.0, 0.06666666644444444, 0.004329004328067315]\n",
            "training on batch 686 for epoch number 0\n",
            "[0.09694494, 1.0, 0.06666666644444444, 0.004329004328067315]\n",
            "training on batch 687 for epoch number 0\n",
            "[0.23901758, 0.9375, 0.06666666644444444, 0.004319654426712817]\n",
            "training on batch 688 for epoch number 0\n",
            "[0.09028128, 1.0, 0.06666666644444444, 0.004319654426712817]\n",
            "training on batch 689 for epoch number 0\n",
            "[0.08680864, 1.0, 0.06666666644444444, 0.004319654426712817]\n",
            "training on batch 690 for epoch number 0\n",
            "[0.08317064, 1.0, 0.06666666644444444, 0.004319654426712817]\n",
            "training on batch 691 for epoch number 0\n",
            "[0.079452835, 1.0, 0.06666666644444444, 0.004319654426712817]\n",
            "training on batch 692 for epoch number 0\n",
            "[0.23538336, 0.9375, 0.06666666644444444, 0.004310344826657253]\n",
            "training on batch 693 for epoch number 0\n",
            "[0.07245546, 1.0, 0.06666666644444444, 0.004310344826657253]\n",
            "training on batch 694 for epoch number 0\n",
            "[0.06918973, 1.0, 0.06666666644444444, 0.004310344826657253]\n",
            "training on batch 695 for epoch number 0\n",
            "[0.06596511, 1.0, 0.06666666644444444, 0.004310344826657253]\n",
            "training on batch 696 for epoch number 0\n",
            "[0.062813364, 1.0, 0.06666666644444444, 0.004310344826657253]\n",
            "training on batch 697 for epoch number 0\n",
            "[0.23496681, 0.9375, 0.06666666644444444, 0.004301075267892242]\n",
            "training on batch 698 for epoch number 0\n",
            "[0.057162832, 1.0, 0.06666666644444444, 0.004301075267892242]\n",
            "training on batch 699 for epoch number 0\n",
            "[0.05462897, 1.0, 0.06666666644444444, 0.004301075267892242]\n",
            "training on batch 700 for epoch number 0\n",
            "[0.05217048, 1.0, 0.06666666644444444, 0.004301075267892242]\n",
            "training on batch 701 for epoch number 0\n",
            "[0.04980029, 1.0, 0.06666666644444444, 0.004301075267892242]\n",
            "training on batch 702 for epoch number 0\n",
            "[0.047526136, 1.0, 0.06666666644444444, 0.004301075267892242]\n",
            "training on batch 703 for epoch number 0\n",
            "[0.045353528, 1.0, 0.06666666644444444, 0.004301075267892242]\n",
            "training on batch 704 for epoch number 0\n",
            "[0.23926087, 0.9375, 0.06666666644444444, 0.004291845492641235]\n",
            "training on batch 705 for epoch number 0\n",
            "[0.04158387, 1.0, 0.06666666644444444, 0.004291845492641235]\n",
            "training on batch 706 for epoch number 0\n",
            "[0.039941173, 1.0, 0.06666666644444444, 0.004291845492641235]\n",
            "training on batch 707 for epoch number 0\n",
            "[0.03835195, 1.0, 0.06666666644444444, 0.004291845492641235]\n",
            "training on batch 708 for epoch number 0\n",
            "[0.24294789, 0.9375, 0.06666666644444444, 0.00428265524533562]\n",
            "training on batch 709 for epoch number 0\n",
            "[0.0356043, 1.0, 0.06666666644444444, 0.00428265524533562]\n",
            "training on batch 710 for epoch number 0\n",
            "[0.034405947, 1.0, 0.06666666644444444, 0.00428265524533562]\n",
            "training on batch 711 for epoch number 0\n",
            "[0.033227667, 1.0, 0.06666666644444444, 0.00428265524533562]\n",
            "training on batch 712 for epoch number 0\n",
            "[0.24677491, 0.9375, 0.06666666644444444, 0.004273504272591131]\n",
            "training on batch 713 for epoch number 0\n",
            "[0.031194812, 1.0, 0.06666666644444444, 0.004273504272591131]\n",
            "training on batch 714 for epoch number 0\n",
            "[0.030308662, 1.0, 0.06666666644444444, 0.004273504272591131]\n",
            "training on batch 715 for epoch number 0\n",
            "[0.029421143, 1.0, 0.06666666644444444, 0.004273504272591131]\n",
            "training on batch 716 for epoch number 0\n",
            "[0.028542228, 1.0, 0.06666666644444444, 0.004273504272591131]\n",
            "training on batch 717 for epoch number 0\n",
            "[0.25154746, 0.9375, 0.06666666644444444, 0.004264392323184565]\n",
            "training on batch 718 for epoch number 0\n",
            "[0.2523071, 0.9375, 0.06666666644444444, 0.0042553191480307825]\n",
            "training on batch 719 for epoch number 0\n",
            "[0.026636444, 1.0, 0.06666666644444444, 0.0042553191480307825]\n",
            "training on batch 720 for epoch number 0\n",
            "[0.026174417, 1.0, 0.06666666644444444, 0.0042553191480307825]\n",
            "training on batch 721 for epoch number 0\n",
            "[0.025661752, 1.0, 0.06666666644444444, 0.0042553191480307825]\n",
            "training on batch 722 for epoch number 0\n",
            "[0.025123991, 1.0, 0.06666666644444444, 0.0042553191480307825]\n",
            "training on batch 723 for epoch number 0\n",
            "[0.024568928, 1.0, 0.06666666644444444, 0.0042553191480307825]\n",
            "training on batch 724 for epoch number 0\n",
            "[0.25665724, 0.9375, 0.06666666644444444, 0.004246284500160024]\n",
            "training on batch 725 for epoch number 0\n",
            "[0.023633821, 1.0, 0.06666666644444444, 0.004246284500160024]\n",
            "training on batch 726 for epoch number 0\n",
            "[0.023236614, 1.0, 0.06666666644444444, 0.004246284500160024]\n",
            "training on batch 727 for epoch number 0\n",
            "[0.02280798, 1.0, 0.06666666644444444, 0.004246284500160024]\n",
            "training on batch 728 for epoch number 0\n",
            "[0.022358663, 1.0, 0.06666666644444444, 0.004246284500160024]\n",
            "training on batch 729 for epoch number 0\n",
            "[0.2602815, 0.9375, 0.06666666644444444, 0.00423728813469549]\n",
            "training on batch 730 for epoch number 0\n",
            "[0.021616407, 1.0, 0.06666666644444444, 0.00423728813469549]\n",
            "training on batch 731 for epoch number 0\n",
            "[0.5013855, 0.875, 0.06666666644444444, 0.004219409281810251]\n",
            "training on batch 732 for epoch number 0\n",
            "[0.021408545, 1.0, 0.06666666644444444, 0.004219409281810251]\n",
            "training on batch 733 for epoch number 0\n",
            "[0.26119798, 0.9375, 0.06666666644444444, 0.004210526314903047]\n",
            "training on batch 734 for epoch number 0\n",
            "[0.021574996, 1.0, 0.06666666644444444, 0.004210526314903047]\n",
            "training on batch 735 for epoch number 0\n",
            "[0.73954123, 0.8125, 0.06666666644444444, 0.004184100417534707]\n",
            "training on batch 736 for epoch number 0\n",
            "[0.25989524, 0.9375, 0.06666666644444444, 0.004175365343595957]\n",
            "training on batch 737 for epoch number 0\n",
            "[0.023115654, 1.0, 0.06666666644444444, 0.004175365343595957]\n",
            "training on batch 738 for epoch number 0\n",
            "[0.257968, 0.9375, 0.06666666644444444, 0.004166666665798611]\n",
            "training on batch 739 for epoch number 0\n",
            "[0.024213143, 1.0, 0.06666666644444444, 0.004166666665798611]\n",
            "training on batch 740 for epoch number 0\n",
            "[0.02457168, 1.0, 0.06666666644444444, 0.004166666665798611]\n",
            "training on batch 741 for epoch number 0\n",
            "[0.255863, 0.9375, 0.06666666644444444, 0.004158004157139708]\n",
            "training on batch 742 for epoch number 0\n",
            "[0.025133803, 1.0, 0.06666666644444444, 0.004158004157139708]\n",
            "training on batch 743 for epoch number 0\n",
            "[0.025352422, 1.0, 0.06666666644444444, 0.004158004157139708]\n",
            "training on batch 744 for epoch number 0\n",
            "[0.48388356, 0.875, 0.06666666644444444, 0.0041407867486250955]\n",
            "training on batch 745 for epoch number 0\n",
            "[0.025952922, 1.0, 0.06666666644444444, 0.0041407867486250955]\n",
            "training on batch 746 for epoch number 0\n",
            "[0.25337565, 0.9375, 0.06666666644444444, 0.004132231404104911]\n",
            "training on batch 747 for epoch number 0\n",
            "[0.026754437, 1.0, 0.06666666644444444, 0.004132231404104911]\n",
            "training on batch 748 for epoch number 0\n",
            "[0.027033359, 1.0, 0.06666666644444444, 0.004132231404104911]\n",
            "training on batch 749 for epoch number 0\n",
            "[0.02716422, 1.0, 0.06666666644444444, 0.004132231404104911]\n",
            "training on batch 750 for epoch number 0\n",
            "[0.027180297, 1.0, 0.06666666644444444, 0.004132231404104911]\n",
            "training on batch 751 for epoch number 0\n",
            "[0.2521153, 0.9375, 0.06666666644444444, 0.004123711339355935]\n",
            "training on batch 752 for epoch number 0\n",
            "[0.027167015, 1.0, 0.06666666644444444, 0.004123711339355935]\n",
            "training on batch 753 for epoch number 0\n",
            "[0.027138269, 1.0, 0.06666666644444444, 0.004123711339355935]\n",
            "training on batch 754 for epoch number 0\n",
            "[0.027019395, 1.0, 0.06666666644444444, 0.004123711339355935]\n",
            "training on batch 755 for epoch number 0\n",
            "[0.47803727, 0.875, 0.06666666644444444, 0.004106776179854871]\n",
            "training on batch 756 for epoch number 0\n",
            "[0.027018547, 1.0, 0.06666666644444444, 0.004106776179854871]\n",
            "training on batch 757 for epoch number 0\n",
            "[0.027109167, 1.0, 0.06666666644444444, 0.004106776179854871]\n",
            "training on batch 758 for epoch number 0\n",
            "[0.027076779, 1.0, 0.06666666644444444, 0.004106776179854871]\n",
            "training on batch 759 for epoch number 0\n",
            "[0.02694039, 1.0, 0.06666666644444444, 0.004106776179854871]\n",
            "training on batch 760 for epoch number 0\n",
            "[0.026726345, 1.0, 0.06666666644444444, 0.004106776179854871]\n",
            "training on batch 761 for epoch number 0\n",
            "[0.4792957, 0.875, 0.06666666644444444, 0.004089979549265853]\n",
            "training on batch 762 for epoch number 0\n",
            "[0.026565818, 1.0, 0.06666666644444444, 0.004089979549265853]\n",
            "training on batch 763 for epoch number 0\n",
            "[0.25259763, 0.9375, 0.06666666644444444, 0.004081632652228238]\n",
            "training on batch 764 for epoch number 0\n",
            "[0.02676875, 1.0, 0.06666666644444444, 0.004081632652228238]\n",
            "training on batch 765 for epoch number 0\n",
            "[0.026799513, 1.0, 0.06666666644444444, 0.004081632652228238]\n",
            "training on batch 766 for epoch number 0\n",
            "[0.47808927, 0.875, 0.06666666644444444, 0.0040650406495802765]\n",
            "training on batch 767 for epoch number 0\n",
            "[0.027042907, 1.0, 0.06666666644444444, 0.0040650406495802765]\n",
            "training on batch 768 for epoch number 0\n",
            "[0.027220998, 1.0, 0.06666666644444444, 0.0040650406495802765]\n",
            "training on batch 769 for epoch number 0\n",
            "[0.25167045, 0.9375, 0.06666666644444444, 0.004056795131022963]\n",
            "training on batch 770 for epoch number 0\n",
            "[0.02742517, 1.0, 0.06666666644444444, 0.004056795131022963]\n",
            "training on batch 771 for epoch number 0\n",
            "[0.25137246, 0.9375, 0.06666666644444444, 0.004048582995131866]\n",
            "training on batch 772 for epoch number 0\n",
            "[0.2511354, 0.9375, 0.06666666644444444, 0.004040404039587797]\n",
            "training on batch 773 for epoch number 0\n",
            "[0.027954936, 1.0, 0.06666666644444444, 0.004040404039587797]\n",
            "training on batch 774 for epoch number 0\n",
            "[0.028106172, 1.0, 0.06666666644444444, 0.004040404039587797]\n",
            "training on batch 775 for epoch number 0\n",
            "[0.2505205, 0.9375, 0.06666666644444444, 0.004032258063703173]\n",
            "training on batch 776 for epoch number 0\n",
            "[0.25031945, 0.9375, 0.06666666644444444, 0.004024144868405605]\n",
            "training on batch 777 for epoch number 0\n",
            "[0.24996677, 0.9375, 0.06666666644444444, 0.004016064256221674]\n",
            "training on batch 778 for epoch number 0\n",
            "[0.24950045, 0.9375, 0.06666666644444444, 0.004008016031260919]\n",
            "training on batch 779 for epoch number 0\n",
            "[0.02951735, 1.0, 0.06666666644444444, 0.004008016031260919]\n",
            "training on batch 780 for epoch number 0\n",
            "[0.029819213, 1.0, 0.06666666644444444, 0.004008016031260919]\n",
            "training on batch 781 for epoch number 0\n",
            "[0.029926833, 1.0, 0.06666666644444444, 0.004008016031260919]\n",
            "training on batch 782 for epoch number 0\n",
            "[0.24843918, 0.9375, 0.06666666644444444, 0.003999999999199999]\n",
            "training on batch 783 for epoch number 0\n",
            "[0.03002554, 1.0, 0.06666666644444444, 0.003999999999199999]\n",
            "training on batch 784 for epoch number 0\n",
            "[0.030021168, 1.0, 0.06666666644444444, 0.003999999999199999]\n",
            "training on batch 785 for epoch number 0\n",
            "[0.46699643, 0.875, 0.06666666644444444, 0.003984063744226282]\n",
            "training on batch 786 for epoch number 0\n",
            "[0.030203205, 1.0, 0.06666666644444444, 0.003984063744226282]\n",
            "training on batch 787 for epoch number 0\n",
            "[0.030356852, 1.0, 0.06666666644444444, 0.003984063744226282]\n",
            "training on batch 788 for epoch number 0\n",
            "[0.24788865, 0.9375, 0.06666666644444444, 0.003976143140362595]\n",
            "training on batch 789 for epoch number 0\n",
            "[0.030506697, 1.0, 0.06666666644444444, 0.003976143140362595]\n",
            "training on batch 790 for epoch number 0\n",
            "[0.46494332, 0.875, 0.06666666644444444, 0.003960396038819723]\n",
            "training on batch 791 for epoch number 0\n",
            "[0.030934243, 1.0, 0.06666666644444444, 0.003960396038819723]\n",
            "training on batch 792 for epoch number 0\n",
            "[0.24704753, 0.9375, 0.06666666644444444, 0.003952569169179334]\n",
            "training on batch 793 for epoch number 0\n",
            "[0.031530607, 1.0, 0.06666666644444444, 0.003952569169179334]\n",
            "training on batch 794 for epoch number 0\n",
            "[0.031671327, 1.0, 0.06666666644444444, 0.003952569169179334]\n",
            "training on batch 795 for epoch number 0\n",
            "[0.031642977, 1.0, 0.06666666644444444, 0.003952569169179334]\n",
            "training on batch 796 for epoch number 0\n",
            "[0.031496488, 1.0, 0.06666666644444444, 0.003952569169179334]\n",
            "training on batch 797 for epoch number 0\n",
            "[0.03126, 1.0, 0.06666666644444444, 0.003952569169179334]\n",
            "training on batch 798 for epoch number 0\n",
            "[0.03094741, 1.0, 0.06666666644444444, 0.003952569169179334]\n",
            "training on batch 799 for epoch number 0\n",
            "[0.24792875, 0.9375, 0.06666666644444444, 0.003944773174764344]\n",
            "training on batch 800 for epoch number 0\n",
            "[0.030342843, 1.0, 0.06666666644444444, 0.003944773174764344]\n",
            "training on batch 801 for epoch number 0\n",
            "[0.030042093, 1.0, 0.06666666644444444, 0.003944773174764344]\n",
            "training on batch 802 for epoch number 0\n",
            "[0.24887033, 0.9375, 0.06666666644444444, 0.003937007873240746]\n",
            "training on batch 803 for epoch number 0\n",
            "[0.029474497, 1.0, 0.06666666644444444, 0.003937007873240746]\n",
            "training on batch 804 for epoch number 0\n",
            "[0.029198974, 1.0, 0.06666666644444444, 0.003937007873240746]\n",
            "training on batch 805 for epoch number 0\n",
            "[0.028844465, 1.0, 0.06666666644444444, 0.003937007873240746]\n",
            "training on batch 806 for epoch number 0\n",
            "[0.028425379, 1.0, 0.06666666644444444, 0.003937007873240746]\n",
            "training on batch 807 for epoch number 0\n",
            "[0.02795743, 1.0, 0.06666666644444444, 0.003937007873240746]\n",
            "training on batch 808 for epoch number 0\n",
            "[0.027451746, 1.0, 0.06666666644444444, 0.003937007873240746]\n",
            "training on batch 809 for epoch number 0\n",
            "[0.25238568, 0.9375, 0.06666666644444444, 0.003929273083707412]\n",
            "training on batch 810 for epoch number 0\n",
            "[0.026565813, 1.0, 0.06666666644444444, 0.003929273083707412]\n",
            "training on batch 811 for epoch number 0\n",
            "[0.026173461, 1.0, 0.06666666644444444, 0.003929273083707412]\n",
            "training on batch 812 for epoch number 0\n",
            "[0.025739294, 1.0, 0.06666666644444444, 0.003929273083707412]\n",
            "training on batch 813 for epoch number 0\n",
            "[0.25468755, 0.9375, 0.06666666644444444, 0.003921568626682045]\n",
            "training on batch 814 for epoch number 0\n",
            "[0.024987448, 1.0, 0.06666666644444444, 0.003921568626682045]\n",
            "training on batch 815 for epoch number 0\n",
            "[0.024655847, 1.0, 0.06666666644444444, 0.003921568626682045]\n",
            "training on batch 816 for epoch number 0\n",
            "[0.024278376, 1.0, 0.06666666644444444, 0.003921568626682045]\n",
            "training on batch 817 for epoch number 0\n",
            "[0.02386408, 1.0, 0.06666666644444444, 0.003921568626682045]\n",
            "training on batch 818 for epoch number 0\n",
            "[0.25758404, 0.9375, 0.06666666644444444, 0.003913894324087301]\n",
            "training on batch 819 for epoch number 0\n",
            "[0.023165362, 1.0, 0.06666666644444444, 0.003913894324087301]\n",
            "training on batch 820 for epoch number 0\n",
            "[0.022865338, 1.0, 0.06666666644444444, 0.003913894324087301]\n",
            "training on batch 821 for epoch number 0\n",
            "[0.25909016, 0.9375, 0.06666666644444444, 0.00390624999923706]\n",
            "training on batch 822 for epoch number 0\n",
            "[0.022361938, 1.0, 0.06666666644444444, 0.00390624999923706]\n",
            "training on batch 823 for epoch number 0\n",
            "[0.25973734, 0.9375, 0.06666666644444444, 0.0038986354768228773]\n",
            "training on batch 824 for epoch number 0\n",
            "[0.49754196, 0.875, 0.06666666644444444, 0.0038834951448769913]\n",
            "training on batch 825 for epoch number 0\n",
            "[0.022464748, 1.0, 0.06666666644444444, 0.0038834951448769913]\n",
            "training on batch 826 for epoch number 0\n",
            "[0.2589306, 0.9375, 0.06666666644444444, 0.0038759689914969054]\n",
            "training on batch 827 for epoch number 0\n",
            "[0.023049729, 1.0, 0.06666666644444444, 0.0038759689914969054]\n",
            "training on batch 828 for epoch number 0\n",
            "[0.2580886, 0.9375, 0.06666666644444444, 0.003868471952830083]\n",
            "training on batch 829 for epoch number 0\n",
            "[0.023508124, 1.0, 0.06666666644444444, 0.003868471952830083]\n",
            "training on batch 830 for epoch number 0\n",
            "[0.2572912, 0.9375, 0.06666666644444444, 0.003861003860258494]\n",
            "training on batch 831 for epoch number 0\n",
            "[0.023927914, 1.0, 0.06666666644444444, 0.003861003860258494]\n",
            "training on batch 832 for epoch number 0\n",
            "[0.024073558, 1.0, 0.06666666644444444, 0.003861003860258494]\n",
            "training on batch 833 for epoch number 0\n",
            "[0.25644645, 0.9375, 0.06666666644444444, 0.003853564546463668]\n",
            "training on batch 834 for epoch number 0\n",
            "[0.024296407, 1.0, 0.06666666644444444, 0.003853564546463668]\n",
            "training on batch 835 for epoch number 0\n",
            "[0.25598896, 0.9375, 0.06666666644444444, 0.0038461538454142015]\n",
            "training on batch 836 for epoch number 0\n",
            "[0.02458744, 1.0, 0.06666666644444444, 0.0038461538454142015]\n",
            "training on batch 837 for epoch number 0\n",
            "[0.024686873, 1.0, 0.06666666644444444, 0.0038461538454142015]\n",
            "training on batch 838 for epoch number 0\n",
            "[0.7169421, 0.8125, 0.06666666644444444, 0.0038240917774714934]\n",
            "training on batch 839 for epoch number 0\n",
            "[1.1710083, 0.6875, 0.06666666644444444, 0.003787878787161387]\n",
            "training on batch 840 for epoch number 0\n",
            "[0.25252682, 0.9375, 0.06666666644444444, 0.0037807183357692407]\n",
            "training on batch 841 for epoch number 0\n",
            "[0.029979376, 1.0, 0.06666666644444444, 0.0037807183357692407]\n",
            "training on batch 842 for epoch number 0\n",
            "[0.031609323, 1.0, 0.06666666644444444, 0.0037807183357692407]\n",
            "training on batch 843 for epoch number 0\n",
            "[0.24816325, 0.9375, 0.06666666644444444, 0.0037735849049483805]\n",
            "training on batch 844 for epoch number 0\n",
            "[0.24693242, 0.9375, 0.06666666644444444, 0.0037664783420402114]\n",
            "training on batch 845 for epoch number 0\n",
            "[0.45714048, 0.875, 0.06666666644444444, 0.0037523452150558453]\n",
            "training on batch 846 for epoch number 0\n",
            "[0.8705226, 0.75, 0.06666666644444444, 0.0037243947851537443]\n",
            "training on batch 847 for epoch number 0\n",
            "[0.44724813, 0.875, 0.06666666644444444, 0.0037105751384581497]\n",
            "training on batch 848 for epoch number 0\n",
            "[0.24165608, 0.9375, 0.06666666644444444, 0.0037037037030178328]\n",
            "training on batch 849 for epoch number 0\n",
            "[0.04514981, 1.0, 0.06666666644444444, 0.0037037037030178328]\n",
            "training on batch 850 for epoch number 0\n",
            "[1.0086749, 0.6875, 0.06666666644444444, 0.0036697247699688583]\n",
            "training on batch 851 for epoch number 0\n",
            "[0.42554063, 0.875, 0.06666666644444444, 0.0036563071291304742]\n",
            "training on batch 852 for epoch number 0\n",
            "[0.057586417, 1.0, 0.06666666644444444, 0.0036563071291304742]\n",
            "training on batch 853 for epoch number 0\n",
            "[0.23795256, 0.9375, 0.06666666644444444, 0.0036496350358303587]\n",
            "training on batch 854 for epoch number 0\n",
            "[0.06354004, 1.0, 0.06666666644444444, 0.0036496350358303587]\n",
            "training on batch 855 for epoch number 0\n",
            "[0.06411434, 1.0, 0.06666666644444444, 0.0036496350358303587]\n",
            "training on batch 856 for epoch number 0\n",
            "[0.2360838, 0.9375, 0.06666666644444444, 0.003642987248881059]\n",
            "training on batch 857 for epoch number 0\n",
            "[0.06406881, 1.0, 0.06666666644444444, 0.003642987248881059]\n",
            "training on batch 858 for epoch number 0\n",
            "[0.40829074, 0.875, 0.06666666644444444, 0.003629764064676994]\n",
            "training on batch 859 for epoch number 0\n",
            "[0.06468162, 1.0, 0.06666666644444444, 0.003629764064676994]\n",
            "training on batch 860 for epoch number 0\n",
            "[0.23598623, 0.9375, 0.06666666644444444, 0.003623188405140727]\n",
            "training on batch 861 for epoch number 0\n",
            "[0.2357563, 0.9375, 0.06666666644444444, 0.0036166365273749305]\n",
            "training on batch 862 for epoch number 0\n",
            "[0.0652901, 1.0, 0.06666666644444444, 0.0036166365273749305]\n",
            "training on batch 863 for epoch number 0\n",
            "[0.065145686, 1.0, 0.06666666644444444, 0.0036166365273749305]\n",
            "training on batch 864 for epoch number 0\n",
            "[0.23517889, 0.9375, 0.06666666644444444, 0.0036101083025974535]\n",
            "training on batch 865 for epoch number 0\n",
            "[0.40580672, 0.875, 0.06666666644444444, 0.0035971223015113093]\n",
            "training on batch 866 for epoch number 0\n",
            "[0.4051913, 0.875, 0.06666666644444444, 0.003584229390038669]\n",
            "training on batch 867 for epoch number 0\n",
            "[0.23481783, 0.9375, 0.06666666644444444, 0.0035778175306658646]\n",
            "training on batch 868 for epoch number 0\n",
            "[0.066458434, 1.0, 0.06666666644444444, 0.0035778175306658646]\n",
            "training on batch 869 for epoch number 0\n",
            "[0.066578574, 1.0, 0.06666666644444444, 0.0035778175306658646]\n",
            "training on batch 870 for epoch number 0\n",
            "[0.23458904, 0.9375, 0.06666666644444444, 0.0035714285707908167]\n",
            "training on batch 871 for epoch number 0\n",
            "[0.23450081, 0.9375, 0.06666666644444444, 0.003565062387956317]\n",
            "training on batch 872 for epoch number 0\n",
            "[0.23445168, 0.9375, 0.06666666644444444, 0.0035587188605767407]\n",
            "training on batch 873 for epoch number 0\n",
            "[0.23441313, 0.9375, 0.06666666644444444, 0.0035523978679303024]\n",
            "training on batch 874 for epoch number 0\n",
            "[0.06457907, 1.0, 0.06666666644444444, 0.0035523978679303024]\n",
            "training on batch 875 for epoch number 0\n",
            "[0.23441142, 0.9375, 0.06666666644444444, 0.003546099290151401]\n",
            "training on batch 876 for epoch number 0\n",
            "[0.06350657, 1.0, 0.06666666644444444, 0.003546099290151401]\n",
            "training on batch 877 for epoch number 0\n",
            "[0.23448975, 0.9375, 0.06666666644444444, 0.0035398230082230404]\n",
            "training on batch 878 for epoch number 0\n",
            "[0.06208745, 1.0, 0.06666666644444444, 0.0035398230082230404]\n",
            "training on batch 879 for epoch number 0\n",
            "[0.061195888, 1.0, 0.06666666644444444, 0.0035398230082230404]\n",
            "training on batch 880 for epoch number 0\n",
            "[0.23478131, 0.9375, 0.06666666644444444, 0.0035335689039693342]\n",
            "training on batch 881 for epoch number 0\n",
            "[0.059188396, 1.0, 0.06666666644444444, 0.0035335689039693342]\n",
            "training on batch 882 for epoch number 0\n",
            "[0.058101416, 1.0, 0.06666666644444444, 0.0035335689039693342]\n",
            "training on batch 883 for epoch number 0\n",
            "[0.056872644, 1.0, 0.06666666644444444, 0.0035335689039693342]\n",
            "training on batch 884 for epoch number 0\n",
            "[0.23553103, 0.9375, 0.06666666644444444, 0.003527336860048089]\n",
            "training on batch 885 for epoch number 0\n",
            "[0.054429233, 1.0, 0.06666666644444444, 0.003527336860048089]\n",
            "training on batch 886 for epoch number 0\n",
            "[0.23597217, 0.9375, 0.06666666644444444, 0.003521126759943464]\n",
            "training on batch 887 for epoch number 0\n",
            "[0.05222809, 1.0, 0.06666666644444444, 0.003521126759943464]\n",
            "training on batch 888 for epoch number 0\n",
            "[0.051127627, 1.0, 0.06666666644444444, 0.003521126759943464]\n",
            "training on batch 889 for epoch number 0\n",
            "[0.23670477, 0.9375, 0.06666666644444444, 0.0035149384879587103]\n",
            "training on batch 890 for epoch number 0\n",
            "[0.048964657, 1.0, 0.06666666644444444, 0.0035149384879587103]\n",
            "training on batch 891 for epoch number 0\n",
            "[0.4266209, 0.875, 0.06666666644444444, 0.0035026269696142513]\n",
            "training on batch 892 for epoch number 0\n",
            "[0.2372835, 0.9375, 0.06666666644444444, 0.00349650349589222]\n",
            "training on batch 893 for epoch number 0\n",
            "[0.23728171, 0.9375, 0.06666666644444444, 0.0034904013955514134]\n",
            "training on batch 894 for epoch number 0\n",
            "[0.04697188, 1.0, 0.06666666644444444, 0.0034904013955514134]\n",
            "training on batch 895 for epoch number 0\n",
            "[0.23735425, 0.9375, 0.06666666644444444, 0.0034843205568842648]\n",
            "training on batch 896 for epoch number 0\n",
            "[0.23739979, 0.9375, 0.06666666644444444, 0.0034782608689603025]\n",
            "training on batch 897 for epoch number 0\n",
            "[0.046096716, 1.0, 0.06666666644444444, 0.0034782608689603025]\n",
            "training on batch 898 for epoch number 0\n",
            "[0.045629192, 1.0, 0.06666666644444444, 0.0034782608689603025]\n",
            "training on batch 899 for epoch number 0\n",
            "[0.23776458, 0.9375, 0.06666666644444444, 0.003472222221619406]\n",
            "training on batch 900 for epoch number 0\n",
            "[0.431422, 0.875, 0.06666666644444444, 0.0034602076118580957]\n",
            "training on batch 901 for epoch number 0\n",
            "[0.43115756, 0.875, 0.06666666644444444, 0.0034482758614744354]\n",
            "training on batch 902 for epoch number 0\n",
            "[0.04519293, 1.0, 0.06666666644444444, 0.0034482758614744354]\n",
            "training on batch 903 for epoch number 0\n",
            "[0.045476258, 1.0, 0.06666666644444444, 0.0034482758614744354]\n",
            "training on batch 904 for epoch number 0\n",
            "[0.045343023, 1.0, 0.06666666644444444, 0.0034482758614744354]\n",
            "training on batch 905 for epoch number 0\n",
            "[0.044898357, 1.0, 0.06666666644444444, 0.0034482758614744354]\n",
            "training on batch 906 for epoch number 0\n",
            "[0.2379326, 0.9375, 0.06666666644444444, 0.003442340791145897]\n",
            "training on batch 907 for epoch number 0\n",
            "[0.04387195, 1.0, 0.06666666644444444, 0.003442340791145897]\n",
            "training on batch 908 for epoch number 0\n",
            "[0.2384766, 0.9375, 0.06666666644444444, 0.003436426116248037]\n",
            "training on batch 909 for epoch number 0\n",
            "[0.23864979, 0.9375, 0.06666666644444444, 0.0034305317318300975]\n",
            "training on batch 910 for epoch number 0\n",
            "[0.042734858, 1.0, 0.06666666644444444, 0.0034305317318300975]\n",
            "training on batch 911 for epoch number 0\n",
            "[0.23886804, 0.9375, 0.06666666644444444, 0.0034246575336601614]\n",
            "training on batch 912 for epoch number 0\n",
            "[0.042159144, 1.0, 0.06666666644444444, 0.0034246575336601614]\n",
            "training on batch 913 for epoch number 0\n",
            "[0.04176784, 1.0, 0.06666666644444444, 0.0034246575336601614]\n",
            "training on batch 914 for epoch number 0\n",
            "[0.04121749, 1.0, 0.06666666644444444, 0.0034246575336601614]\n",
            "training on batch 915 for epoch number 0\n",
            "[0.43920162, 0.875, 0.06666666644444444, 0.0034129692826940328]\n",
            "training on batch 916 for epoch number 0\n",
            "[0.23989604, 0.9375, 0.06666666644444444, 0.0034071550249732277]\n",
            "training on batch 917 for epoch number 0\n",
            "[0.040419355, 1.0, 0.06666666644444444, 0.0034071550249732277]\n",
            "training on batch 918 for epoch number 0\n",
            "[0.040243547, 1.0, 0.06666666644444444, 0.0034071550249732277]\n",
            "training on batch 919 for epoch number 0\n",
            "[0.03985462, 1.0, 0.06666666644444444, 0.0034071550249732277]\n",
            "training on batch 920 for epoch number 0\n",
            "[0.24048892, 0.9375, 0.06666666644444444, 0.0034013605436392248]\n",
            "training on batch 921 for epoch number 0\n",
            "[0.24070935, 0.9375, 0.06666666644444444, 0.0033955857379633983]\n",
            "training on batch 922 for epoch number 0\n",
            "[0.6448083, 0.8125, 0.06666666644444444, 0.0033783783778077066]\n",
            "training on batch 923 for epoch number 0\n",
            "[0.039501283, 1.0, 0.06666666644444444, 0.0033783783778077066]\n",
            "training on batch 924 for epoch number 0\n",
            "[0.24018651, 0.9375, 0.06666666644444444, 0.0033726812810501382]\n",
            "training on batch 925 for epoch number 0\n",
            "[0.2399814, 0.9375, 0.06666666644444444, 0.0033670033664365316]\n",
            "training on batch 926 for epoch number 0\n",
            "[0.04105327, 1.0, 0.06666666644444444, 0.0033670033664365316]\n",
            "training on batch 927 for epoch number 0\n",
            "[0.23961248, 0.9375, 0.06666666644444444, 0.0033613445372501945]\n",
            "training on batch 928 for epoch number 0\n",
            "[0.23946579, 0.9375, 0.06666666644444444, 0.0033557046974235396]\n",
            "training on batch 929 for epoch number 0\n",
            "[0.041669216, 1.0, 0.06666666644444444, 0.0033557046974235396]\n",
            "training on batch 930 for epoch number 0\n",
            "[0.23925444, 0.9375, 0.06666666644444444, 0.0033500837515326493]\n",
            "training on batch 931 for epoch number 0\n",
            "[0.2391748, 0.9375, 0.06666666644444444, 0.003344481604791893]\n",
            "training on batch 932 for epoch number 0\n",
            "[0.04194861, 1.0, 0.06666666644444444, 0.003344481604791893]\n",
            "training on batch 933 for epoch number 0\n",
            "[0.04189391, 1.0, 0.06666666644444444, 0.003344481604791893]\n",
            "training on batch 934 for epoch number 0\n",
            "[0.041619726, 1.0, 0.06666666644444444, 0.003344481604791893]\n",
            "training on batch 935 for epoch number 0\n",
            "[0.04119656, 1.0, 0.06666666644444444, 0.003344481604791893]\n",
            "training on batch 936 for epoch number 0\n",
            "[0.2399579, 0.9375, 0.06666666644444444, 0.0033388981630485984]\n",
            "training on batch 937 for epoch number 0\n",
            "[0.04032897, 1.0, 0.06666666644444444, 0.0033388981630485984]\n",
            "training on batch 938 for epoch number 0\n",
            "[0.24050158, 0.9375, 0.06666666644444444, 0.003333333332777778]\n",
            "training on batch 939 for epoch number 0\n",
            "[0.03956836, 1.0, 0.06666666644444444, 0.003333333332777778]\n",
            "training on batch 940 for epoch number 0\n",
            "[0.039159544, 1.0, 0.06666666644444444, 0.003333333332777778]\n",
            "training on batch 941 for epoch number 0\n",
            "[0.24119328, 0.9375, 0.06666666644444444, 0.0033277870210769076]\n",
            "training on batch 942 for epoch number 0\n",
            "[0.24136533, 0.9375, 0.06666666644444444, 0.0033222591356607545]\n",
            "training on batch 943 for epoch number 0\n",
            "[0.03816524, 1.0, 0.06666666644444444, 0.0033222591356607545]\n",
            "training on batch 944 for epoch number 0\n",
            "[0.0378679, 1.0, 0.06666666644444444, 0.0033222591356607545]\n",
            "training on batch 945 for epoch number 0\n",
            "[0.24189381, 0.9375, 0.06666666644444444, 0.0033167495848562606]\n",
            "training on batch 946 for epoch number 0\n",
            "[0.65184826, 0.8125, 0.06666666644444444, 0.0033003300324586917]\n",
            "training on batch 947 for epoch number 0\n",
            "[0.03775941, 1.0, 0.06666666644444444, 0.0033003300324586917]\n",
            "training on batch 948 for epoch number 0\n",
            "[0.038121715, 1.0, 0.06666666644444444, 0.0033003300324586917]\n",
            "training on batch 949 for epoch number 0\n",
            "[0.038152702, 1.0, 0.06666666644444444, 0.0033003300324586917]\n",
            "training on batch 950 for epoch number 0\n",
            "[0.64869946, 0.8125, 0.06666666644444444, 0.0032840722490502347]\n",
            "training on batch 951 for epoch number 0\n",
            "[0.8487651, 0.75, 0.06666666644444444, 0.0032626427400876605]\n",
            "training on batch 952 for epoch number 0\n",
            "[1.0392097, 0.6875, 0.06666666644444444, 0.0032362459541688923]\n",
            "training on batch 953 for epoch number 0\n",
            "[1.408039, 0.5625, 0.06666666644444444, 0.003199999999488]\n",
            "training on batch 954 for epoch number 0\n",
            "[0.8002225, 0.75, 0.06666666644444444, 0.0031796502379682593]\n",
            "training on batch 955 for epoch number 0\n",
            "[0.072684966, 1.0, 0.06666666644444444, 0.0031796502379682593]\n",
            "training on batch 956 for epoch number 0\n",
            "[0.084216, 1.0, 0.06666666644444444, 0.0031796502379682593]\n",
            "training on batch 957 for epoch number 0\n",
            "[0.24749443, 0.9375, 0.06666666644444444, 0.0031746031740992695]\n",
            "training on batch 958 for epoch number 0\n",
            "[0.24498053, 0.9375, 0.06666666644444444, 0.0031695721072631427]\n",
            "training on batch 959 for epoch number 0\n",
            "[0.24289484, 0.9375, 0.06666666644444444, 0.0031645569615245954]\n",
            "training on batch 960 for epoch number 0\n",
            "[0.08003047, 1.0, 0.06666666644444444, 0.0031645569615245954]\n",
            "training on batch 961 for epoch number 0\n",
            "[0.24071492, 0.9375, 0.06666666644444444, 0.00315955766142819]\n",
            "training on batch 962 for epoch number 0\n",
            "[0.24058957, 0.9375, 0.06666666644444444, 0.0031545741319945466]\n",
            "training on batch 963 for epoch number 0\n",
            "[0.2406509, 0.9375, 0.06666666644444444, 0.0031496062987165976]\n",
            "training on batch 964 for epoch number 0\n",
            "[0.24056979, 0.9375, 0.06666666644444444, 0.003144654087555872]\n",
            "training on batch 965 for epoch number 0\n",
            "[0.40195882, 0.875, 0.06666666644444444, 0.003134796237753167]\n",
            "training on batch 966 for epoch number 0\n",
            "[0.4003668, 0.875, 0.06666666644444444, 0.003124999999511719]\n",
            "training on batch 967 for epoch number 0\n",
            "[0.3982809, 0.875, 0.06666666644444444, 0.0031152647970225447]\n",
            "training on batch 968 for epoch number 0\n",
            "[0.23848628, 0.9375, 0.06666666644444444, 0.0031104199062036673]\n",
            "training on batch 969 for epoch number 0\n",
            "[0.39431715, 0.875, 0.06666666644444444, 0.0031007751933177096]\n",
            "training on batch 970 for epoch number 0\n",
            "[0.23843886, 0.9375, 0.06666666644444444, 0.0030959752317188892]\n",
            "training on batch 971 for epoch number 0\n",
            "[0.23867075, 0.9375, 0.06666666644444444, 0.003091190107713881]\n",
            "training on batch 972 for epoch number 0\n",
            "[0.23883633, 0.9375, 0.06666666644444444, 0.0030864197526101205]\n",
            "training on batch 973 for epoch number 0\n",
            "[0.23885709, 0.9375, 0.06666666644444444, 0.003081664098138419]\n",
            "training on batch 974 for epoch number 0\n",
            "[0.23873396, 0.9375, 0.06666666644444444, 0.0030769230764497043]\n",
            "training on batch 975 for epoch number 0\n",
            "[0.08811128, 1.0, 0.06666666644444444, 0.0030769230764497043]\n",
            "training on batch 976 for epoch number 0\n",
            "[0.23809561, 0.9375, 0.06666666644444444, 0.003072196620111798]\n",
            "training on batch 977 for epoch number 0\n",
            "[0.3893435, 0.875, 0.06666666644444444, 0.0030627871358249945]\n",
            "training on batch 978 for epoch number 0\n",
            "[0.23763455, 0.9375, 0.06666666644444444, 0.0030581039750675683]\n",
            "training on batch 979 for epoch number 0\n",
            "[0.08554538, 1.0, 0.06666666644444444, 0.0030581039750675683]\n",
            "training on batch 980 for epoch number 0\n",
            "[0.084678166, 1.0, 0.06666666644444444, 0.0030581039750675683]\n",
            "training on batch 981 for epoch number 0\n",
            "[0.23717695, 0.9375, 0.06666666644444444, 0.0030534351140376437]\n",
            "training on batch 982 for epoch number 0\n",
            "[0.23703244, 0.9375, 0.06666666644444444, 0.003048780487340125]\n",
            "training on batch 983 for epoch number 0\n",
            "[0.08128602, 1.0, 0.06666666644444444, 0.003048780487340125]\n",
            "training on batch 984 for epoch number 0\n",
            "[0.07995466, 1.0, 0.06666666644444444, 0.003048780487340125]\n",
            "training on batch 985 for epoch number 0\n",
            "[0.2365313, 0.9375, 0.06666666644444444, 0.003044140029978061]\n",
            "training on batch 986 for epoch number 0\n",
            "[0.07697339, 1.0, 0.06666666644444444, 0.003044140029978061]\n",
            "training on batch 987 for epoch number 0\n",
            "[0.23616162, 0.9375, 0.06666666644444444, 0.003039513677349618]\n",
            "training on batch 988 for epoch number 0\n",
            "[0.07391849, 1.0, 0.06666666644444444, 0.003039513677349618]\n",
            "training on batch 989 for epoch number 0\n",
            "[0.23577432, 0.9375, 0.06666666644444444, 0.0030349013652450833]\n",
            "training on batch 990 for epoch number 0\n",
            "[0.23556875, 0.9375, 0.06666666644444444, 0.0030303030298438937]\n",
            "training on batch 991 for epoch number 0\n",
            "[0.23534879, 0.9375, 0.06666666644444444, 0.003025718607711692]\n",
            "training on batch 992 for epoch number 0\n",
            "[0.06854414, 1.0, 0.06666666644444444, 0.003025718607711692]\n",
            "training on batch 993 for epoch number 0\n",
            "[0.06725166, 1.0, 0.06666666644444444, 0.003025718607711692]\n",
            "training on batch 994 for epoch number 0\n",
            "[0.23493606, 0.9375, 0.06666666644444444, 0.0030211480357974097]\n",
            "training on batch 995 for epoch number 0\n",
            "[0.2348727, 0.9375, 0.06666666644444444, 0.0030165912514303786]\n",
            "training on batch 996 for epoch number 0\n",
            "[0.2348044, 0.9375, 0.06666666644444444, 0.003012048192317463]\n",
            "training on batch 997 for epoch number 0\n",
            "[0.062431514, 1.0, 0.06666666644444444, 0.003012048192317463]\n",
            "training on batch 998 for epoch number 0\n",
            "[0.23473403, 0.9375, 0.06666666644444444, 0.003007518796540223]\n",
            "training on batch 999 for epoch number 0\n",
            "[0.58363473, 0.8125, 0.06666666644444444, 0.002994011975599699]\n",
            "training on batch 1000 for epoch number 0\n",
            "[0.23458634, 0.9375, 0.06666666644444444, 0.002989536621376751]\n",
            "training on batch 1001 for epoch number 0\n",
            "[0.40868002, 0.875, 0.06666666644444444, 0.0029806259310013973]\n",
            "training on batch 1002 for epoch number 0\n",
            "[0.23448935, 0.9375, 0.06666666644444444, 0.0029761904757475907]\n",
            "training on batch 1003 for epoch number 0\n",
            "[0.23449565, 0.9375, 0.06666666644444444, 0.0029717682016386676]\n",
            "training on batch 1004 for epoch number 0\n",
            "[0.23448925, 0.9375, 0.06666666644444444, 0.002967359050004843]\n",
            "training on batch 1005 for epoch number 0\n",
            "[0.23445797, 0.9375, 0.06666666644444444, 0.002962962962524006]\n",
            "training on batch 1006 for epoch number 0\n",
            "[0.0625318, 1.0, 0.06666666644444444, 0.002962962962524006]\n",
            "training on batch 1007 for epoch number 0\n",
            "[0.2342985, 0.9375, 0.06666666644444444, 0.002958579881219145]\n",
            "training on batch 1008 for epoch number 0\n",
            "[0.0617006, 1.0, 0.06666666644444444, 0.002958579881219145]\n",
            "training on batch 1009 for epoch number 0\n",
            "[0.23423097, 0.9375, 0.06666666644444444, 0.0029542097484558036]\n",
            "training on batch 1010 for epoch number 0\n",
            "[0.060282744, 1.0, 0.06666666644444444, 0.0029542097484558036]\n",
            "training on batch 1011 for epoch number 0\n",
            "[0.05940975, 1.0, 0.06666666644444444, 0.0029542097484558036]\n",
            "training on batch 1012 for epoch number 0\n",
            "[0.23468214, 0.9375, 0.06666666644444444, 0.00294985250693955]\n",
            "training on batch 1013 for epoch number 0\n",
            "[0.057551228, 1.0, 0.06666666644444444, 0.00294985250693955]\n",
            "training on batch 1014 for epoch number 0\n",
            "[0.05656871, 1.0, 0.06666666644444444, 0.00294985250693955]\n",
            "training on batch 1015 for epoch number 0\n",
            "[0.055461846, 1.0, 0.06666666644444444, 0.00294985250693955]\n",
            "training on batch 1016 for epoch number 0\n",
            "[0.23581053, 0.9375, 0.06666666644444444, 0.002945508099713475]\n",
            "training on batch 1017 for epoch number 0\n",
            "[0.05323744, 1.0, 0.06666666644444444, 0.002945508099713475]\n",
            "training on batch 1018 for epoch number 0\n",
            "[0.23625502, 0.9375, 0.06666666644444444, 0.0029411764701557093]\n",
            "training on batch 1019 for epoch number 0\n",
            "[0.051192883, 1.0, 0.06666666644444444, 0.0029411764701557093]\n",
            "training on batch 1020 for epoch number 0\n",
            "[0.23658054, 0.9375, 0.06666666644444444, 0.002936857561976967]\n",
            "training on batch 1021 for epoch number 0\n",
            "[0.04936461, 1.0, 0.06666666644444444, 0.002936857561976967]\n",
            "training on batch 1022 for epoch number 0\n",
            "[0.23692901, 0.9375, 0.06666666644444444, 0.002932551319218101]\n",
            "training on batch 1023 for epoch number 0\n",
            "[0.04772731, 1.0, 0.06666666644444444, 0.002932551319218101]\n",
            "training on batch 1024 for epoch number 0\n",
            "[0.2373449, 0.9375, 0.06666666644444444, 0.002928257686247693]\n",
            "training on batch 1025 for epoch number 0\n",
            "[0.04621458, 1.0, 0.06666666644444444, 0.002928257686247693]\n",
            "training on batch 1026 for epoch number 0\n",
            "[0.43023553, 0.875, 0.06666666644444444, 0.0029197080287708455]\n",
            "training on batch 1027 for epoch number 0\n",
            "[0.04514409, 1.0, 0.06666666644444444, 0.0029197080287708455]\n",
            "training on batch 1028 for epoch number 0\n",
            "[0.43129522, 0.875, 0.06666666644444444, 0.0029112081509590675]\n",
            "training on batch 1029 for epoch number 0\n",
            "[0.23791781, 0.9375, 0.06666666644444444, 0.002906976743763521]\n",
            "training on batch 1030 for epoch number 0\n",
            "[0.04497047, 1.0, 0.06666666644444444, 0.002906976743763521]\n",
            "training on batch 1031 for epoch number 0\n",
            "[0.044826254, 1.0, 0.06666666644444444, 0.002906976743763521]\n",
            "training on batch 1032 for epoch number 0\n",
            "[0.4315987, 0.875, 0.06666666644444444, 0.0028985507242176014]\n",
            "training on batch 1033 for epoch number 0\n",
            "[0.04448815, 1.0, 0.06666666644444444, 0.0028985507242176014]\n",
            "training on batch 1034 for epoch number 0\n",
            "[0.044286456, 1.0, 0.06666666644444444, 0.0028985507242176014]\n",
            "training on batch 1035 for epoch number 0\n",
            "[0.043831848, 1.0, 0.06666666644444444, 0.0028985507242176014]\n",
            "training on batch 1036 for epoch number 0\n",
            "[0.043200564, 1.0, 0.06666666644444444, 0.0028985507242176014]\n",
            "training on batch 1037 for epoch number 0\n",
            "[0.04245886, 1.0, 0.06666666644444444, 0.0028985507242176014]\n",
            "training on batch 1038 for epoch number 0\n",
            "[0.04164743, 1.0, 0.06666666644444444, 0.0028985507242176014]\n",
            "training on batch 1039 for epoch number 0\n",
            "[0.24012645, 0.9375, 0.06666666644444444, 0.002894356005369847]\n",
            "training on batch 1040 for epoch number 0\n",
            "[0.040130135, 1.0, 0.06666666644444444, 0.002894356005369847]\n",
            "training on batch 1041 for epoch number 0\n",
            "[0.039390817, 1.0, 0.06666666644444444, 0.002894356005369847]\n",
            "training on batch 1042 for epoch number 0\n",
            "[0.03857979, 1.0, 0.06666666644444444, 0.002894356005369847]\n",
            "training on batch 1043 for epoch number 0\n",
            "[0.4468249, 0.875, 0.06666666644444444, 0.0028860028855864357]\n",
            "training on batch 1044 for epoch number 0\n",
            "[0.037323732, 1.0, 0.06666666644444444, 0.0028860028855864357]\n",
            "training on batch 1045 for epoch number 0\n",
            "[0.24256176, 0.9375, 0.06666666644444444, 0.002881844379988207]\n",
            "training on batch 1046 for epoch number 0\n",
            "[0.036607392, 1.0, 0.06666666644444444, 0.002881844379988207]\n",
            "training on batch 1047 for epoch number 0\n",
            "[0.03622053, 1.0, 0.06666666644444444, 0.002881844379988207]\n",
            "training on batch 1048 for epoch number 0\n",
            "[0.035697132, 1.0, 0.06666666644444444, 0.002881844379988207]\n",
            "training on batch 1049 for epoch number 0\n",
            "[0.03506259, 1.0, 0.06666666644444444, 0.002881844379988207]\n",
            "training on batch 1050 for epoch number 0\n",
            "[0.034347206, 1.0, 0.06666666644444444, 0.002881844379988207]\n",
            "training on batch 1051 for epoch number 0\n",
            "[0.2450927, 0.9375, 0.06666666644444444, 0.0028776978413125615]\n",
            "training on batch 1052 for epoch number 0\n",
            "[0.4580947, 0.875, 0.06666666644444444, 0.002869440458698789]\n",
            "training on batch 1053 for epoch number 0\n",
            "[0.033001095, 1.0, 0.06666666644444444, 0.002869440458698789]\n",
            "training on batch 1054 for epoch number 0\n",
            "[0.032849804, 1.0, 0.06666666644444444, 0.002869440458698789]\n",
            "training on batch 1055 for epoch number 0\n",
            "[0.03254933, 1.0, 0.06666666644444444, 0.002869440458698789]\n",
            "training on batch 1056 for epoch number 0\n",
            "[0.032117065, 1.0, 0.06666666644444444, 0.002869440458698789]\n",
            "training on batch 1057 for epoch number 0\n",
            "[0.24674195, 0.9375, 0.06666666644444444, 0.0028653295124834773]\n",
            "training on batch 1058 for epoch number 0\n",
            "[0.03128308, 1.0, 0.06666666644444444, 0.0028653295124834773]\n",
            "training on batch 1059 for epoch number 0\n",
            "[0.03087591, 1.0, 0.06666666644444444, 0.0028653295124834773]\n",
            "training on batch 1060 for epoch number 0\n",
            "[0.24801563, 0.9375, 0.06666666644444444, 0.002861230328632156]\n",
            "training on batch 1061 for epoch number 0\n",
            "[0.03009929, 1.0, 0.06666666644444444, 0.002861230328632156]\n",
            "training on batch 1062 for epoch number 0\n",
            "[0.029727533, 1.0, 0.06666666644444444, 0.002861230328632156]\n",
            "training on batch 1063 for epoch number 0\n",
            "[0.249244, 0.9375, 0.06666666644444444, 0.002857142856734694]\n",
            "training on batch 1064 for epoch number 0\n",
            "[0.24950454, 0.9375, 0.06666666644444444, 0.002853067046668607]\n",
            "training on batch 1065 for epoch number 0\n",
            "[0.24953105, 0.9375, 0.06666666644444444, 0.0028490028485970084]\n",
            "training on batch 1066 for epoch number 0\n",
            "[0.24937855, 0.9375, 0.06666666644444444, 0.002844950212966579]\n",
            "training on batch 1067 for epoch number 0\n",
            "[0.029398771, 1.0, 0.06666666644444444, 0.002844950212966579]\n",
            "training on batch 1068 for epoch number 0\n",
            "[0.029495576, 1.0, 0.06666666644444444, 0.002844950212966579]\n",
            "training on batch 1069 for epoch number 0\n",
            "[0.029402617, 1.0, 0.06666666644444444, 0.002844950212966579]\n",
            "training on batch 1070 for epoch number 0\n",
            "[0.029163783, 1.0, 0.06666666644444444, 0.002844950212966579]\n",
            "training on batch 1071 for epoch number 0\n",
            "[0.24971378, 0.9375, 0.06666666644444444, 0.0028409090905055527]\n",
            "training on batch 1072 for epoch number 0\n",
            "[0.028676571, 1.0, 0.06666666644444444, 0.0028409090905055527]\n",
            "training on batch 1073 for epoch number 0\n",
            "[0.02843208, 1.0, 0.06666666644444444, 0.0028409090905055527]\n",
            "training on batch 1074 for epoch number 0\n",
            "[0.25061804, 0.9375, 0.06666666644444444, 0.0028368794322217194]\n",
            "training on batch 1075 for epoch number 0\n",
            "[0.25077906, 0.9375, 0.06666666644444444, 0.002832861189400445]\n",
            "training on batch 1076 for epoch number 0\n",
            "[0.028012535, 1.0, 0.06666666644444444, 0.002832861189400445]\n",
            "training on batch 1077 for epoch number 0\n",
            "[0.027930452, 1.0, 0.06666666644444444, 0.002832861189400445]\n",
            "training on batch 1078 for epoch number 0\n",
            "[0.47430214, 0.875, 0.06666666644444444, 0.0028248587566631557]\n",
            "training on batch 1079 for epoch number 0\n",
            "[0.25066754, 0.9375, 0.06666666644444444, 0.00282087447068817]\n",
            "training on batch 1080 for epoch number 0\n",
            "[0.028450515, 1.0, 0.06666666644444444, 0.00282087447068817]\n",
            "training on batch 1081 for epoch number 0\n",
            "[0.028693236, 1.0, 0.06666666644444444, 0.00282087447068817]\n",
            "training on batch 1082 for epoch number 0\n",
            "[0.24987862, 0.9375, 0.06666666644444444, 0.0028169014080539575]\n",
            "training on batch 1083 for epoch number 0\n",
            "[0.028919615, 1.0, 0.06666666644444444, 0.0028169014080539575]\n",
            "training on batch 1084 for epoch number 0\n",
            "[0.028922992, 1.0, 0.06666666644444444, 0.0028169014080539575]\n",
            "training on batch 1085 for epoch number 0\n",
            "[0.028783916, 1.0, 0.06666666644444444, 0.0028169014080539575]\n",
            "training on batch 1086 for epoch number 0\n",
            "[0.25003985, 0.9375, 0.06666666644444444, 0.00281293952140465]\n",
            "training on batch 1087 for epoch number 0\n",
            "[0.9149353, 0.75, 0.06666666644444444, 0.00279720279681158]\n",
            "training on batch 1088 for epoch number 0\n",
            "[0.6880448, 0.8125, 0.06666666644444444, 0.002785515319946307]\n",
            "training on batch 1089 for epoch number 0\n",
            "[0.031451933, 1.0, 0.06666666644444444, 0.002785515319946307]\n",
            "training on batch 1090 for epoch number 0\n",
            "[0.033103306, 1.0, 0.06666666644444444, 0.002785515319946307]\n",
            "training on batch 1091 for epoch number 0\n",
            "[0.034150533, 1.0, 0.06666666644444444, 0.002785515319946307]\n",
            "training on batch 1092 for epoch number 0\n",
            "[0.24522154, 0.9375, 0.06666666644444444, 0.0027816411679024143]\n",
            "training on batch 1093 for epoch number 0\n",
            "[0.035227075, 1.0, 0.06666666644444444, 0.0027816411679024143]\n",
            "training on batch 1094 for epoch number 0\n",
            "[0.0354534, 1.0, 0.06666666644444444, 0.0027816411679024143]\n",
            "training on batch 1095 for epoch number 0\n",
            "[0.24393606, 0.9375, 0.06666666644444444, 0.0027777777773919754]\n",
            "training on batch 1096 for epoch number 0\n",
            "[0.035624307, 1.0, 0.06666666644444444, 0.0027777777773919754]\n",
            "training on batch 1097 for epoch number 0\n",
            "[0.24360979, 0.9375, 0.06666666644444444, 0.0027739251036374584]\n",
            "training on batch 1098 for epoch number 0\n",
            "[0.035810597, 1.0, 0.06666666644444444, 0.0027739251036374584]\n",
            "training on batch 1099 for epoch number 0\n",
            "[0.03582379, 1.0, 0.06666666644444444, 0.0027739251036374584]\n",
            "training on batch 1100 for epoch number 0\n",
            "[0.24353155, 0.9375, 0.06666666644444444, 0.002770083102109407]\n",
            "training on batch 1101 for epoch number 0\n",
            "[0.03569841, 1.0, 0.06666666644444444, 0.002770083102109407]\n",
            "training on batch 1102 for epoch number 0\n",
            "[0.035576474, 1.0, 0.06666666644444444, 0.002770083102109407]\n",
            "training on batch 1103 for epoch number 0\n",
            "[0.035343688, 1.0, 0.06666666644444444, 0.002770083102109407]\n",
            "training on batch 1104 for epoch number 0\n",
            "[0.035013817, 1.0, 0.06666666644444444, 0.002770083102109407]\n",
            "training on batch 1105 for epoch number 0\n",
            "[0.24443583, 0.9375, 0.06666666644444444, 0.002766251728524723]\n",
            "training on batch 1106 for epoch number 0\n",
            "[0.034342907, 1.0, 0.06666666644444444, 0.002766251728524723]\n",
            "training on batch 1107 for epoch number 0\n",
            "[0.2448655, 0.9375, 0.06666666644444444, 0.002762430938844968]\n",
            "training on batch 1108 for epoch number 0\n",
            "[0.03383038, 1.0, 0.06666666644444444, 0.002762430938844968]\n",
            "training on batch 1109 for epoch number 0\n",
            "[0.03356065, 1.0, 0.06666666644444444, 0.002762430938844968]\n",
            "training on batch 1110 for epoch number 0\n",
            "[0.033191677, 1.0, 0.06666666644444444, 0.002762430938844968]\n",
            "training on batch 1111 for epoch number 0\n",
            "[0.032740083, 1.0, 0.06666666644444444, 0.002762430938844968]\n",
            "training on batch 1112 for epoch number 0\n",
            "[0.246424, 0.9375, 0.06666666644444444, 0.002758620689274673]\n",
            "training on batch 1113 for epoch number 0\n",
            "[0.031896606, 1.0, 0.06666666644444444, 0.002758620689274673]\n",
            "training on batch 1114 for epoch number 0\n",
            "[0.0314937, 1.0, 0.06666666644444444, 0.002758620689274673]\n",
            "training on batch 1115 for epoch number 0\n",
            "[0.24760148, 0.9375, 0.06666666644444444, 0.002754820936259667]\n",
            "training on batch 1116 for epoch number 0\n",
            "[0.030743161, 1.0, 0.06666666644444444, 0.002754820936259667]\n",
            "training on batch 1117 for epoch number 0\n",
            "[0.030384457, 1.0, 0.06666666644444444, 0.002754820936259667]\n",
            "training on batch 1118 for epoch number 0\n",
            "[0.029955333, 1.0, 0.06666666644444444, 0.002754820936259667]\n",
            "training on batch 1119 for epoch number 0\n",
            "[0.029469417, 1.0, 0.06666666644444444, 0.002754820936259667]\n",
            "training on batch 1120 for epoch number 0\n",
            "[0.24989253, 0.9375, 0.06666666644444444, 0.0027510316364854153]\n",
            "training on batch 1121 for epoch number 0\n",
            "[0.02860465, 1.0, 0.06666666644444444, 0.0027510316364854153]\n",
            "training on batch 1122 for epoch number 0\n",
            "[0.25072417, 0.9375, 0.06666666644444444, 0.0027472527468753775]\n",
            "training on batch 1123 for epoch number 0\n",
            "[0.02799624, 1.0, 0.06666666644444444, 0.0027472527468753775]\n",
            "training on batch 1124 for epoch number 0\n",
            "[0.027711915, 1.0, 0.06666666644444444, 0.0027472527468753775]\n",
            "training on batch 1125 for epoch number 0\n",
            "[0.02735784, 1.0, 0.06666666644444444, 0.0027472527468753775]\n",
            "training on batch 1126 for epoch number 0\n",
            "[0.026946433, 1.0, 0.06666666644444444, 0.0027472527468753775]\n",
            "training on batch 1127 for epoch number 0\n",
            "[0.47926202, 0.875, 0.06666666644444444, 0.0027397260270219555]\n",
            "training on batch 1128 for epoch number 0\n",
            "[0.026456013, 1.0, 0.06666666644444444, 0.0027397260270219555]\n",
            "training on batch 1129 for epoch number 0\n",
            "[0.026343172, 1.0, 0.06666666644444444, 0.0027397260270219555]\n",
            "training on batch 1130 for epoch number 0\n",
            "[0.25323257, 0.9375, 0.06666666644444444, 0.002735978111800824]\n",
            "training on batch 1131 for epoch number 0\n",
            "[0.02610907, 1.0, 0.06666666644444444, 0.002735978111800824]\n",
            "training on batch 1132 for epoch number 0\n",
            "[0.025969744, 1.0, 0.06666666644444444, 0.002735978111800824]\n",
            "training on batch 1133 for epoch number 0\n",
            "[0.25376278, 0.9375, 0.06666666644444444, 0.0027322404367852134]\n",
            "training on batch 1134 for epoch number 0\n",
            "[0.025678301, 1.0, 0.06666666644444444, 0.0027322404367852134]\n",
            "training on batch 1135 for epoch number 0\n",
            "[0.025523467, 1.0, 0.06666666644444444, 0.0027322404367852134]\n",
            "training on batch 1136 for epoch number 0\n",
            "[0.025289034, 1.0, 0.06666666644444444, 0.0027322404367852134]\n",
            "training on batch 1137 for epoch number 0\n",
            "[0.024995048, 1.0, 0.06666666644444444, 0.0027322404367852134]\n",
            "training on batch 1138 for epoch number 0\n",
            "[0.02465803, 1.0, 0.06666666644444444, 0.0027322404367852134]\n",
            "training on batch 1139 for epoch number 0\n",
            "[0.25610933, 0.9375, 0.06666666644444444, 0.002728512960064323]\n",
            "training on batch 1140 for epoch number 0\n",
            "[0.4887284, 0.875, 0.06666666644444444, 0.0027210884350039335]\n",
            "training on batch 1141 for epoch number 0\n",
            "[0.95115656, 0.75, 0.06666666644444444, 0.002706359945506582]\n",
            "training on batch 1142 for epoch number 0\n",
            "[0.025558492, 1.0, 0.06666666644444444, 0.002706359945506582]\n",
            "training on batch 1143 for epoch number 0\n",
            "[0.026757564, 1.0, 0.06666666644444444, 0.002706359945506582]\n",
            "training on batch 1144 for epoch number 0\n",
            "[0.25283116, 0.9375, 0.06666666644444444, 0.002702702702337473]\n",
            "training on batch 1145 for epoch number 0\n",
            "[0.028330069, 1.0, 0.06666666644444444, 0.002702702702337473]\n",
            "training on batch 1146 for epoch number 0\n",
            "[0.028705887, 1.0, 0.06666666644444444, 0.002702702702337473]\n",
            "training on batch 1147 for epoch number 0\n",
            "[0.028806737, 1.0, 0.06666666644444444, 0.002702702702337473]\n",
            "training on batch 1148 for epoch number 0\n",
            "[0.028750079, 1.0, 0.06666666644444444, 0.002702702702337473]\n",
            "training on batch 1149 for epoch number 0\n",
            "[0.25073382, 0.9375, 0.06666666644444444, 0.0026990553302700332]\n",
            "training on batch 1150 for epoch number 0\n",
            "[0.028666915, 1.0, 0.06666666644444444, 0.0026990553302700332]\n",
            "training on batch 1151 for epoch number 0\n",
            "[0.2506386, 0.9375, 0.06666666644444444, 0.002695417789394149]\n",
            "training on batch 1152 for epoch number 0\n",
            "[0.028727503, 1.0, 0.06666666644444444, 0.002695417789394149]\n",
            "training on batch 1153 for epoch number 0\n",
            "[0.028721487, 1.0, 0.06666666644444444, 0.002695417789394149]\n",
            "training on batch 1154 for epoch number 0\n",
            "[0.028617464, 1.0, 0.06666666644444444, 0.002695417789394149]\n",
            "training on batch 1155 for epoch number 0\n",
            "[0.028433478, 1.0, 0.06666666644444444, 0.002695417789394149]\n",
            "training on batch 1156 for epoch number 0\n",
            "[0.2510398, 0.9375, 0.06666666644444444, 0.002691790040014564]\n",
            "training on batch 1157 for epoch number 0\n",
            "[0.028091239, 1.0, 0.06666666644444444, 0.002691790040014564]\n",
            "training on batch 1158 for epoch number 0\n",
            "[0.027920095, 1.0, 0.06666666644444444, 0.002691790040014564]\n",
            "training on batch 1159 for epoch number 0\n",
            "[0.027678704, 1.0, 0.06666666644444444, 0.002691790040014564]\n",
            "training on batch 1160 for epoch number 0\n",
            "[0.027377868, 1.0, 0.06666666644444444, 0.002691790040014564]\n",
            "training on batch 1161 for epoch number 0\n",
            "[0.027029734, 1.0, 0.06666666644444444, 0.002691790040014564]\n",
            "training on batch 1162 for epoch number 0\n",
            "[0.026642874, 1.0, 0.06666666644444444, 0.002691790040014564]\n",
            "training on batch 1163 for epoch number 0\n",
            "[0.0262235, 1.0, 0.06666666644444444, 0.002691790040014564]\n",
            "training on batch 1164 for epoch number 0\n",
            "[0.2543261, 0.9375, 0.06666666644444444, 0.0026881720426494395]\n",
            "training on batch 1165 for epoch number 0\n",
            "[0.025493583, 1.0, 0.06666666644444444, 0.0026881720426494395]\n",
            "training on batch 1166 for epoch number 0\n",
            "[0.025172872, 1.0, 0.06666666644444444, 0.0026881720426494395]\n",
            "training on batch 1167 for epoch number 0\n",
            "[0.024816513, 1.0, 0.06666666644444444, 0.0026881720426494395]\n",
            "training on batch 1168 for epoch number 0\n",
            "[0.024428336, 1.0, 0.06666666644444444, 0.0026881720426494395]\n",
            "training on batch 1169 for epoch number 0\n",
            "[0.25690395, 0.9375, 0.06666666644444444, 0.002684563758028918]\n",
            "training on batch 1170 for epoch number 0\n",
            "[0.023771806, 1.0, 0.06666666644444444, 0.002684563758028918]\n",
            "training on batch 1171 for epoch number 0\n",
            "[0.023487987, 1.0, 0.06666666644444444, 0.002684563758028918]\n",
            "training on batch 1172 for epoch number 0\n",
            "[0.25818956, 0.9375, 0.06666666644444444, 0.0026809651470937044]\n",
            "training on batch 1173 for epoch number 0\n",
            "[0.023010572, 1.0, 0.06666666644444444, 0.0026809651470937044]\n",
            "training on batch 1174 for epoch number 0\n",
            "[0.022802364, 1.0, 0.06666666644444444, 0.0026809651470937044]\n",
            "training on batch 1175 for epoch number 0\n",
            "[0.02254051, 1.0, 0.06666666644444444, 0.0026809651470937044]\n",
            "training on batch 1176 for epoch number 0\n",
            "[0.022235855, 1.0, 0.06666666644444444, 0.0026809651470937044]\n",
            "training on batch 1177 for epoch number 0\n",
            "[0.021900391, 1.0, 0.06666666644444444, 0.0026809651470937044]\n",
            "training on batch 1178 for epoch number 0\n",
            "[0.2610531, 0.9375, 0.06666666644444444, 0.002677376170993658]\n",
            "training on batch 1179 for epoch number 0\n",
            "[0.021350848, 1.0, 0.06666666644444444, 0.002677376170993658]\n",
            "training on batch 1180 for epoch number 0\n",
            "[0.02112199, 1.0, 0.06666666644444444, 0.002677376170993658]\n",
            "training on batch 1181 for epoch number 0\n",
            "[0.02085898, 1.0, 0.06666666644444444, 0.002677376170993658]\n",
            "training on batch 1182 for epoch number 0\n",
            "[0.02056583, 1.0, 0.06666666644444444, 0.002677376170993658]\n",
            "training on batch 1183 for epoch number 0\n",
            "[0.020249294, 1.0, 0.06666666644444444, 0.002677376170993658]\n",
            "training on batch 1184 for epoch number 0\n",
            "[0.019915525, 1.0, 0.06666666644444444, 0.002677376170993658]\n",
            "training on batch 1185 for epoch number 0\n",
            "[0.26515353, 0.9375, 0.06666666644444444, 0.002673796791086391]\n",
            "training on batch 1186 for epoch number 0\n",
            "[0.01937927, 1.0, 0.06666666644444444, 0.002673796791086391]\n",
            "training on batch 1187 for epoch number 0\n",
            "[0.019164786, 1.0, 0.06666666644444444, 0.002673796791086391]\n",
            "training on batch 1188 for epoch number 0\n",
            "[0.018923001, 1.0, 0.06666666644444444, 0.002673796791086391]\n",
            "training on batch 1189 for epoch number 0\n",
            "[0.018655417, 1.0, 0.06666666644444444, 0.002673796791086391]\n",
            "training on batch 1190 for epoch number 0\n",
            "[0.26781285, 0.9375, 0.06666666644444444, 0.0026702269689358846]\n",
            "training on batch 1191 for epoch number 0\n",
            "[0.0182426, 1.0, 0.06666666644444444, 0.0026702269689358846]\n",
            "training on batch 1192 for epoch number 0\n",
            "[0.018083032, 1.0, 0.06666666644444444, 0.0026702269689358846]\n",
            "training on batch 1193 for epoch number 0\n",
            "[0.017887978, 1.0, 0.06666666644444444, 0.0026702269689358846]\n",
            "training on batch 1194 for epoch number 0\n",
            "[0.017662425, 1.0, 0.06666666644444444, 0.0026702269689358846]\n",
            "training on batch 1195 for epoch number 0\n",
            "[0.52286416, 0.875, 0.06666666644444444, 0.0026631158451846716]\n",
            "training on batch 1196 for epoch number 0\n",
            "[0.017519195, 1.0, 0.06666666644444444, 0.0026631158451846716]\n",
            "training on batch 1197 for epoch number 0\n",
            "[0.017578209, 1.0, 0.06666666644444444, 0.0026631158451846716]\n",
            "training on batch 1198 for epoch number 0\n",
            "[0.017565401, 1.0, 0.06666666644444444, 0.0026631158451846716]\n",
            "training on batch 1199 for epoch number 0\n",
            "[0.017481958, 1.0, 0.06666666644444444, 0.0026631158451846716]\n",
            "training on batch 1200 for epoch number 0\n",
            "[0.27026567, 0.9375, 0.06666666644444444, 0.0026595744677314396]\n",
            "training on batch 1201 for epoch number 0\n",
            "[0.01737177, 1.0, 0.06666666644444444, 0.0026595744677314396]\n",
            "training on batch 1202 for epoch number 0\n",
            "[0.017335897, 1.0, 0.06666666644444444, 0.0026595744677314396]\n",
            "training on batch 1203 for epoch number 0\n",
            "[0.017248595, 1.0, 0.06666666644444444, 0.0026595744677314396]\n",
            "training on batch 1204 for epoch number 0\n",
            "[0.017121503, 1.0, 0.06666666644444444, 0.0026595744677314396]\n",
            "training on batch 1205 for epoch number 0\n",
            "[0.016964408, 1.0, 0.06666666644444444, 0.0026595744677314396]\n",
            "training on batch 1206 for epoch number 0\n",
            "[0.016784614, 1.0, 0.06666666644444444, 0.0026595744677314396]\n",
            "training on batch 1207 for epoch number 0\n",
            "[0.016586415, 1.0, 0.06666666644444444, 0.0026595744677314396]\n",
            "training on batch 1208 for epoch number 0\n",
            "[0.27315354, 0.9375, 0.06666666644444444, 0.002656042496327219]\n",
            "training on batch 1209 for epoch number 0\n",
            "[0.27329594, 0.9375, 0.06666666644444444, 0.0026525198935474113]\n",
            "training on batch 1210 for epoch number 0\n",
            "[0.0163699, 1.0, 0.06666666644444444, 0.0026525198935474113]\n",
            "training on batch 1211 for epoch number 0\n",
            "[0.016412098, 1.0, 0.06666666644444444, 0.0026525198935474113]\n",
            "training on batch 1212 for epoch number 0\n",
            "[0.016393535, 1.0, 0.06666666644444444, 0.0026525198935474113]\n",
            "training on batch 1213 for epoch number 0\n",
            "[0.016316067, 1.0, 0.06666666644444444, 0.0026525198935474113]\n",
            "training on batch 1214 for epoch number 0\n",
            "[0.01619464, 1.0, 0.06666666644444444, 0.0026525198935474113]\n",
            "training on batch 1215 for epoch number 0\n",
            "[0.016043268, 1.0, 0.06666666644444444, 0.0026525198935474113]\n",
            "training on batch 1216 for epoch number 0\n",
            "[0.015871, 1.0, 0.06666666644444444, 0.0026525198935474113]\n",
            "training on batch 1217 for epoch number 0\n",
            "[0.01568313, 1.0, 0.06666666644444444, 0.0026525198935474113]\n",
            "training on batch 1218 for epoch number 0\n",
            "[0.015482234, 1.0, 0.06666666644444444, 0.0026525198935474113]\n",
            "training on batch 1219 for epoch number 0\n",
            "[0.2765022, 0.9375, 0.06666666644444444, 0.0026490066221656946]\n",
            "training on batch 1220 for epoch number 0\n",
            "[0.015189478, 1.0, 0.06666666644444444, 0.0026490066221656946]\n",
            "training on batch 1221 for epoch number 0\n",
            "[0.01509103, 1.0, 0.06666666644444444, 0.0026490066221656946]\n",
            "training on batch 1222 for epoch number 0\n",
            "[0.27729672, 0.9375, 0.06666666644444444, 0.0026455026451527113]\n",
            "training on batch 1223 for epoch number 0\n",
            "[0.014997597, 1.0, 0.06666666644444444, 0.0026455026451527113]\n",
            "training on batch 1224 for epoch number 0\n",
            "[0.014985451, 1.0, 0.06666666644444444, 0.0026455026451527113]\n",
            "training on batch 1225 for epoch number 0\n",
            "[0.01492429, 1.0, 0.06666666644444444, 0.0026455026451527113]\n",
            "training on batch 1226 for epoch number 0\n",
            "[0.014821102, 1.0, 0.06666666644444444, 0.0026455026451527113]\n",
            "training on batch 1227 for epoch number 0\n",
            "[0.27813324, 0.9375, 0.06666666644444444, 0.0026420079256747678]\n",
            "training on batch 1228 for epoch number 0\n",
            "[0.014705972, 1.0, 0.06666666644444444, 0.0026420079256747678]\n",
            "training on batch 1229 for epoch number 0\n",
            "[0.2781305, 0.9375, 0.06666666644444444, 0.0026385224270925435]\n",
            "training on batch 1230 for epoch number 0\n",
            "[0.014805703, 1.0, 0.06666666644444444, 0.0026385224270925435]\n",
            "training on batch 1231 for epoch number 0\n",
            "[0.014869812, 1.0, 0.06666666644444444, 0.0026385224270925435]\n",
            "training on batch 1232 for epoch number 0\n",
            "[0.0148706995, 1.0, 0.06666666644444444, 0.0026385224270925435]\n",
            "training on batch 1233 for epoch number 0\n",
            "[0.014821092, 1.0, 0.06666666644444444, 0.0026385224270925435]\n",
            "training on batch 1234 for epoch number 0\n",
            "[0.27793947, 0.9375, 0.06666666644444444, 0.0026350461129598097]\n",
            "training on batch 1235 for epoch number 0\n",
            "[0.014799708, 1.0, 0.06666666644444444, 0.0026350461129598097]\n",
            "training on batch 1236 for epoch number 0\n",
            "[0.27767032, 0.9375, 0.06666666644444444, 0.0026315789470221606]\n",
            "training on batch 1237 for epoch number 0\n",
            "[0.014973839, 1.0, 0.06666666644444444, 0.0026315789470221606]\n",
            "training on batch 1238 for epoch number 0\n",
            "[0.015070604, 1.0, 0.06666666644444444, 0.0026315789470221606]\n",
            "training on batch 1239 for epoch number 0\n",
            "[0.015101209, 1.0, 0.06666666644444444, 0.0026315789470221606]\n",
            "training on batch 1240 for epoch number 0\n",
            "[0.27682802, 0.9375, 0.06666666644444444, 0.002628120893215753]\n",
            "training on batch 1241 for epoch number 0\n",
            "[0.27644446, 0.9375, 0.06666666644444444, 0.0026246719156660537]\n",
            "training on batch 1242 for epoch number 0\n",
            "[0.015470081, 1.0, 0.06666666644444444, 0.0026246719156660537]\n",
            "training on batch 1243 for epoch number 0\n",
            "[0.27517298, 0.9375, 0.06666666644444444, 0.0026212319786866013]\n",
            "training on batch 1244 for epoch number 0\n",
            "[0.015979804, 1.0, 0.06666666644444444, 0.0026212319786866013]\n",
            "training on batch 1245 for epoch number 0\n",
            "[0.01619155, 1.0, 0.06666666644444444, 0.0026212319786866013]\n",
            "training on batch 1246 for epoch number 0\n",
            "[0.01630044, 1.0, 0.06666666644444444, 0.0026212319786866013]\n",
            "training on batch 1247 for epoch number 0\n",
            "[0.53004277, 0.875, 0.06666666644444444, 0.0026143790846255713]\n",
            "training on batch 1248 for epoch number 0\n",
            "[0.016736927, 1.0, 0.06666666644444444, 0.0026143790846255713]\n",
            "training on batch 1249 for epoch number 0\n",
            "[0.017035337, 1.0, 0.06666666644444444, 0.0026143790846255713]\n",
            "training on batch 1250 for epoch number 0\n",
            "[0.27083033, 0.9375, 0.06666666644444444, 0.002610966057100396]\n",
            "training on batch 1251 for epoch number 0\n",
            "[0.2700139, 0.9375, 0.06666666644444444, 0.002607561929255859]\n",
            "training on batch 1252 for epoch number 0\n",
            "[0.018003123, 1.0, 0.06666666644444444, 0.002607561929255859]\n",
            "training on batch 1253 for epoch number 0\n",
            "[0.018317427, 1.0, 0.06666666644444444, 0.002607561929255859]\n",
            "training on batch 1254 for epoch number 0\n",
            "[0.018504996, 1.0, 0.06666666644444444, 0.002607561929255859]\n",
            "training on batch 1255 for epoch number 0\n",
            "[0.018598827, 1.0, 0.06666666644444444, 0.002607561929255859]\n",
            "training on batch 1256 for epoch number 0\n",
            "[0.018628646, 1.0, 0.06666666644444444, 0.002607561929255859]\n",
            "training on batch 1257 for epoch number 0\n",
            "[0.018612718, 1.0, 0.06666666644444444, 0.002607561929255859]\n",
            "training on batch 1258 for epoch number 0\n",
            "[0.018559696, 1.0, 0.06666666644444444, 0.002607561929255859]\n",
            "training on batch 1259 for epoch number 0\n",
            "[0.26776618, 0.9375, 0.06666666644444444, 0.0026041666663275827]\n",
            "training on batch 1260 for epoch number 0\n",
            "[0.01850812, 1.0, 0.06666666644444444, 0.0026041666663275827]\n",
            "training on batch 1261 for epoch number 0\n",
            "[0.018499823, 1.0, 0.06666666644444444, 0.0026041666663275827]\n",
            "training on batch 1262 for epoch number 0\n",
            "[0.018447172, 1.0, 0.06666666644444444, 0.0026041666663275827]\n",
            "training on batch 1263 for epoch number 0\n",
            "[0.018352255, 1.0, 0.06666666644444444, 0.0026041666663275827]\n",
            "training on batch 1264 for epoch number 0\n",
            "[0.018220775, 1.0, 0.06666666644444444, 0.0026041666663275827]\n",
            "training on batch 1265 for epoch number 0\n",
            "[0.01805854, 1.0, 0.06666666644444444, 0.0026041666663275827]\n",
            "training on batch 1266 for epoch number 0\n",
            "[0.01787019, 1.0, 0.06666666644444444, 0.0026041666663275827]\n",
            "training on batch 1267 for epoch number 0\n",
            "[0.017659742, 1.0, 0.06666666644444444, 0.0026041666663275827]\n",
            "training on batch 1268 for epoch number 0\n",
            "[0.017431023, 1.0, 0.06666666644444444, 0.0026041666663275827]\n",
            "training on batch 1269 for epoch number 0\n",
            "[0.017186832, 1.0, 0.06666666644444444, 0.0026041666663275827]\n",
            "training on batch 1270 for epoch number 0\n",
            "[0.2718768, 0.9375, 0.06666666644444444, 0.002600780233732018]\n",
            "training on batch 1271 for epoch number 0\n",
            "[0.016812569, 1.0, 0.06666666644444444, 0.002600780233732018]\n",
            "training on batch 1272 for epoch number 0\n",
            "[0.016677298, 1.0, 0.06666666644444444, 0.002600780233732018]\n",
            "training on batch 1273 for epoch number 0\n",
            "[0.016515438, 1.0, 0.06666666644444444, 0.002600780233732018]\n",
            "training on batch 1274 for epoch number 0\n",
            "[0.016326778, 1.0, 0.06666666644444444, 0.002600780233732018]\n",
            "training on batch 1275 for epoch number 0\n",
            "[0.016115867, 1.0, 0.06666666644444444, 0.002600780233732018]\n",
            "training on batch 1276 for epoch number 0\n",
            "[0.015888067, 1.0, 0.06666666644444444, 0.002600780233732018]\n",
            "training on batch 1277 for epoch number 0\n",
            "[0.015648035, 1.0, 0.06666666644444444, 0.002600780233732018]\n",
            "training on batch 1278 for epoch number 0\n",
            "[0.27614483, 0.9375, 0.06666666644444444, 0.0025974025970652726]\n",
            "training on batch 1279 for epoch number 0\n",
            "[0.015295663, 1.0, 0.06666666644444444, 0.0025974025970652726]\n",
            "training on batch 1280 for epoch number 0\n",
            "[0.015173274, 1.0, 0.06666666644444444, 0.0025974025970652726]\n",
            "training on batch 1281 for epoch number 0\n",
            "[0.015025697, 1.0, 0.06666666644444444, 0.0025974025970652726]\n",
            "training on batch 1282 for epoch number 0\n",
            "[0.01485358, 1.0, 0.06666666644444444, 0.0025974025970652726]\n",
            "training on batch 1283 for epoch number 0\n",
            "[0.014662513, 1.0, 0.06666666644444444, 0.0025974025970652726]\n",
            "training on batch 1284 for epoch number 0\n",
            "[0.014458363, 1.0, 0.06666666644444444, 0.0025974025970652726]\n",
            "training on batch 1285 for epoch number 0\n",
            "[0.014244449, 1.0, 0.06666666644444444, 0.0025974025970652726]\n",
            "training on batch 1286 for epoch number 0\n",
            "[0.014022587, 1.0, 0.06666666644444444, 0.0025974025970652726]\n",
            "training on batch 1287 for epoch number 0\n",
            "[0.2813893, 0.9375, 0.06666666644444444, 0.002594033722101941]\n",
            "training on batch 1288 for epoch number 0\n",
            "[0.013706314, 1.0, 0.06666666644444444, 0.002594033722101941]\n",
            "training on batch 1289 for epoch number 0\n",
            "[0.013605072, 1.0, 0.06666666644444444, 0.002594033722101941]\n",
            "training on batch 1290 for epoch number 0\n",
            "[0.013482093, 1.0, 0.06666666644444444, 0.002594033722101941]\n",
            "training on batch 1291 for epoch number 0\n",
            "[0.013335948, 1.0, 0.06666666644444444, 0.002594033722101941]\n",
            "training on batch 1292 for epoch number 0\n",
            "[0.013171997, 1.0, 0.06666666644444444, 0.002594033722101941]\n",
            "training on batch 1293 for epoch number 0\n",
            "[0.012995848, 1.0, 0.06666666644444444, 0.002594033722101941]\n",
            "training on batch 1294 for epoch number 0\n",
            "[0.28490317, 0.9375, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1295 for epoch number 0\n",
            "[0.012767554, 1.0, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1296 for epoch number 0\n",
            "[0.012704091, 1.0, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1297 for epoch number 0\n",
            "[0.01261485, 1.0, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1298 for epoch number 0\n",
            "[0.01250108, 1.0, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1299 for epoch number 0\n",
            "[0.012368278, 1.0, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1300 for epoch number 0\n",
            "[0.012222052, 1.0, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1301 for epoch number 0\n",
            "[0.012066057, 1.0, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1302 for epoch number 0\n",
            "[0.011902711, 1.0, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1303 for epoch number 0\n",
            "[0.011733206, 1.0, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1304 for epoch number 0\n",
            "[0.011557851, 1.0, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1305 for epoch number 0\n",
            "[0.011377797, 1.0, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1306 for epoch number 0\n",
            "[0.011194277, 1.0, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1307 for epoch number 0\n",
            "[0.011008177, 1.0, 0.06666666644444444, 0.002590673574793954]\n",
            "training on batch 1308 for epoch number 0\n",
            "[0.29373217, 0.9375, 0.06666666644444444, 0.002587322121269428]\n",
            "training on batch 1309 for epoch number 0\n",
            "[0.5769713, 0.875, 0.06666666644444444, 0.0025806451609573363]\n",
            "training on batch 1310 for epoch number 0\n",
            "[0.011068269, 1.0, 0.06666666644444444, 0.0025806451609573363]\n",
            "training on batch 1311 for epoch number 0\n",
            "[0.8526566, 0.8125, 0.06666666644444444, 0.0025706940870731756]\n",
            "training on batch 1312 for epoch number 0\n",
            "[0.56638575, 0.875, 0.06666666644444444, 0.002564102563773833]\n",
            "training on batch 1313 for epoch number 0\n",
            "[0.013936959, 1.0, 0.06666666644444444, 0.002564102563773833]\n",
            "training on batch 1314 for epoch number 0\n",
            "[0.0150504485, 1.0, 0.06666666644444444, 0.002564102563773833]\n",
            "training on batch 1315 for epoch number 0\n",
            "[0.015638772, 1.0, 0.06666666644444444, 0.002564102563773833]\n",
            "training on batch 1316 for epoch number 0\n",
            "[0.015876556, 1.0, 0.06666666644444444, 0.002564102563773833]\n",
            "training on batch 1317 for epoch number 0\n",
            "[0.27869266, 0.9375, 0.06666666644444444, 0.002560819461900023]\n",
            "training on batch 1318 for epoch number 0\n",
            "[0.016264401, 1.0, 0.06666666644444444, 0.002560819461900023]\n",
            "training on batch 1319 for epoch number 0\n",
            "[0.016475357, 1.0, 0.06666666644444444, 0.002560819461900023]\n",
            "training on batch 1320 for epoch number 0\n",
            "[0.27518186, 0.9375, 0.06666666644444444, 0.0025575447567061965]\n",
            "training on batch 1321 for epoch number 0\n",
            "[0.016922908, 1.0, 0.06666666644444444, 0.0025575447567061965]\n",
            "training on batch 1322 for epoch number 0\n",
            "[0.529108, 0.875, 0.06666666644444444, 0.00255102040783788]\n",
            "training on batch 1323 for epoch number 0\n",
            "[0.27141032, 0.9375, 0.06666666644444444, 0.002547770700312386]\n",
            "training on batch 1324 for epoch number 0\n",
            "[0.01837992, 1.0, 0.06666666644444444, 0.002547770700312386]\n",
            "training on batch 1325 for epoch number 0\n",
            "[0.26819372, 0.9375, 0.06666666644444444, 0.0025445292617627827]\n",
            "training on batch 1326 for epoch number 0\n",
            "[0.019616479, 1.0, 0.06666666644444444, 0.0025445292617627827]\n",
            "training on batch 1327 for epoch number 0\n",
            "[0.020080099, 1.0, 0.06666666644444444, 0.0025445292617627827]\n",
            "training on batch 1328 for epoch number 0\n",
            "[0.26455393, 0.9375, 0.06666666644444444, 0.0025412960606681963]\n",
            "training on batch 1329 for epoch number 0\n",
            "[0.020792345, 1.0, 0.06666666644444444, 0.0025412960606681963]\n",
            "training on batch 1330 for epoch number 0\n",
            "[0.021076059, 1.0, 0.06666666644444444, 0.0025412960606681963]\n",
            "training on batch 1331 for epoch number 0\n",
            "[0.5031195, 0.875, 0.06666666644444444, 0.002534854245559588]\n",
            "training on batch 1332 for epoch number 0\n",
            "[0.021805638, 1.0, 0.06666666644444444, 0.002534854245559588]\n",
            "training on batch 1333 for epoch number 0\n",
            "[0.022219205, 1.0, 0.06666666644444444, 0.002534854245559588]\n",
            "training on batch 1334 for epoch number 0\n",
            "[0.022495875, 1.0, 0.06666666644444444, 0.002534854245559588]\n",
            "training on batch 1335 for epoch number 0\n",
            "[0.022661306, 1.0, 0.06666666644444444, 0.002534854245559588]\n",
            "training on batch 1336 for epoch number 0\n",
            "[0.022744348, 1.0, 0.06666666644444444, 0.002534854245559588]\n",
            "training on batch 1337 for epoch number 0\n",
            "[0.022762924, 1.0, 0.06666666644444444, 0.002534854245559588]\n",
            "training on batch 1338 for epoch number 0\n",
            "[0.25903547, 0.9375, 0.06666666644444444, 0.0025316455692997917]\n",
            "training on batch 1339 for epoch number 0\n",
            "[0.02281221, 1.0, 0.06666666644444444, 0.0025316455692997917]\n",
            "training on batch 1340 for epoch number 0\n",
            "[0.022832733, 1.0, 0.06666666644444444, 0.0025316455692997917]\n",
            "training on batch 1341 for epoch number 0\n",
            "[0.25881338, 0.9375, 0.06666666644444444, 0.002528445006001461]\n",
            "training on batch 1342 for epoch number 0\n",
            "[0.022889206, 1.0, 0.06666666644444444, 0.002528445006001461]\n",
            "training on batch 1343 for epoch number 0\n",
            "[0.022914901, 1.0, 0.06666666644444444, 0.002528445006001461]\n",
            "training on batch 1344 for epoch number 0\n",
            "[0.022867423, 1.0, 0.06666666644444444, 0.002528445006001461]\n",
            "training on batch 1345 for epoch number 0\n",
            "[0.022756154, 1.0, 0.06666666644444444, 0.002528445006001461]\n",
            "training on batch 1346 for epoch number 0\n",
            "[0.022591675, 1.0, 0.06666666644444444, 0.002528445006001461]\n",
            "training on batch 1347 for epoch number 0\n",
            "[0.25952446, 0.9375, 0.06666666644444444, 0.0025252525249336803]\n",
            "training on batch 1348 for epoch number 0\n",
            "[0.022321539, 1.0, 0.06666666644444444, 0.0025252525249336803]\n",
            "training on batch 1349 for epoch number 0\n",
            "[0.49734223, 0.875, 0.06666666644444444, 0.00251889168734019]\n",
            "training on batch 1350 for epoch number 0\n",
            "[0.0224686, 1.0, 0.06666666644444444, 0.00251889168734019]\n",
            "training on batch 1351 for epoch number 0\n",
            "[0.022654815, 1.0, 0.06666666644444444, 0.00251889168734019]\n",
            "training on batch 1352 for epoch number 0\n",
            "[0.022718366, 1.0, 0.06666666644444444, 0.00251889168734019]\n",
            "training on batch 1353 for epoch number 0\n",
            "[0.022666793, 1.0, 0.06666666644444444, 0.00251889168734019]\n",
            "training on batch 1354 for epoch number 0\n",
            "[0.022534428, 1.0, 0.06666666644444444, 0.00251889168734019]\n",
            "training on batch 1355 for epoch number 0\n",
            "[0.02235107, 1.0, 0.06666666644444444, 0.00251889168734019]\n",
            "training on batch 1356 for epoch number 0\n",
            "[0.25985682, 0.9375, 0.06666666644444444, 0.0025157232701238084]\n",
            "training on batch 1357 for epoch number 0\n",
            "[0.02206746, 1.0, 0.06666666644444444, 0.0025157232701238084]\n",
            "training on batch 1358 for epoch number 0\n",
            "[0.021947473, 1.0, 0.06666666644444444, 0.0025157232701238084]\n",
            "training on batch 1359 for epoch number 0\n",
            "[0.021779615, 1.0, 0.06666666644444444, 0.0025157232701238084]\n",
            "training on batch 1360 for epoch number 0\n",
            "[0.02157007, 1.0, 0.06666666644444444, 0.0025157232701238084]\n",
            "training on batch 1361 for epoch number 0\n",
            "[0.021325583, 1.0, 0.06666666644444444, 0.0025157232701238084]\n",
            "training on batch 1362 for epoch number 0\n",
            "[0.02105321, 1.0, 0.06666666644444444, 0.0025157232701238084]\n",
            "training on batch 1363 for epoch number 0\n",
            "[0.020758968, 1.0, 0.06666666644444444, 0.0025157232701238084]\n",
            "training on batch 1364 for epoch number 0\n",
            "[0.7492211, 0.8125, 0.06666666644444444, 0.0025062656638463326]\n",
            "training on batch 1365 for epoch number 0\n",
            "[0.020660238, 1.0, 0.06666666644444444, 0.0025062656638463326]\n",
            "training on batch 1366 for epoch number 0\n",
            "[0.020873476, 1.0, 0.06666666644444444, 0.0025062656638463326]\n",
            "training on batch 1367 for epoch number 0\n",
            "[0.020987643, 1.0, 0.06666666644444444, 0.0025062656638463326]\n",
            "training on batch 1368 for epoch number 0\n",
            "[0.020974386, 1.0, 0.06666666644444444, 0.0025062656638463326]\n",
            "training on batch 1369 for epoch number 0\n",
            "[0.020859595, 1.0, 0.06666666644444444, 0.0025062656638463326]\n",
            "training on batch 1370 for epoch number 0\n",
            "[0.26262793, 0.9375, 0.06666666644444444, 0.002503128910825641]\n",
            "training on batch 1371 for epoch number 0\n",
            "[0.020676268, 1.0, 0.06666666644444444, 0.002503128910825641]\n",
            "training on batch 1372 for epoch number 0\n",
            "[0.020602027, 1.0, 0.06666666644444444, 0.002503128910825641]\n",
            "training on batch 1373 for epoch number 0\n",
            "[0.020473123, 1.0, 0.06666666644444444, 0.002503128910825641]\n",
            "training on batch 1374 for epoch number 0\n",
            "[0.020300826, 1.0, 0.06666666644444444, 0.002503128910825641]\n",
            "training on batch 1375 for epoch number 0\n",
            "[0.020096928, 1.0, 0.06666666644444444, 0.002503128910825641]\n",
            "training on batch 1376 for epoch number 0\n",
            "[0.01986835, 1.0, 0.06666666644444444, 0.002503128910825641]\n",
            "training on batch 1377 for epoch number 0\n",
            "[0.01961766, 1.0, 0.06666666644444444, 0.002503128910825641]\n",
            "training on batch 1378 for epoch number 0\n",
            "[0.26576847, 0.9375, 0.06666666644444444, 0.0024999999996875]\n",
            "training on batch 1379 for epoch number 0\n",
            "[0.01921246, 1.0, 0.06666666644444444, 0.0024999999996875]\n",
            "training on batch 1380 for epoch number 0\n",
            "[0.019055324, 1.0, 0.06666666644444444, 0.0024999999996875]\n",
            "training on batch 1381 for epoch number 0\n",
            "[0.018868508, 1.0, 0.06666666644444444, 0.0024999999996875]\n",
            "training on batch 1382 for epoch number 0\n",
            "[0.018651234, 1.0, 0.06666666644444444, 0.0024999999996875]\n",
            "training on batch 1383 for epoch number 0\n",
            "[0.26777658, 0.9375, 0.06666666644444444, 0.002496878901061563]\n",
            "training on batch 1384 for epoch number 0\n",
            "[0.018323977, 1.0, 0.06666666644444444, 0.002496878901061563]\n",
            "training on batch 1385 for epoch number 0\n",
            "[0.01820015, 1.0, 0.06666666644444444, 0.002496878901061563]\n",
            "training on batch 1386 for epoch number 0\n",
            "[0.01803545, 1.0, 0.06666666644444444, 0.002496878901061563]\n",
            "training on batch 1387 for epoch number 0\n",
            "[0.0178363, 1.0, 0.06666666644444444, 0.002496878901061563]\n",
            "training on batch 1388 for epoch number 0\n",
            "[0.01761244, 1.0, 0.06666666644444444, 0.002496878901061563]\n",
            "training on batch 1389 for epoch number 0\n",
            "[0.017371986, 1.0, 0.06666666644444444, 0.002496878901061563]\n",
            "training on batch 1390 for epoch number 0\n",
            "[0.017119518, 1.0, 0.06666666644444444, 0.002496878901061563]\n",
            "training on batch 1391 for epoch number 0\n",
            "[0.271836, 0.9375, 0.06666666644444444, 0.0024937655857239695]\n",
            "training on batch 1392 for epoch number 0\n",
            "[0.527439, 0.875, 0.06666666644444444, 0.002487562188745328]\n",
            "training on batch 1393 for epoch number 0\n",
            "[0.016966254, 1.0, 0.06666666644444444, 0.002487562188745328]\n",
            "training on batch 1394 for epoch number 0\n",
            "[0.017178336, 1.0, 0.06666666644444444, 0.002487562188745328]\n",
            "training on batch 1395 for epoch number 0\n",
            "[0.017287632, 1.0, 0.06666666644444444, 0.002487562188745328]\n",
            "training on batch 1396 for epoch number 0\n",
            "[0.2707461, 0.9375, 0.06666666644444444, 0.0024844720493808112]\n",
            "training on batch 1397 for epoch number 0\n",
            "[0.017432408, 1.0, 0.06666666644444444, 0.0024844720493808112]\n",
            "training on batch 1398 for epoch number 0\n",
            "[0.5229954, 0.875, 0.06666666644444444, 0.0024783147456656366]\n",
            "training on batch 1399 for epoch number 0\n",
            "[0.772148, 0.8125, 0.06666666644444444, 0.0024691358021643045]\n",
            "training on batch 1400 for epoch number 0\n",
            "[0.019076008, 1.0, 0.06666666644444444, 0.0024691358021643045]\n",
            "training on batch 1401 for epoch number 0\n",
            "[0.02002979, 1.0, 0.06666666644444444, 0.0024691358021643045]\n",
            "training on batch 1402 for epoch number 0\n",
            "[0.26487884, 0.9375, 0.06666666644444444, 0.002466091245071999]\n",
            "training on batch 1403 for epoch number 0\n",
            "[0.26360518, 0.9375, 0.06666666644444444, 0.0024630541868887867]\n",
            "training on batch 1404 for epoch number 0\n",
            "[0.50260115, 0.875, 0.06666666644444444, 0.002457002456700614]\n",
            "training on batch 1405 for epoch number 0\n",
            "[0.02301444, 1.0, 0.06666666644444444, 0.002457002456700614]\n",
            "training on batch 1406 for epoch number 0\n",
            "[0.023745138, 1.0, 0.06666666644444444, 0.002457002456700614]\n",
            "training on batch 1407 for epoch number 0\n",
            "[0.024183663, 1.0, 0.06666666644444444, 0.002457002456700614]\n",
            "training on batch 1408 for epoch number 0\n",
            "[0.02443101, 1.0, 0.06666666644444444, 0.002457002456700614]\n",
            "training on batch 1409 for epoch number 0\n",
            "[0.02457449, 1.0, 0.06666666644444444, 0.002457002456700614]\n",
            "training on batch 1410 for epoch number 0\n",
            "[0.024658715, 1.0, 0.06666666644444444, 0.002457002456700614]\n",
            "training on batch 1411 for epoch number 0\n",
            "[0.02469408, 1.0, 0.06666666644444444, 0.002457002456700614]\n",
            "training on batch 1412 for epoch number 0\n",
            "[0.024676466, 1.0, 0.06666666644444444, 0.002457002456700614]\n",
            "training on batch 1413 for epoch number 0\n",
            "[0.25679713, 0.9375, 0.06666666644444444, 0.002453987729760247]\n",
            "training on batch 1414 for epoch number 0\n",
            "[0.024625015, 1.0, 0.06666666644444444, 0.002453987729760247]\n",
            "training on batch 1415 for epoch number 0\n",
            "[0.024592524, 1.0, 0.06666666644444444, 0.002453987729760247]\n",
            "training on batch 1416 for epoch number 0\n",
            "[0.02450607, 1.0, 0.06666666644444444, 0.002453987729760247]\n",
            "training on batch 1417 for epoch number 0\n",
            "[0.024366422, 1.0, 0.06666666644444444, 0.002453987729760247]\n",
            "training on batch 1418 for epoch number 0\n",
            "[0.024177248, 1.0, 0.06666666644444444, 0.002453987729760247]\n",
            "training on batch 1419 for epoch number 0\n",
            "[0.02394509, 1.0, 0.06666666644444444, 0.002453987729760247]\n",
            "training on batch 1420 for epoch number 0\n",
            "[0.023675755, 1.0, 0.06666666644444444, 0.002453987729760247]\n",
            "training on batch 1421 for epoch number 0\n",
            "[0.023375843, 1.0, 0.06666666644444444, 0.002453987729760247]\n",
            "training on batch 1422 for epoch number 0\n",
            "[0.023051256, 1.0, 0.06666666644444444, 0.002453987729760247]\n",
            "training on batch 1423 for epoch number 0\n",
            "[0.25959232, 0.9375, 0.06666666644444444, 0.0024509803918564975]\n",
            "training on batch 1424 for epoch number 0\n",
            "[0.49714547, 0.875, 0.06666666644444444, 0.0024449877747622266]\n",
            "training on batch 1425 for epoch number 0\n",
            "[0.25931606, 0.9375, 0.06666666644444444, 0.0024420024417042734]\n",
            "training on batch 1426 for epoch number 0\n",
            "[0.72991616, 0.8125, 0.06666666644444444, 0.002433090024034904]\n",
            "training on batch 1427 for epoch number 0\n",
            "[0.02435521, 1.0, 0.06666666644444444, 0.002433090024034904]\n",
            "training on batch 1428 for epoch number 0\n",
            "[0.025355197, 1.0, 0.06666666644444444, 0.002433090024034904]\n",
            "training on batch 1429 for epoch number 0\n",
            "[0.025899032, 1.0, 0.06666666644444444, 0.002433090024034904]\n",
            "training on batch 1430 for epoch number 0\n",
            "[0.026062844, 1.0, 0.06666666644444444, 0.002433090024034904]\n",
            "training on batch 1431 for epoch number 0\n",
            "[0.25478843, 0.9375, 0.06666666644444444, 0.002430133657055877]\n",
            "training on batch 1432 for epoch number 0\n",
            "[0.2543477, 0.9375, 0.06666666644444444, 0.0024271844657248564]\n",
            "training on batch 1433 for epoch number 0\n",
            "[0.25379452, 0.9375, 0.06666666644444444, 0.002424242423948577]\n",
            "training on batch 1434 for epoch number 0\n",
            "[0.026872631, 1.0, 0.06666666644444444, 0.002424242423948577]\n",
            "training on batch 1435 for epoch number 0\n",
            "[0.027122471, 1.0, 0.06666666644444444, 0.002424242423948577]\n",
            "training on batch 1436 for epoch number 0\n",
            "[0.027227156, 1.0, 0.06666666644444444, 0.002424242423948577]\n",
            "training on batch 1437 for epoch number 0\n",
            "[0.02722415, 1.0, 0.06666666644444444, 0.002424242423948577]\n",
            "training on batch 1438 for epoch number 0\n",
            "[0.027144143, 1.0, 0.06666666644444444, 0.002424242423948577]\n",
            "training on batch 1439 for epoch number 0\n",
            "[0.027002973, 1.0, 0.06666666644444444, 0.002424242423948577]\n",
            "training on batch 1440 for epoch number 0\n",
            "[0.026807947, 1.0, 0.06666666644444444, 0.002424242423948577]\n",
            "training on batch 1441 for epoch number 0\n",
            "[0.25344905, 0.9375, 0.06666666644444444, 0.0024213075057601323]\n",
            "training on batch 1442 for epoch number 0\n",
            "[0.026442787, 1.0, 0.06666666644444444, 0.0024213075057601323]\n",
            "training on batch 1443 for epoch number 0\n",
            "[0.026270196, 1.0, 0.06666666644444444, 0.0024213075057601323]\n",
            "training on batch 1444 for epoch number 0\n",
            "[0.026046602, 1.0, 0.06666666644444444, 0.0024213075057601323]\n",
            "training on batch 1445 for epoch number 0\n",
            "[0.025774362, 1.0, 0.06666666644444444, 0.0024213075057601323]\n",
            "training on batch 1446 for epoch number 0\n",
            "[0.025458032, 1.0, 0.06666666644444444, 0.0024213075057601323]\n",
            "training on batch 1447 for epoch number 0\n",
            "[0.025105353, 1.0, 0.06666666644444444, 0.0024213075057601323]\n",
            "training on batch 1448 for epoch number 0\n",
            "[0.024724914, 1.0, 0.06666666644444444, 0.0024213075057601323]\n",
            "training on batch 1449 for epoch number 0\n",
            "[0.024322445, 1.0, 0.06666666644444444, 0.0024213075057601323]\n",
            "training on batch 1450 for epoch number 0\n",
            "[0.023900805, 1.0, 0.06666666644444444, 0.0024213075057601323]\n",
            "training on batch 1451 for epoch number 0\n",
            "[0.023463981, 1.0, 0.06666666644444444, 0.0024213075057601323]\n",
            "training on batch 1452 for epoch number 0\n",
            "[0.73072946, 0.8125, 0.06666666644444444, 0.0024125452349321418]\n",
            "training on batch 1453 for epoch number 0\n",
            "[0.023111697, 1.0, 0.06666666644444444, 0.0024125452349321418]\n",
            "training on batch 1454 for epoch number 0\n",
            "[0.023220778, 1.0, 0.06666666644444444, 0.0024125452349321418]\n",
            "training on batch 1455 for epoch number 0\n",
            "[0.023235017, 1.0, 0.06666666644444444, 0.0024125452349321418]\n",
            "training on batch 1456 for epoch number 0\n",
            "[0.023120008, 1.0, 0.06666666644444444, 0.0024125452349321418]\n",
            "training on batch 1457 for epoch number 0\n",
            "[0.022906035, 1.0, 0.06666666644444444, 0.0024125452349321418]\n",
            "training on batch 1458 for epoch number 0\n",
            "[0.022635888, 1.0, 0.06666666644444444, 0.0024125452349321418]\n",
            "training on batch 1459 for epoch number 0\n",
            "[0.022336585, 1.0, 0.06666666644444444, 0.0024125452349321418]\n",
            "training on batch 1460 for epoch number 0\n",
            "[0.2602581, 0.9375, 0.06666666644444444, 0.0024096385539265496]\n",
            "training on batch 1461 for epoch number 0\n",
            "[0.021860588, 1.0, 0.06666666644444444, 0.0024096385539265496]\n",
            "training on batch 1462 for epoch number 0\n",
            "[0.26086646, 0.9375, 0.06666666644444444, 0.0024067388685431123]\n",
            "training on batch 1463 for epoch number 0\n",
            "[0.5001228, 0.875, 0.06666666644444444, 0.002400960383865431]\n",
            "training on batch 1464 for epoch number 0\n",
            "[0.4984471, 0.875, 0.06666666644444444, 0.002395209580551472]\n",
            "training on batch 1465 for epoch number 0\n",
            "[0.022846041, 1.0, 0.06666666644444444, 0.002395209580551472]\n",
            "training on batch 1466 for epoch number 0\n",
            "[0.2584254, 0.9375, 0.06666666644444444, 0.00239234449732149]\n",
            "training on batch 1467 for epoch number 0\n",
            "[0.02425008, 1.0, 0.06666666644444444, 0.00239234449732149]\n",
            "training on batch 1468 for epoch number 0\n",
            "[0.024618128, 1.0, 0.06666666644444444, 0.00239234449732149]\n",
            "training on batch 1469 for epoch number 0\n",
            "[0.024717966, 1.0, 0.06666666644444444, 0.00239234449732149]\n",
            "training on batch 1470 for epoch number 0\n",
            "[0.024671191, 1.0, 0.06666666644444444, 0.00239234449732149]\n",
            "training on batch 1471 for epoch number 0\n",
            "[0.024565144, 1.0, 0.06666666644444444, 0.00239234449732149]\n",
            "training on batch 1472 for epoch number 0\n",
            "[0.2564902, 0.9375, 0.06666666644444444, 0.00238948626016852]\n",
            "training on batch 1473 for epoch number 0\n",
            "[0.024467118, 1.0, 0.06666666644444444, 0.00238948626016852]\n",
            "training on batch 1474 for epoch number 0\n",
            "[0.25648046, 0.9375, 0.06666666644444444, 0.002386634844583934]\n",
            "training on batch 1475 for epoch number 0\n",
            "[0.024501875, 1.0, 0.06666666644444444, 0.002386634844583934]\n",
            "training on batch 1476 for epoch number 0\n",
            "[0.024508465, 1.0, 0.06666666644444444, 0.002386634844583934]\n",
            "training on batch 1477 for epoch number 0\n",
            "[0.024444012, 1.0, 0.06666666644444444, 0.002386634844583934]\n",
            "training on batch 1478 for epoch number 0\n",
            "[0.024314454, 1.0, 0.06666666644444444, 0.002386634844583934]\n",
            "training on batch 1479 for epoch number 0\n",
            "[0.48920313, 0.875, 0.06666666644444444, 0.0023809523806689343]\n",
            "training on batch 1480 for epoch number 0\n",
            "[0.7201865, 0.8125, 0.06666666644444444, 0.0023724792405252102]\n",
            "training on batch 1481 for epoch number 0\n",
            "[0.025205385, 1.0, 0.06666666644444444, 0.0023724792405252102]\n",
            "training on batch 1482 for epoch number 0\n",
            "[0.026001265, 1.0, 0.06666666644444444, 0.0023724792405252102]\n",
            "training on batch 1483 for epoch number 0\n",
            "[0.02649875, 1.0, 0.06666666644444444, 0.0023724792405252102]\n",
            "training on batch 1484 for epoch number 0\n",
            "[0.026694834, 1.0, 0.06666666644444444, 0.0023724792405252102]\n",
            "training on batch 1485 for epoch number 0\n",
            "[0.25302562, 0.9375, 0.06666666644444444, 0.0023696682461647315]\n",
            "training on batch 1486 for epoch number 0\n",
            "[0.026849622, 1.0, 0.06666666644444444, 0.0023696682461647315]\n",
            "training on batch 1487 for epoch number 0\n",
            "[0.02687686, 1.0, 0.06666666644444444, 0.0023696682461647315]\n",
            "training on batch 1488 for epoch number 0\n",
            "[0.026818302, 1.0, 0.06666666644444444, 0.0023696682461647315]\n",
            "training on batch 1489 for epoch number 0\n",
            "[0.252817, 0.9375, 0.06666666644444444, 0.0023668639050453415]\n",
            "training on batch 1490 for epoch number 0\n",
            "[0.026725633, 1.0, 0.06666666644444444, 0.0023668639050453415]\n",
            "training on batch 1491 for epoch number 0\n",
            "[0.026670633, 1.0, 0.06666666644444444, 0.0023668639050453415]\n",
            "training on batch 1492 for epoch number 0\n",
            "[0.02654572, 1.0, 0.06666666644444444, 0.0023668639050453415]\n",
            "training on batch 1493 for epoch number 0\n",
            "[0.25330016, 0.9375, 0.06666666644444444, 0.0023640661935739877]\n",
            "training on batch 1494 for epoch number 0\n",
            "[0.026306521, 1.0, 0.06666666644444444, 0.0023640661935739877]\n",
            "training on batch 1495 for epoch number 0\n",
            "[0.026189826, 1.0, 0.06666666644444444, 0.0023640661935739877]\n",
            "training on batch 1496 for epoch number 0\n",
            "[0.02600915, 1.0, 0.06666666644444444, 0.0023640661935739877]\n",
            "training on batch 1497 for epoch number 0\n",
            "[0.25400552, 0.9375, 0.06666666644444444, 0.002361275088269035]\n",
            "training on batch 1498 for epoch number 0\n",
            "[0.025690025, 1.0, 0.06666666644444444, 0.002361275088269035]\n",
            "training on batch 1499 for epoch number 0\n",
            "[0.025538977, 1.0, 0.06666666644444444, 0.002361275088269035]\n",
            "training on batch 1500 for epoch number 0\n",
            "[0.025320968, 1.0, 0.06666666644444444, 0.002361275088269035]\n",
            "training on batch 1501 for epoch number 0\n",
            "[0.2550185, 0.9375, 0.06666666644444444, 0.002358490565759612]\n",
            "training on batch 1502 for epoch number 0\n",
            "[0.024933212, 1.0, 0.06666666644444444, 0.002358490565759612]\n",
            "training on batch 1503 for epoch number 0\n",
            "[0.024754941, 1.0, 0.06666666644444444, 0.002358490565759612]\n",
            "training on batch 1504 for epoch number 0\n",
            "[0.024518432, 1.0, 0.06666666644444444, 0.002358490565759612]\n",
            "training on batch 1505 for epoch number 0\n",
            "[0.25623724, 0.9375, 0.06666666644444444, 0.002355712602784957]\n",
            "training on batch 1506 for epoch number 0\n",
            "[0.024114078, 1.0, 0.06666666644444444, 0.002355712602784957]\n",
            "training on batch 1507 for epoch number 0\n",
            "[0.2566767, 0.9375, 0.06666666644444444, 0.0023529411761937717]\n",
            "training on batch 1508 for epoch number 0\n",
            "[0.02390898, 1.0, 0.06666666644444444, 0.0023529411761937717]\n",
            "training on batch 1509 for epoch number 0\n",
            "[0.023809938, 1.0, 0.06666666644444444, 0.0023529411761937717]\n",
            "training on batch 1510 for epoch number 0\n",
            "[0.023635667, 1.0, 0.06666666644444444, 0.0023529411761937717]\n",
            "training on batch 1511 for epoch number 0\n",
            "[0.25747928, 0.9375, 0.06666666644444444, 0.002350176262943575]\n",
            "training on batch 1512 for epoch number 0\n",
            "[0.023331413, 1.0, 0.06666666644444444, 0.002350176262943575]\n",
            "training on batch 1513 for epoch number 0\n",
            "[0.023192454, 1.0, 0.06666666644444444, 0.002350176262943575]\n",
            "training on batch 1514 for epoch number 0\n",
            "[0.022992939, 1.0, 0.06666666644444444, 0.002350176262943575]\n",
            "training on batch 1515 for epoch number 0\n",
            "[0.25861293, 0.9375, 0.06666666644444444, 0.0023474178401000686]\n",
            "training on batch 1516 for epoch number 0\n",
            "[0.25874078, 0.9375, 0.06666666644444444, 0.002344665884836499]\n",
            "training on batch 1517 for epoch number 0\n",
            "[0.022727266, 1.0, 0.06666666644444444, 0.002344665884836499]\n",
            "training on batch 1518 for epoch number 0\n",
            "[0.2586072, 0.9375, 0.06666666644444444, 0.0023419203744330305]\n",
            "training on batch 1519 for epoch number 0\n",
            "[0.022863656, 1.0, 0.06666666644444444, 0.0023419203744330305]\n",
            "training on batch 1520 for epoch number 0\n",
            "[0.022893744, 1.0, 0.06666666644444444, 0.0023419203744330305]\n",
            "training on batch 1521 for epoch number 0\n",
            "[0.022815859, 1.0, 0.06666666644444444, 0.0023419203744330305]\n",
            "training on batch 1522 for epoch number 0\n",
            "[0.022659656, 1.0, 0.06666666644444444, 0.0023419203744330305]\n",
            "training on batch 1523 for epoch number 0\n",
            "[0.49572998, 0.875, 0.06666666644444444, 0.0023364485978578917]\n",
            "training on batch 1524 for epoch number 0\n",
            "[0.022624386, 1.0, 0.06666666644444444, 0.0023364485978578917]\n",
            "training on batch 1525 for epoch number 0\n",
            "[0.0227085, 1.0, 0.06666666644444444, 0.0023364485978578917]\n",
            "training on batch 1526 for epoch number 0\n",
            "[0.2586252, 0.9375, 0.06666666644444444, 0.0023337222867755286]\n",
            "training on batch 1527 for epoch number 0\n",
            "[0.022837343, 1.0, 0.06666666644444444, 0.0023337222867755286]\n",
            "training on batch 1528 for epoch number 0\n",
            "[0.022868173, 1.0, 0.06666666644444444, 0.0023337222867755286]\n",
            "training on batch 1529 for epoch number 0\n",
            "[0.022803498, 1.0, 0.06666666644444444, 0.0023337222867755286]\n",
            "training on batch 1530 for epoch number 0\n",
            "[0.022667676, 1.0, 0.06666666644444444, 0.0023337222867755286]\n",
            "training on batch 1531 for epoch number 0\n",
            "[0.259047, 0.9375, 0.06666666644444444, 0.0023310023307306525]\n",
            "training on batch 1532 for epoch number 0\n",
            "[0.022454597, 1.0, 0.06666666644444444, 0.0023310023307306525]\n",
            "training on batch 1533 for epoch number 0\n",
            "[0.25926438, 0.9375, 0.06666666644444444, 0.002328288707528721]\n",
            "training on batch 1534 for epoch number 0\n",
            "[0.022417694, 1.0, 0.06666666644444444, 0.002328288707528721]\n",
            "training on batch 1535 for epoch number 0\n",
            "[0.49591765, 0.875, 0.06666666644444444, 0.002322880371391071]\n",
            "training on batch 1536 for epoch number 0\n",
            "[0.022777813, 1.0, 0.06666666644444444, 0.002322880371391071]\n",
            "training on batch 1537 for epoch number 0\n",
            "[0.25817603, 0.9375, 0.06666666644444444, 0.002320185614580025]\n",
            "training on batch 1538 for epoch number 0\n",
            "[0.023429673, 1.0, 0.06666666644444444, 0.002320185614580025]\n",
            "training on batch 1539 for epoch number 0\n",
            "[0.023632035, 1.0, 0.06666666644444444, 0.002320185614580025]\n",
            "training on batch 1540 for epoch number 0\n",
            "[0.023671927, 1.0, 0.06666666644444444, 0.002320185614580025]\n",
            "training on batch 1541 for epoch number 0\n",
            "[0.023604574, 1.0, 0.06666666644444444, 0.002320185614580025]\n",
            "training on batch 1542 for epoch number 0\n",
            "[0.023475673, 1.0, 0.06666666644444444, 0.002320185614580025]\n",
            "training on batch 1543 for epoch number 0\n",
            "[0.023308618, 1.0, 0.06666666644444444, 0.002320185614580025]\n",
            "training on batch 1544 for epoch number 0\n",
            "[0.2582068, 0.9375, 0.06666666644444444, 0.0023174971028600815]\n",
            "training on batch 1545 for epoch number 0\n",
            "[0.023049872, 1.0, 0.06666666644444444, 0.0023174971028600815]\n",
            "training on batch 1546 for epoch number 0\n",
            "[0.022935888, 1.0, 0.06666666644444444, 0.0023174971028600815]\n",
            "training on batch 1547 for epoch number 0\n",
            "[0.022774165, 1.0, 0.06666666644444444, 0.0023174971028600815]\n",
            "training on batch 1548 for epoch number 0\n",
            "[0.022567926, 1.0, 0.06666666644444444, 0.0023174971028600815]\n",
            "training on batch 1549 for epoch number 0\n",
            "[0.0223239, 1.0, 0.06666666644444444, 0.0023174971028600815]\n",
            "training on batch 1550 for epoch number 0\n",
            "[0.022049472, 1.0, 0.06666666644444444, 0.0023174971028600815]\n",
            "training on batch 1551 for epoch number 0\n",
            "[0.26077965, 0.9375, 0.06666666644444444, 0.0023148148145468966]\n",
            "training on batch 1552 for epoch number 0\n",
            "[0.021598607, 1.0, 0.06666666644444444, 0.0023148148145468966]\n",
            "training on batch 1553 for epoch number 0\n",
            "[0.021414183, 1.0, 0.06666666644444444, 0.0023148148145468966]\n",
            "training on batch 1554 for epoch number 0\n",
            "[0.021191483, 1.0, 0.06666666644444444, 0.0023148148145468966]\n",
            "training on batch 1555 for epoch number 0\n",
            "[0.020933008, 1.0, 0.06666666644444444, 0.0023148148145468966]\n",
            "training on batch 1556 for epoch number 0\n",
            "[0.26283145, 0.9375, 0.06666666644444444, 0.0023121387280564004]\n",
            "training on batch 1557 for epoch number 0\n",
            "[0.020517528, 1.0, 0.06666666644444444, 0.0023121387280564004]\n",
            "training on batch 1558 for epoch number 0\n",
            "[0.2633643, 0.9375, 0.06666666644444444, 0.0023094688219042186]\n",
            "training on batch 1559 for epoch number 0\n",
            "[0.020339388, 1.0, 0.06666666644444444, 0.0023094688219042186]\n",
            "training on batch 1560 for epoch number 0\n",
            "[0.020268276, 1.0, 0.06666666644444444, 0.0023094688219042186]\n",
            "training on batch 1561 for epoch number 0\n",
            "[0.26375312, 0.9375, 0.06666666644444444, 0.0023068050747050975]\n",
            "training on batch 1562 for epoch number 0\n",
            "[0.02015216, 1.0, 0.06666666644444444, 0.0023068050747050975]\n",
            "training on batch 1563 for epoch number 0\n",
            "[0.020095918, 1.0, 0.06666666644444444, 0.0023068050747050975]\n",
            "training on batch 1564 for epoch number 0\n",
            "[0.01997163, 1.0, 0.06666666644444444, 0.0023068050747050975]\n",
            "training on batch 1565 for epoch number 0\n",
            "[0.26441836, 0.9375, 0.06666666644444444, 0.0023041474651723333]\n",
            "training on batch 1566 for epoch number 0\n",
            "[0.019783657, 1.0, 0.06666666644444444, 0.0023041474651723333]\n",
            "training on batch 1567 for epoch number 0\n",
            "[0.01970987, 1.0, 0.06666666644444444, 0.0023041474651723333]\n",
            "training on batch 1568 for epoch number 0\n",
            "[0.264868, 0.9375, 0.06666666644444444, 0.002301495972117204]\n",
            "training on batch 1569 for epoch number 0\n",
            "[0.019609567, 1.0, 0.06666666644444444, 0.002301495972117204]\n",
            "training on batch 1570 for epoch number 0\n",
            "[0.01957275, 1.0, 0.06666666644444444, 0.002301495972117204]\n",
            "training on batch 1571 for epoch number 0\n",
            "[0.019475939, 1.0, 0.06666666644444444, 0.002301495972117204]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1W82HPzrz1q2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lrcn.save(\"trained on 21-50\")\n",
        "# x_t,y_t,x_tpaths=load_data(x_folder,y_folder,50,51);\n",
        "#x_test,y_test=format_data(x_t, y_t, 1 ,timesteps, y_t.shape[0] - timesteps - 10);\n",
        "#y_pred=lrcn.predict(x_test,batch_size=batch_size);\n",
        "\n",
        "\n",
        "#np.savetxt('y.csv',y_pred,delimiter=',',fmt='%.6f')\n",
        "#print(y_pred);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RRSZTPCaz1q6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_test,y_test=format_data(x_t, y_t, 1 ,timesteps, 500);\n",
        "y_pred=lrcn.predict(x_test,batch_size=batch_size);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zHKpwyIBEcok",
        "colab_type": "code",
        "outputId": "2d1321f7-7a84-40cf-b4ef-d164dd32cf93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(y_pred.max())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.050530244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iEtx5O69Ecw4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6RpOWFszEc0S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LKtkTFp0Ec3I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MIsC6HuTz1q9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(43):\n",
        "    print(i)\n",
        "    plt.imshow(x_data[y_data==1][i])\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xiqJ_fJEz1rB",
        "colab_type": "code",
        "outputId": "3abe29b7-0e49-408b-ac1a-d3ea9828c8f4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.where(y_data==1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 26,  38,  73, 139, 143, 186, 193, 197, 204, 211, 267, 275, 288,\n",
              "        303, 316, 363, 365, 386, 390, 446, 447, 455, 457, 485, 511, 531,\n",
              "        547, 562, 573, 574, 577, 618, 620, 624, 634, 727, 738, 741, 783,\n",
              "        786, 792, 818, 846]),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "nkCutjyUz1rF",
        "colab_type": "code",
        "outputId": "4f3428d4-fc9f-4dfa-a994-bbce0df1b4c6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x12e1158d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "oA0c6fZbz1rI",
        "colab_type": "code",
        "outputId": "3cb64a3b-db7b-4b25-e9c1-9de49522d3e1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.where(y_data==1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  26,   38,   73,  139,  143,  186,  193,  197,  204,  211,  267,\n",
              "         275,  288,  303,  316,  363,  365,  386,  390,  446,  447,  455,\n",
              "         457,  485,  511,  531,  547,  562,  573,  574,  577,  618,  620,\n",
              "         624,  634,  727,  738,  741,  783,  786,  792,  818,  846,  899,\n",
              "         902,  903,  972,  983,  984, 1055, 1059, 1108, 1146, 1149, 1158,\n",
              "        1161, 1257, 1342, 1357, 1359, 1400, 1403, 1419, 1420, 1422, 1436,\n",
              "        1437, 1508, 1570, 1573, 1645, 1670, 1672, 1676, 1689, 1719, 1763,\n",
              "        1809, 1811, 1860, 1864, 1938, 1970, 1973, 2037, 2040, 2060, 2109,\n",
              "        2115, 2176, 2181, 2267, 2269, 2389, 2391, 2393, 2408, 2411, 2414,\n",
              "        2420, 2492, 2496, 2548, 2561, 2635, 2699, 2702, 2705, 2709, 2719,\n",
              "        2736, 2749, 2761, 2791, 2809, 2823, 2832, 2843, 2852, 2854, 2887,\n",
              "        2899, 2931, 2943, 2961, 2971, 2984, 2988, 2993, 2999, 3014, 3017,\n",
              "        3041, 3053, 3066, 3095, 3105, 3159, 3191, 3220, 3221, 3225, 3231,\n",
              "        3241, 3242, 3252, 3264, 3272, 3292, 3300, 3308, 3322, 3343, 3381,\n",
              "        3401, 3405, 3414, 3423, 3425, 3426, 3430, 3442, 3448, 3458, 3488,\n",
              "        3512, 3523, 3527, 3578, 3588, 3610, 3620, 3686, 3698, 3705, 3708,\n",
              "        3746, 3754, 3757, 3787, 3788, 3826, 3883, 3893, 3899, 3955, 4002,\n",
              "        4074, 4160, 4217, 4218, 4238, 4321, 4391, 4440, 4490]),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "H0LdIL7Hz1rL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nHTz71aMz1rN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.listdir('board-snapper/labels')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Fc2yzf5z1rU",
        "colab_type": "code",
        "outputId": "5d79ebd4-b89c-4766-c769-bd8e2fa279f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.zeros((1,2,3,4,5)).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2, 3, 4, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Gfl7ArMhq9eR",
        "colab_type": "code",
        "outputId": "907adc66-c502-46db-bd49-cd40289c9e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "os.listdir('board-snapper')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['convLstmbatch.py',\n",
              " 'labels',\n",
              " '.ipynb_checkpoints',\n",
              " '.DS_Store',\n",
              " 'Board Snapper.ipynb',\n",
              " 'slides',\n",
              " '.git']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "IiiI5Eykq9hT",
        "colab_type": "code",
        "outputId": "744c1d9a-d103-47d2-885e-3c1c550036a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "cell_type": "code",
      "source": [
        "label=np.array(pd.read_csv(('')));"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "board-snapper  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y8DXZJEOq9kB",
        "colab_type": "code",
        "outputId": "c33951a6-dd06-4f9c-9b8e-c40c910edc87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16711
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "np.array(pd.read_csv(\"board-snapper/labels/lec26.csv\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "WRv13ck1q9m3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UBQCBCXFq9p2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Md4EXZu0q9si",
        "colab_type": "code",
        "outputId": "9d91348d-29ac-46b5-b665-e869dc180b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(847,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "_C-DHRU5z1rX",
        "colab_type": "code",
        "outputId": "f156ee92-487d-48db-a01c-e8839125fa3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'board-snapper'...\n",
            "remote: Enumerating objects: 39594, done.\u001b[K\n",
            "remote: Counting objects: 100% (39594/39594), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39533/39533), done.\u001b[K\n",
            "remote: Total 39594 (delta 61), reused 39594 (delta 61), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (39594/39594), 1.05 GiB | 13.83 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n",
            "Checking out files: 100% (40976/40976), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q3Qom2JIzva2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}